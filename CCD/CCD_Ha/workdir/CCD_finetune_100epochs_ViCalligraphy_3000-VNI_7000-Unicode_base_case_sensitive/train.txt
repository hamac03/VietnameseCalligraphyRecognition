ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/evaluation_Vicalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/evaluation_Vicalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = pretrained_model/Base_ARD_checkpoint.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training-->22431'

'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/evaluation_Vicalligraphy/evaluation-->3107'

'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/evaluation_Vicalligraphy/evaluation-->3107'

each epoch iteration: 535
Read vision model from pretrained_model/Base_ARD_checkpoint.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

Start training from scratch.
iteration:0--> train loss:5.542469024658203
eval model
iteration:5000--> train loss:0.5739584565162659
eval model
iteration:10000--> train loss:0.32087787985801697
eval model
iteration:15000--> train loss:0.2578381597995758
eval model
iteration:20000--> train loss:0.204208642244339
eval model
iteration:25000--> train loss:0.16124513745307922
eval model
iteration:30000--> train loss:0.12399999797344208
eval model
iteration:35000--> train loss:0.09058333933353424
eval model
iteration:40000--> train loss:0.0650622770190239
eval model
iteration:45000--> train loss:0.04871145635843277
eval model
iteration:50000--> train loss:0.03782054781913757
eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/evaluation_Vicalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/evaluation_Vicalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = ./saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training-->22431'

'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/evaluation_Vicalligraphy/evaluation-->3107'

'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/evaluation_Vicalligraphy/evaluation-->3107'

each epoch iteration: 535
Read vision model from ./saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

Start training from scratch.
iteration:0--> train loss:0.07311802357435226
eval model
iteration:5000--> train loss:0.20604445040225983
eval model
iteration:10000--> train loss:0.2022046148777008
eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/evaluation_Vicalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/evaluation_Vicalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = ./saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/evaluation_Vicalligraphy/evaluation-->3107'

Read vision model from ./saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/evaluation_Vicalligraphy/evaluation']
	(22): dataset_train_batch_size = 1
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 1
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/evaluation_Vicalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 1
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 1
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 1
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 1
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 1
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 1
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 1
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 1
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 1
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 1
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 1
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 1
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 1
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 1
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 1
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 1
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 1
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 1
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 1
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 1
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 1
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 1
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 1
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 1
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = pretrained_model/Base_ARD_checkpoint.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Constructing dataset.
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = pretrained_model/Base_ARD_checkpoint.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Constructing dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Saving ground truth images and labels.
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = pretrained_model/Base_ARD_checkpoint.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Constructing dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Saving ground truth images and labels.
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = pretrained_model/Base_ARD_checkpoint.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Constructing dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Saving ground truth images and labels.
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = pretrained_model/Base_ARD_checkpoint.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Constructing dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Saving ground truth images and labels.
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 1
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = pretrained_model/Base_ARD_checkpoint.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Constructing dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Saving ground truth images and labels.
Saved image: ./gt_images_test/images_gt/0_0_gt.jpg
Saved label: 0_0_gt.jpg	('Nm',)
Saved image: ./gt_images_test/images_gt/1_0_gt.jpg
Saved label: 1_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/2_0_gt.jpg
Saved label: 2_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/3_0_gt.jpg
Saved label: 3_0_gt.jpg	('bt',)
Saved image: ./gt_images_test/images_gt/4_0_gt.jpg
Saved label: 4_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/5_0_gt.jpg
Saved label: 5_0_gt.jpg	('Nh',)
Saved image: ./gt_images_test/images_gt/6_0_gt.jpg
Saved label: 6_0_gt.jpg	('nhng',)
Saved image: ./gt_images_test/images_gt/7_0_gt.jpg
Saved label: 7_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/8_0_gt.jpg
Saved label: 8_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/9_0_gt.jpg
Saved label: 9_0_gt.jpg	('Vinh',)
Saved image: ./gt_images_test/images_gt/10_0_gt.jpg
Saved label: 10_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/11_0_gt.jpg
Saved label: 11_0_gt.jpg	('h',)
Saved image: ./gt_images_test/images_gt/12_0_gt.jpg
Saved label: 12_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/13_0_gt.jpg
Saved label: 13_0_gt.jpg	('Qun',)
Saved image: ./gt_images_test/images_gt/14_0_gt.jpg
Saved label: 14_0_gt.jpg	('Chn',)
Saved image: ./gt_images_test/images_gt/15_0_gt.jpg
Saved label: 15_0_gt.jpg	('Hoa',)
Saved image: ./gt_images_test/images_gt/16_0_gt.jpg
Saved label: 16_0_gt.jpg	('Hc',)
Saved image: ./gt_images_test/images_gt/17_0_gt.jpg
Saved label: 17_0_gt.jpg	('trong',)
Saved image: ./gt_images_test/images_gt/18_0_gt.jpg
Saved label: 18_0_gt.jpg	('Sinh',)
Saved image: ./gt_images_test/images_gt/19_0_gt.jpg
Saved label: 19_0_gt.jpg	('Xun',)
Saved image: ./gt_images_test/images_gt/20_0_gt.jpg
Saved label: 20_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/21_0_gt.jpg
Saved label: 21_0_gt.jpg	('cm',)
Saved image: ./gt_images_test/images_gt/22_0_gt.jpg
Saved label: 22_0_gt.jpg	('Minh',)
Saved image: ./gt_images_test/images_gt/23_0_gt.jpg
Saved label: 23_0_gt.jpg	('y',)
Saved image: ./gt_images_test/images_gt/24_0_gt.jpg
Saved label: 24_0_gt.jpg	('bic',)
Saved image: ./gt_images_test/images_gt/25_0_gt.jpg
Saved label: 25_0_gt.jpg	('hy',)
Saved image: ./gt_images_test/images_gt/26_0_gt.jpg
Saved label: 26_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/27_0_gt.jpg
Saved label: 27_0_gt.jpg	('Bn',)
Saved image: ./gt_images_test/images_gt/28_0_gt.jpg
Saved label: 28_0_gt.jpg	('Hoa',)
Saved image: ./gt_images_test/images_gt/29_0_gt.jpg
Saved label: 29_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/30_0_gt.jpg
Saved label: 30_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/31_0_gt.jpg
Saved label: 31_0_gt.jpg	('Bnh',)
Saved image: ./gt_images_test/images_gt/32_0_gt.jpg
Saved label: 32_0_gt.jpg	('bun',)
Saved image: ./gt_images_test/images_gt/33_0_gt.jpg
Saved label: 33_0_gt.jpg	('tui',)
Saved image: ./gt_images_test/images_gt/34_0_gt.jpg
Saved label: 34_0_gt.jpg	('Thm',)
Saved image: ./gt_images_test/images_gt/35_0_gt.jpg
Saved label: 35_0_gt.jpg	('Cung',)
Saved image: ./gt_images_test/images_gt/36_0_gt.jpg
Saved label: 36_0_gt.jpg	('iu',)
Saved image: ./gt_images_test/images_gt/37_0_gt.jpg
Saved label: 37_0_gt.jpg	('Tinh',)
Saved image: ./gt_images_test/images_gt/38_0_gt.jpg
Saved label: 38_0_gt.jpg	('gi',)
Saved image: ./gt_images_test/images_gt/39_0_gt.jpg
Saved label: 39_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/40_0_gt.jpg
Saved label: 40_0_gt.jpg	('un',)
Saved image: ./gt_images_test/images_gt/41_0_gt.jpg
Saved label: 41_0_gt.jpg	('Vit',)
Saved image: ./gt_images_test/images_gt/42_0_gt.jpg
Saved label: 42_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/43_0_gt.jpg
Saved label: 43_0_gt.jpg	('phn',)
Saved image: ./gt_images_test/images_gt/44_0_gt.jpg
Saved label: 44_0_gt.jpg	('cnh',)
Saved image: ./gt_images_test/images_gt/45_0_gt.jpg
Saved label: 45_0_gt.jpg	('Gnh',)
Saved image: ./gt_images_test/images_gt/46_0_gt.jpg
Saved label: 46_0_gt.jpg	('Tr',)
Saved image: ./gt_images_test/images_gt/47_0_gt.jpg
Saved label: 47_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/48_0_gt.jpg
Saved label: 48_0_gt.jpg	('hoa',)
Saved image: ./gt_images_test/images_gt/49_0_gt.jpg
Saved label: 49_0_gt.jpg	('ln',)
Saved image: ./gt_images_test/images_gt/50_0_gt.jpg
Saved label: 50_0_gt.jpg	('Lng',)
Saved image: ./gt_images_test/images_gt/51_0_gt.jpg
Saved label: 51_0_gt.jpg	('sau',)
Saved image: ./gt_images_test/images_gt/52_0_gt.jpg
Saved label: 52_0_gt.jpg	('ring',)
Saved image: ./gt_images_test/images_gt/53_0_gt.jpg
Saved label: 53_0_gt.jpg	('lm',)
Saved image: ./gt_images_test/images_gt/54_0_gt.jpg
Saved label: 54_0_gt.jpg	('thy',)
Saved image: ./gt_images_test/images_gt/55_0_gt.jpg
Saved label: 55_0_gt.jpg	('au',)
Saved image: ./gt_images_test/images_gt/56_0_gt.jpg
Saved label: 56_0_gt.jpg	('Bch',)
Saved image: ./gt_images_test/images_gt/57_0_gt.jpg
Saved label: 57_0_gt.jpg	('Ti',)
Saved image: ./gt_images_test/images_gt/58_0_gt.jpg
Saved label: 58_0_gt.jpg	('Sen',)
Saved image: ./gt_images_test/images_gt/59_0_gt.jpg
Saved label: 59_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/60_0_gt.jpg
Saved label: 60_0_gt.jpg	('thnh',)
Saved image: ./gt_images_test/images_gt/61_0_gt.jpg
Saved label: 61_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/62_0_gt.jpg
Saved label: 62_0_gt.jpg	('Thm',)
Saved image: ./gt_images_test/images_gt/63_0_gt.jpg
Saved label: 63_0_gt.jpg	('chu',)
Saved image: ./gt_images_test/images_gt/64_0_gt.jpg
Saved label: 64_0_gt.jpg	('Thip',)
Saved image: ./gt_images_test/images_gt/65_0_gt.jpg
Saved label: 65_0_gt.jpg	('a',)
Saved image: ./gt_images_test/images_gt/66_0_gt.jpg
Saved label: 66_0_gt.jpg	('ru',)
Saved image: ./gt_images_test/images_gt/67_0_gt.jpg
Saved label: 67_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/68_0_gt.jpg
Saved label: 68_0_gt.jpg	('Khi',)
Saved image: ./gt_images_test/images_gt/69_0_gt.jpg
Saved label: 69_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/70_0_gt.jpg
Saved label: 70_0_gt.jpg	('x',)
Saved image: ./gt_images_test/images_gt/71_0_gt.jpg
Saved label: 71_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/72_0_gt.jpg
Saved label: 72_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/73_0_gt.jpg
Saved label: 73_0_gt.jpg	('bng',)
Saved image: ./gt_images_test/images_gt/74_0_gt.jpg
Saved label: 74_0_gt.jpg	('vui',)
Saved image: ./gt_images_test/images_gt/75_0_gt.jpg
Saved label: 75_0_gt.jpg	('chnh',)
Saved image: ./gt_images_test/images_gt/76_0_gt.jpg
Saved label: 76_0_gt.jpg	('ring',)
Saved image: ./gt_images_test/images_gt/77_0_gt.jpg
Saved label: 77_0_gt.jpg	('nhng',)
Saved image: ./gt_images_test/images_gt/78_0_gt.jpg
Saved label: 78_0_gt.jpg	('thc',)
Saved image: ./gt_images_test/images_gt/79_0_gt.jpg
Saved label: 79_0_gt.jpg	('Mng',)
Saved image: ./gt_images_test/images_gt/80_0_gt.jpg
Saved label: 80_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/81_0_gt.jpg
Saved label: 81_0_gt.jpg	('on',)
Saved image: ./gt_images_test/images_gt/82_0_gt.jpg
Saved label: 82_0_gt.jpg	('bng',)
Saved image: ./gt_images_test/images_gt/83_0_gt.jpg
Saved label: 83_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/84_0_gt.jpg
Saved label: 84_0_gt.jpg	('Lc',)
Saved image: ./gt_images_test/images_gt/85_0_gt.jpg
Saved label: 85_0_gt.jpg	('ngha',)
Saved image: ./gt_images_test/images_gt/86_0_gt.jpg
Saved label: 86_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/87_0_gt.jpg
Saved label: 87_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/88_0_gt.jpg
Saved label: 88_0_gt.jpg	('Bi',)
Saved image: ./gt_images_test/images_gt/89_0_gt.jpg
Saved label: 89_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/90_0_gt.jpg
Saved label: 90_0_gt.jpg	('thm',)
Saved image: ./gt_images_test/images_gt/91_0_gt.jpg
Saved label: 91_0_gt.jpg	('hin',)
Saved image: ./gt_images_test/images_gt/92_0_gt.jpg
Saved label: 92_0_gt.jpg	('Tng',)
Saved image: ./gt_images_test/images_gt/93_0_gt.jpg
Saved label: 93_0_gt.jpg	('yn',)
Saved image: ./gt_images_test/images_gt/94_0_gt.jpg
Saved label: 94_0_gt.jpg	('hay',)
Saved image: ./gt_images_test/images_gt/95_0_gt.jpg
Saved label: 95_0_gt.jpg	('sch',)
Saved image: ./gt_images_test/images_gt/96_0_gt.jpg
Saved label: 96_0_gt.jpg	('Tr',)
Saved image: ./gt_images_test/images_gt/97_0_gt.jpg
Saved label: 97_0_gt.jpg	('thu',)
Saved image: ./gt_images_test/images_gt/98_0_gt.jpg
Saved label: 98_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/99_0_gt.jpg
Saved label: 99_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/100_0_gt.jpg
Saved label: 100_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/101_0_gt.jpg
Saved label: 101_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/102_0_gt.jpg
Saved label: 102_0_gt.jpg	('ta',)
Saved image: ./gt_images_test/images_gt/103_0_gt.jpg
Saved label: 103_0_gt.jpg	('L',)
Saved image: ./gt_images_test/images_gt/104_0_gt.jpg
Saved label: 104_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/105_0_gt.jpg
Saved label: 105_0_gt.jpg	('xung',)
Saved image: ./gt_images_test/images_gt/106_0_gt.jpg
Saved label: 106_0_gt.jpg	('thy',)
Saved image: ./gt_images_test/images_gt/107_0_gt.jpg
Saved label: 107_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/108_0_gt.jpg
Saved label: 108_0_gt.jpg	('to',)
Saved image: ./gt_images_test/images_gt/109_0_gt.jpg
Saved label: 109_0_gt.jpg	('nhng',)
Saved image: ./gt_images_test/images_gt/110_0_gt.jpg
Saved label: 110_0_gt.jpg	('ngha',)
Saved image: ./gt_images_test/images_gt/111_0_gt.jpg
Saved label: 111_0_gt.jpg	('Quang',)
Saved image: ./gt_images_test/images_gt/112_0_gt.jpg
Saved label: 112_0_gt.jpg	('nch',)
Saved image: ./gt_images_test/images_gt/113_0_gt.jpg
Saved label: 113_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/114_0_gt.jpg
Saved label: 114_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/115_0_gt.jpg
Saved label: 115_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/116_0_gt.jpg
Saved label: 116_0_gt.jpg	('Yn',)
Saved image: ./gt_images_test/images_gt/117_0_gt.jpg
Saved label: 117_0_gt.jpg	('Thnh',)
Saved image: ./gt_images_test/images_gt/118_0_gt.jpg
Saved label: 118_0_gt.jpg	('Ph',)
Saved image: ./gt_images_test/images_gt/119_0_gt.jpg
Saved label: 119_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/120_0_gt.jpg
Saved label: 120_0_gt.jpg	('hn',)
Saved image: ./gt_images_test/images_gt/121_0_gt.jpg
Saved label: 121_0_gt.jpg	('Khang',)
Saved image: ./gt_images_test/images_gt/122_0_gt.jpg
Saved label: 122_0_gt.jpg	('Lc',)
Saved image: ./gt_images_test/images_gt/123_0_gt.jpg
Saved label: 123_0_gt.jpg	('khng',)
Saved image: ./gt_images_test/images_gt/124_0_gt.jpg
Saved label: 124_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/125_0_gt.jpg
Saved label: 125_0_gt.jpg	('Tnh',)
Saved image: ./gt_images_test/images_gt/126_0_gt.jpg
Saved label: 126_0_gt.jpg	('tinh',)
Saved image: ./gt_images_test/images_gt/127_0_gt.jpg
Saved label: 127_0_gt.jpg	('T',)
Saved image: ./gt_images_test/images_gt/128_0_gt.jpg
Saved label: 128_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/129_0_gt.jpg
Saved label: 129_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/130_0_gt.jpg
Saved label: 130_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/131_0_gt.jpg
Saved label: 131_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/132_0_gt.jpg
Saved label: 132_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/133_0_gt.jpg
Saved label: 133_0_gt.jpg	('bn',)
Saved image: ./gt_images_test/images_gt/134_0_gt.jpg
Saved label: 134_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/135_0_gt.jpg
Saved label: 135_0_gt.jpg	('ging',)
Saved image: ./gt_images_test/images_gt/136_0_gt.jpg
Saved label: 136_0_gt.jpg	('Cha',)
Saved image: ./gt_images_test/images_gt/137_0_gt.jpg
Saved label: 137_0_gt.jpg	('truyn',)
Saved image: ./gt_images_test/images_gt/138_0_gt.jpg
Saved label: 138_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/139_0_gt.jpg
Saved label: 139_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/140_0_gt.jpg
Saved label: 140_0_gt.jpg	('php',)
Saved image: ./gt_images_test/images_gt/141_0_gt.jpg
Saved label: 141_0_gt.jpg	('Hu',)
Saved image: ./gt_images_test/images_gt/142_0_gt.jpg
Saved label: 142_0_gt.jpg	('Em',)
Saved image: ./gt_images_test/images_gt/143_0_gt.jpg
Saved label: 143_0_gt.jpg	('hm',)
Saved image: ./gt_images_test/images_gt/144_0_gt.jpg
Saved label: 144_0_gt.jpg	('ngonh',)
Saved image: ./gt_images_test/images_gt/145_0_gt.jpg
Saved label: 145_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/146_0_gt.jpg
Saved label: 146_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/147_0_gt.jpg
Saved label: 147_0_gt.jpg	('nghip',)
Saved image: ./gt_images_test/images_gt/148_0_gt.jpg
Saved label: 148_0_gt.jpg	('K',)
Saved image: ./gt_images_test/images_gt/149_0_gt.jpg
Saved label: 149_0_gt.jpg	('thm',)
Saved image: ./gt_images_test/images_gt/150_0_gt.jpg
Saved label: 150_0_gt.jpg	('Tt',)
Saved image: ./gt_images_test/images_gt/151_0_gt.jpg
Saved label: 151_0_gt.jpg	('Ch',)
Saved image: ./gt_images_test/images_gt/152_0_gt.jpg
Saved label: 152_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/153_0_gt.jpg
Saved label: 153_0_gt.jpg	('gt',)
Saved image: ./gt_images_test/images_gt/154_0_gt.jpg
Saved label: 154_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/155_0_gt.jpg
Saved label: 155_0_gt.jpg	('hoi',)
Saved image: ./gt_images_test/images_gt/156_0_gt.jpg
Saved label: 156_0_gt.jpg	('k',)
Saved image: ./gt_images_test/images_gt/157_0_gt.jpg
Saved label: 157_0_gt.jpg	('cuc',)
Saved image: ./gt_images_test/images_gt/158_0_gt.jpg
Saved label: 158_0_gt.jpg	('Nam',)
Saved image: ./gt_images_test/images_gt/159_0_gt.jpg
Saved label: 159_0_gt.jpg	('ru',)
Saved image: ./gt_images_test/images_gt/160_0_gt.jpg
Saved label: 160_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/161_0_gt.jpg
Saved label: 161_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/162_0_gt.jpg
Saved label: 162_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/163_0_gt.jpg
Saved label: 163_0_gt.jpg	('hoa',)
Saved image: ./gt_images_test/images_gt/164_0_gt.jpg
Saved label: 164_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/165_0_gt.jpg
Saved label: 165_0_gt.jpg	('Ci',)
Saved image: ./gt_images_test/images_gt/166_0_gt.jpg
Saved label: 166_0_gt.jpg	('Ngc',)
Saved image: ./gt_images_test/images_gt/167_0_gt.jpg
Saved label: 167_0_gt.jpg	('bn',)
Saved image: ./gt_images_test/images_gt/168_0_gt.jpg
Saved label: 168_0_gt.jpg	('Phng',)
Saved image: ./gt_images_test/images_gt/169_0_gt.jpg
Saved label: 169_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/170_0_gt.jpg
Saved label: 170_0_gt.jpg	('hm',)
Saved image: ./gt_images_test/images_gt/171_0_gt.jpg
Saved label: 171_0_gt.jpg	('che',)
Saved image: ./gt_images_test/images_gt/172_0_gt.jpg
Saved label: 172_0_gt.jpg	('nhau',)
Saved image: ./gt_images_test/images_gt/173_0_gt.jpg
Saved label: 173_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/174_0_gt.jpg
Saved label: 174_0_gt.jpg	('bng',)
Saved image: ./gt_images_test/images_gt/175_0_gt.jpg
Saved label: 175_0_gt.jpg	('sau',)
Saved image: ./gt_images_test/images_gt/176_0_gt.jpg
Saved label: 176_0_gt.jpg	('xui',)
Saved image: ./gt_images_test/images_gt/177_0_gt.jpg
Saved label: 177_0_gt.jpg	('ma',)
Saved image: ./gt_images_test/images_gt/178_0_gt.jpg
Saved label: 178_0_gt.jpg	('xao',)
Saved image: ./gt_images_test/images_gt/179_0_gt.jpg
Saved label: 179_0_gt.jpg	('s',)
Saved image: ./gt_images_test/images_gt/180_0_gt.jpg
Saved label: 180_0_gt.jpg	('Thy',)
Saved image: ./gt_images_test/images_gt/181_0_gt.jpg
Saved label: 181_0_gt.jpg	('Ct',)
Saved image: ./gt_images_test/images_gt/182_0_gt.jpg
Saved label: 182_0_gt.jpg	('nng',)
Saved image: ./gt_images_test/images_gt/183_0_gt.jpg
Saved label: 183_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/184_0_gt.jpg
Saved label: 184_0_gt.jpg	('gp',)
Saved image: ./gt_images_test/images_gt/185_0_gt.jpg
Saved label: 185_0_gt.jpg	('Thnh',)
Saved image: ./gt_images_test/images_gt/186_0_gt.jpg
Saved label: 186_0_gt.jpg	('gia',)
Saved image: ./gt_images_test/images_gt/187_0_gt.jpg
Saved label: 187_0_gt.jpg	('mit',)
Saved image: ./gt_images_test/images_gt/188_0_gt.jpg
Saved label: 188_0_gt.jpg	('bt',)
Saved image: ./gt_images_test/images_gt/189_0_gt.jpg
Saved label: 189_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/190_0_gt.jpg
Saved label: 190_0_gt.jpg	('Vinh',)
Saved image: ./gt_images_test/images_gt/191_0_gt.jpg
Saved label: 191_0_gt.jpg	('nm',)
Saved image: ./gt_images_test/images_gt/192_0_gt.jpg
Saved label: 192_0_gt.jpg	('na',)
Saved image: ./gt_images_test/images_gt/193_0_gt.jpg
Saved label: 193_0_gt.jpg	('hoa',)
Saved image: ./gt_images_test/images_gt/194_0_gt.jpg
Saved label: 194_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/195_0_gt.jpg
Saved label: 195_0_gt.jpg	('H',)
Saved image: ./gt_images_test/images_gt/196_0_gt.jpg
Saved label: 196_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/197_0_gt.jpg
Saved label: 197_0_gt.jpg	('Thnh',)
Saved image: ./gt_images_test/images_gt/198_0_gt.jpg
Saved label: 198_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/199_0_gt.jpg
Saved label: 199_0_gt.jpg	('Tr',)
Saved image: ./gt_images_test/images_gt/200_0_gt.jpg
Saved label: 200_0_gt.jpg	('y',)
Saved image: ./gt_images_test/images_gt/201_0_gt.jpg
Saved label: 201_0_gt.jpg	('Ngi',)
Saved image: ./gt_images_test/images_gt/202_0_gt.jpg
Saved label: 202_0_gt.jpg	('mng',)
Saved image: ./gt_images_test/images_gt/203_0_gt.jpg
Saved label: 203_0_gt.jpg	('Tr',)
Saved image: ./gt_images_test/images_gt/204_0_gt.jpg
Saved label: 204_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/205_0_gt.jpg
Saved label: 205_0_gt.jpg	('tng',)
Saved image: ./gt_images_test/images_gt/206_0_gt.jpg
Saved label: 206_0_gt.jpg	('lai',)
Saved image: ./gt_images_test/images_gt/207_0_gt.jpg
Saved label: 207_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/208_0_gt.jpg
Saved label: 208_0_gt.jpg	('gng',)
Saved image: ./gt_images_test/images_gt/209_0_gt.jpg
Saved label: 209_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/210_0_gt.jpg
Saved label: 210_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/211_0_gt.jpg
Saved label: 211_0_gt.jpg	('Khp',)
Saved image: ./gt_images_test/images_gt/212_0_gt.jpg
Saved label: 212_0_gt.jpg	('Ho',)
Saved image: ./gt_images_test/images_gt/213_0_gt.jpg
Saved label: 213_0_gt.jpg	('nhc',)
Saved image: ./gt_images_test/images_gt/214_0_gt.jpg
Saved label: 214_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/215_0_gt.jpg
Saved label: 215_0_gt.jpg	('cy',)
Saved image: ./gt_images_test/images_gt/216_0_gt.jpg
Saved label: 216_0_gt.jpg	('an',)
Saved image: ./gt_images_test/images_gt/217_0_gt.jpg
Saved label: 217_0_gt.jpg	('u',)
Saved image: ./gt_images_test/images_gt/218_0_gt.jpg
Saved label: 218_0_gt.jpg	('ta',)
Saved image: ./gt_images_test/images_gt/219_0_gt.jpg
Saved label: 219_0_gt.jpg	('k',)
Saved image: ./gt_images_test/images_gt/220_0_gt.jpg
Saved label: 220_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/221_0_gt.jpg
Saved label: 221_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/222_0_gt.jpg
Saved label: 222_0_gt.jpg	('dy',)
Saved image: ./gt_images_test/images_gt/223_0_gt.jpg
Saved label: 223_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/224_0_gt.jpg
Saved label: 224_0_gt.jpg	('tc',)
Saved image: ./gt_images_test/images_gt/225_0_gt.jpg
Saved label: 225_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/226_0_gt.jpg
Saved label: 226_0_gt.jpg	('thc',)
Saved image: ./gt_images_test/images_gt/227_0_gt.jpg
Saved label: 227_0_gt.jpg	('Lc',)
Saved image: ./gt_images_test/images_gt/228_0_gt.jpg
Saved label: 228_0_gt.jpg	('ME',)
Saved image: ./gt_images_test/images_gt/229_0_gt.jpg
Saved label: 229_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/230_0_gt.jpg
Saved label: 230_0_gt.jpg	('khua',)
Saved image: ./gt_images_test/images_gt/231_0_gt.jpg
Saved label: 231_0_gt.jpg	('Ci',)
Saved image: ./gt_images_test/images_gt/232_0_gt.jpg
Saved label: 232_0_gt.jpg	('nim',)
Saved image: ./gt_images_test/images_gt/233_0_gt.jpg
Saved label: 233_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/234_0_gt.jpg
Saved label: 234_0_gt.jpg	('bt',)
Saved image: ./gt_images_test/images_gt/235_0_gt.jpg
Saved label: 235_0_gt.jpg	('hn',)
Saved image: ./gt_images_test/images_gt/236_0_gt.jpg
Saved label: 236_0_gt.jpg	('che',)
Saved image: ./gt_images_test/images_gt/237_0_gt.jpg
Saved label: 237_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/238_0_gt.jpg
Saved label: 238_0_gt.jpg	('sang',)
Saved image: ./gt_images_test/images_gt/239_0_gt.jpg
Saved label: 239_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/240_0_gt.jpg
Saved label: 240_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/241_0_gt.jpg
Saved label: 241_0_gt.jpg	('Bun',)
Saved image: ./gt_images_test/images_gt/242_0_gt.jpg
Saved label: 242_0_gt.jpg	('ni',)
Saved image: ./gt_images_test/images_gt/243_0_gt.jpg
Saved label: 243_0_gt.jpg	('vo',)
Saved image: ./gt_images_test/images_gt/244_0_gt.jpg
Saved label: 244_0_gt.jpg	('hng',)
Saved image: ./gt_images_test/images_gt/245_0_gt.jpg
Saved label: 245_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/246_0_gt.jpg
Saved label: 246_0_gt.jpg	('Mt',)
Saved image: ./gt_images_test/images_gt/247_0_gt.jpg
Saved label: 247_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/248_0_gt.jpg
Saved label: 248_0_gt.jpg	('s',)
Saved image: ./gt_images_test/images_gt/249_0_gt.jpg
Saved label: 249_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/250_0_gt.jpg
Saved label: 250_0_gt.jpg	('k',)
Saved image: ./gt_images_test/images_gt/251_0_gt.jpg
Saved label: 251_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/252_0_gt.jpg
Saved label: 252_0_gt.jpg	('gn',)
Saved image: ./gt_images_test/images_gt/253_0_gt.jpg
Saved label: 253_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/254_0_gt.jpg
Saved label: 254_0_gt.jpg	('Sinh',)
Saved image: ./gt_images_test/images_gt/255_0_gt.jpg
Saved label: 255_0_gt.jpg	('Hoa',)
Saved image: ./gt_images_test/images_gt/256_0_gt.jpg
Saved label: 256_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/257_0_gt.jpg
Saved label: 257_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/258_0_gt.jpg
Saved label: 258_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/259_0_gt.jpg
Saved label: 259_0_gt.jpg	('chp',)
Saved image: ./gt_images_test/images_gt/260_0_gt.jpg
Saved label: 260_0_gt.jpg	('thc',)
Saved image: ./gt_images_test/images_gt/261_0_gt.jpg
Saved label: 261_0_gt.jpg	('Tt',)
Saved image: ./gt_images_test/images_gt/262_0_gt.jpg
Saved label: 262_0_gt.jpg	('Tt',)
Saved image: ./gt_images_test/images_gt/263_0_gt.jpg
Saved label: 263_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/264_0_gt.jpg
Saved label: 264_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/265_0_gt.jpg
Saved label: 265_0_gt.jpg	('cnh',)
Saved image: ./gt_images_test/images_gt/266_0_gt.jpg
Saved label: 266_0_gt.jpg	('Cao',)
Saved image: ./gt_images_test/images_gt/267_0_gt.jpg
Saved label: 267_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/268_0_gt.jpg
Saved label: 268_0_gt.jpg	('Sn',)
Saved image: ./gt_images_test/images_gt/269_0_gt.jpg
Saved label: 269_0_gt.jpg	('ta',)
Saved image: ./gt_images_test/images_gt/270_0_gt.jpg
Saved label: 270_0_gt.jpg	('Khm',)
Saved image: ./gt_images_test/images_gt/271_0_gt.jpg
Saved label: 271_0_gt.jpg	('ngo',)
Saved image: ./gt_images_test/images_gt/272_0_gt.jpg
Saved label: 272_0_gt.jpg	('tng',)
Saved image: ./gt_images_test/images_gt/273_0_gt.jpg
Saved label: 273_0_gt.jpg	('khoe',)
Saved image: ./gt_images_test/images_gt/274_0_gt.jpg
Saved label: 274_0_gt.jpg	('by',)
Saved image: ./gt_images_test/images_gt/275_0_gt.jpg
Saved label: 275_0_gt.jpg	('my',)
Saved image: ./gt_images_test/images_gt/276_0_gt.jpg
Saved label: 276_0_gt.jpg	('mnh',)
Saved image: ./gt_images_test/images_gt/277_0_gt.jpg
Saved label: 277_0_gt.jpg	('Hin',)
Saved image: ./gt_images_test/images_gt/278_0_gt.jpg
Saved label: 278_0_gt.jpg	('tin',)
Saved image: ./gt_images_test/images_gt/279_0_gt.jpg
Saved label: 279_0_gt.jpg	('Tp',)
Saved image: ./gt_images_test/images_gt/280_0_gt.jpg
Saved label: 280_0_gt.jpg	('vng',)
Saved image: ./gt_images_test/images_gt/281_0_gt.jpg
Saved label: 281_0_gt.jpg	('ci',)
Saved image: ./gt_images_test/images_gt/282_0_gt.jpg
Saved label: 282_0_gt.jpg	('vinh',)
Saved image: ./gt_images_test/images_gt/283_0_gt.jpg
Saved label: 283_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/284_0_gt.jpg
Saved label: 284_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/285_0_gt.jpg
Saved label: 285_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/286_0_gt.jpg
Saved label: 286_0_gt.jpg	('Nhn',)
Saved image: ./gt_images_test/images_gt/287_0_gt.jpg
Saved label: 287_0_gt.jpg	('Thi',)
Saved image: ./gt_images_test/images_gt/288_0_gt.jpg
Saved label: 288_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/289_0_gt.jpg
Saved label: 289_0_gt.jpg	('Cha',)
Saved image: ./gt_images_test/images_gt/290_0_gt.jpg
Saved label: 290_0_gt.jpg	('vp',)
Saved image: ./gt_images_test/images_gt/291_0_gt.jpg
Saved label: 291_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/292_0_gt.jpg
Saved label: 292_0_gt.jpg	('H',)
Saved image: ./gt_images_test/images_gt/293_0_gt.jpg
Saved label: 293_0_gt.jpg	('nin',)
Saved image: ./gt_images_test/images_gt/294_0_gt.jpg
Saved label: 294_0_gt.jpg	('phi',)
Saved image: ./gt_images_test/images_gt/295_0_gt.jpg
Saved label: 295_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/296_0_gt.jpg
Saved label: 296_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/297_0_gt.jpg
Saved label: 297_0_gt.jpg	('Vin',)
Saved image: ./gt_images_test/images_gt/298_0_gt.jpg
Saved label: 298_0_gt.jpg	('thi',)
Saved image: ./gt_images_test/images_gt/299_0_gt.jpg
Saved label: 299_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/300_0_gt.jpg
Saved label: 300_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/301_0_gt.jpg
Saved label: 301_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/302_0_gt.jpg
Saved label: 302_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/303_0_gt.jpg
Saved label: 303_0_gt.jpg	('rng',)
Saved image: ./gt_images_test/images_gt/304_0_gt.jpg
Saved label: 304_0_gt.jpg	('bng',)
Saved image: ./gt_images_test/images_gt/305_0_gt.jpg
Saved label: 305_0_gt.jpg	('Vn',)
Saved image: ./gt_images_test/images_gt/306_0_gt.jpg
Saved label: 306_0_gt.jpg	('ong',)
Saved image: ./gt_images_test/images_gt/307_0_gt.jpg
Saved label: 307_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/308_0_gt.jpg
Saved label: 308_0_gt.jpg	('Mu',)
Saved image: ./gt_images_test/images_gt/309_0_gt.jpg
Saved label: 309_0_gt.jpg	('Em',)
Saved image: ./gt_images_test/images_gt/310_0_gt.jpg
Saved label: 310_0_gt.jpg	('thi',)
Saved image: ./gt_images_test/images_gt/311_0_gt.jpg
Saved label: 311_0_gt.jpg	('rung',)
Saved image: ./gt_images_test/images_gt/312_0_gt.jpg
Saved label: 312_0_gt.jpg	('Bn',)
Saved image: ./gt_images_test/images_gt/313_0_gt.jpg
Saved label: 313_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/314_0_gt.jpg
Saved label: 314_0_gt.jpg	('Mnh',)
Saved image: ./gt_images_test/images_gt/315_0_gt.jpg
Saved label: 315_0_gt.jpg	('on',)
Saved image: ./gt_images_test/images_gt/316_0_gt.jpg
Saved label: 316_0_gt.jpg	('gng',)
Saved image: ./gt_images_test/images_gt/317_0_gt.jpg
Saved label: 317_0_gt.jpg	('thc',)
Saved image: ./gt_images_test/images_gt/318_0_gt.jpg
Saved label: 318_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/319_0_gt.jpg
Saved label: 319_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/320_0_gt.jpg
Saved label: 320_0_gt.jpg	('Nm',)
Saved image: ./gt_images_test/images_gt/321_0_gt.jpg
Saved label: 321_0_gt.jpg	('Hiu',)
Saved image: ./gt_images_test/images_gt/322_0_gt.jpg
Saved label: 322_0_gt.jpg	('nng',)
Saved image: ./gt_images_test/images_gt/323_0_gt.jpg
Saved label: 323_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/324_0_gt.jpg
Saved label: 324_0_gt.jpg	('mu',)
Saved image: ./gt_images_test/images_gt/325_0_gt.jpg
Saved label: 325_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/326_0_gt.jpg
Saved label: 326_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/327_0_gt.jpg
Saved label: 327_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/328_0_gt.jpg
Saved label: 328_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/329_0_gt.jpg
Saved label: 329_0_gt.jpg	('Vng',)
Saved image: ./gt_images_test/images_gt/330_0_gt.jpg
Saved label: 330_0_gt.jpg	('hn',)
Saved image: ./gt_images_test/images_gt/331_0_gt.jpg
Saved label: 331_0_gt.jpg	('ca',)
Saved image: ./gt_images_test/images_gt/332_0_gt.jpg
Saved label: 332_0_gt.jpg	('sang',)
Saved image: ./gt_images_test/images_gt/333_0_gt.jpg
Saved label: 333_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/334_0_gt.jpg
Saved label: 334_0_gt.jpg	('Tri',)
Saved image: ./gt_images_test/images_gt/335_0_gt.jpg
Saved label: 335_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/336_0_gt.jpg
Saved label: 336_0_gt.jpg	('Nm',)
Saved image: ./gt_images_test/images_gt/337_0_gt.jpg
Saved label: 337_0_gt.jpg	('Tra',)
Saved image: ./gt_images_test/images_gt/338_0_gt.jpg
Saved label: 338_0_gt.jpg	('h',)
Saved image: ./gt_images_test/images_gt/339_0_gt.jpg
Saved label: 339_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/340_0_gt.jpg
Saved label: 340_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/341_0_gt.jpg
Saved label: 341_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/342_0_gt.jpg
Saved label: 342_0_gt.jpg	('Ti',)
Saved image: ./gt_images_test/images_gt/343_0_gt.jpg
Saved label: 343_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/344_0_gt.jpg
Saved label: 344_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/345_0_gt.jpg
Saved label: 345_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/346_0_gt.jpg
Saved label: 346_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/347_0_gt.jpg
Saved label: 347_0_gt.jpg	('Sa',)
Saved image: ./gt_images_test/images_gt/348_0_gt.jpg
Saved label: 348_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/349_0_gt.jpg
Saved label: 349_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/350_0_gt.jpg
Saved label: 350_0_gt.jpg	('x',)
Saved image: ./gt_images_test/images_gt/351_0_gt.jpg
Saved label: 351_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/352_0_gt.jpg
Saved label: 352_0_gt.jpg	('oan',)
Saved image: ./gt_images_test/images_gt/353_0_gt.jpg
Saved label: 353_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/354_0_gt.jpg
Saved label: 354_0_gt.jpg	('nn',)
Saved image: ./gt_images_test/images_gt/355_0_gt.jpg
Saved label: 355_0_gt.jpg	('V',)
Saved image: ./gt_images_test/images_gt/356_0_gt.jpg
Saved label: 356_0_gt.jpg	('thi',)
Saved image: ./gt_images_test/images_gt/357_0_gt.jpg
Saved label: 357_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/358_0_gt.jpg
Saved label: 358_0_gt.jpg	('mnh',)
Saved image: ./gt_images_test/images_gt/359_0_gt.jpg
Saved label: 359_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/360_0_gt.jpg
Saved label: 360_0_gt.jpg	('Phng',)
Saved image: ./gt_images_test/images_gt/361_0_gt.jpg
Saved label: 361_0_gt.jpg	('Ngu',)
Saved image: ./gt_images_test/images_gt/362_0_gt.jpg
Saved label: 362_0_gt.jpg	('an',)
Saved image: ./gt_images_test/images_gt/363_0_gt.jpg
Saved label: 363_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/364_0_gt.jpg
Saved label: 364_0_gt.jpg	('Song',)
Saved image: ./gt_images_test/images_gt/365_0_gt.jpg
Saved label: 365_0_gt.jpg	('Tt',)
Saved image: ./gt_images_test/images_gt/366_0_gt.jpg
Saved label: 366_0_gt.jpg	('quang',)
Saved image: ./gt_images_test/images_gt/367_0_gt.jpg
Saved label: 367_0_gt.jpg	('Thy',)
Saved image: ./gt_images_test/images_gt/368_0_gt.jpg
Saved label: 368_0_gt.jpg	('nc',)
Saved image: ./gt_images_test/images_gt/369_0_gt.jpg
Saved label: 369_0_gt.jpg	('thi',)
Saved image: ./gt_images_test/images_gt/370_0_gt.jpg
Saved label: 370_0_gt.jpg	('tha',)
Saved image: ./gt_images_test/images_gt/371_0_gt.jpg
Saved label: 371_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/372_0_gt.jpg
Saved label: 372_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/373_0_gt.jpg
Saved label: 373_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/374_0_gt.jpg
Saved label: 374_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/375_0_gt.jpg
Saved label: 375_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/376_0_gt.jpg
Saved label: 376_0_gt.jpg	('Vinh',)
Saved image: ./gt_images_test/images_gt/377_0_gt.jpg
Saved label: 377_0_gt.jpg	('Thi',)
Saved image: ./gt_images_test/images_gt/378_0_gt.jpg
Saved label: 378_0_gt.jpg	('a',)
Saved image: ./gt_images_test/images_gt/379_0_gt.jpg
Saved label: 379_0_gt.jpg	('tu',)
Saved image: ./gt_images_test/images_gt/380_0_gt.jpg
Saved label: 380_0_gt.jpg	('ty',)
Saved image: ./gt_images_test/images_gt/381_0_gt.jpg
Saved label: 381_0_gt.jpg	('Ch',)
Saved image: ./gt_images_test/images_gt/382_0_gt.jpg
Saved label: 382_0_gt.jpg	('Hng',)
Saved image: ./gt_images_test/images_gt/383_0_gt.jpg
Saved label: 383_0_gt.jpg	('hin',)
Saved image: ./gt_images_test/images_gt/384_0_gt.jpg
Saved label: 384_0_gt.jpg	('Tinh',)
Saved image: ./gt_images_test/images_gt/385_0_gt.jpg
Saved label: 385_0_gt.jpg	('la',)
Saved image: ./gt_images_test/images_gt/386_0_gt.jpg
Saved label: 386_0_gt.jpg	('trong',)
Saved image: ./gt_images_test/images_gt/387_0_gt.jpg
Saved label: 387_0_gt.jpg	('Bn',)
Saved image: ./gt_images_test/images_gt/388_0_gt.jpg
Saved label: 388_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/389_0_gt.jpg
Saved label: 389_0_gt.jpg	('Uy',)
Saved image: ./gt_images_test/images_gt/390_0_gt.jpg
Saved label: 390_0_gt.jpg	('tha',)
Saved image: ./gt_images_test/images_gt/391_0_gt.jpg
Saved label: 391_0_gt.jpg	('N',)
Saved image: ./gt_images_test/images_gt/392_0_gt.jpg
Saved label: 392_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/393_0_gt.jpg
Saved label: 393_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/394_0_gt.jpg
Saved label: 394_0_gt.jpg	('qua',)
Saved image: ./gt_images_test/images_gt/395_0_gt.jpg
Saved label: 395_0_gt.jpg	('Dnh',)
Saved image: ./gt_images_test/images_gt/396_0_gt.jpg
Saved label: 396_0_gt.jpg	('Tu',)
Saved image: ./gt_images_test/images_gt/397_0_gt.jpg
Saved label: 397_0_gt.jpg	('vi',)
Saved image: ./gt_images_test/images_gt/398_0_gt.jpg
Saved label: 398_0_gt.jpg	('ln',)
Saved image: ./gt_images_test/images_gt/399_0_gt.jpg
Saved label: 399_0_gt.jpg	('kia',)
Saved image: ./gt_images_test/images_gt/400_0_gt.jpg
Saved label: 400_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/401_0_gt.jpg
Saved label: 401_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/402_0_gt.jpg
Saved label: 402_0_gt.jpg	('Nh',)
Saved image: ./gt_images_test/images_gt/403_0_gt.jpg
Saved label: 403_0_gt.jpg	('nghe',)
Saved image: ./gt_images_test/images_gt/404_0_gt.jpg
Saved label: 404_0_gt.jpg	('Kh',)
Saved image: ./gt_images_test/images_gt/405_0_gt.jpg
Saved label: 405_0_gt.jpg	('rng',)
Saved image: ./gt_images_test/images_gt/406_0_gt.jpg
Saved label: 406_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/407_0_gt.jpg
Saved label: 407_0_gt.jpg	('hy',)
Saved image: ./gt_images_test/images_gt/408_0_gt.jpg
Saved label: 408_0_gt.jpg	('nhng',)
Saved image: ./gt_images_test/images_gt/409_0_gt.jpg
Saved label: 409_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/410_0_gt.jpg
Saved label: 410_0_gt.jpg	('An',)
Saved image: ./gt_images_test/images_gt/411_0_gt.jpg
Saved label: 411_0_gt.jpg	('nghe',)
Saved image: ./gt_images_test/images_gt/412_0_gt.jpg
Saved label: 412_0_gt.jpg	('Ti',)
Saved image: ./gt_images_test/images_gt/413_0_gt.jpg
Saved label: 413_0_gt.jpg	('Khai',)
Saved image: ./gt_images_test/images_gt/414_0_gt.jpg
Saved label: 414_0_gt.jpg	('Mun',)
Saved image: ./gt_images_test/images_gt/415_0_gt.jpg
Saved label: 415_0_gt.jpg	('Bi',)
Saved image: ./gt_images_test/images_gt/416_0_gt.jpg
Saved label: 416_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/417_0_gt.jpg
Saved label: 417_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/418_0_gt.jpg
Saved label: 418_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/419_0_gt.jpg
Saved label: 419_0_gt.jpg	('my',)
Saved image: ./gt_images_test/images_gt/420_0_gt.jpg
Saved label: 420_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/421_0_gt.jpg
Saved label: 421_0_gt.jpg	('khn',)
Saved image: ./gt_images_test/images_gt/422_0_gt.jpg
Saved label: 422_0_gt.jpg	('Ti',)
Saved image: ./gt_images_test/images_gt/423_0_gt.jpg
Saved label: 423_0_gt.jpg	('vi',)
Saved image: ./gt_images_test/images_gt/424_0_gt.jpg
Saved label: 424_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/425_0_gt.jpg
Saved label: 425_0_gt.jpg	('Vit',)
Saved image: ./gt_images_test/images_gt/426_0_gt.jpg
Saved label: 426_0_gt.jpg	('ung',)
Saved image: ./gt_images_test/images_gt/427_0_gt.jpg
Saved label: 427_0_gt.jpg	('xm',)
Saved image: ./gt_images_test/images_gt/428_0_gt.jpg
Saved label: 428_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/429_0_gt.jpg
Saved label: 429_0_gt.jpg	('khi',)
Saved image: ./gt_images_test/images_gt/430_0_gt.jpg
Saved label: 430_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/431_0_gt.jpg
Saved label: 431_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/432_0_gt.jpg
Saved label: 432_0_gt.jpg	('Hy',)
Saved image: ./gt_images_test/images_gt/433_0_gt.jpg
Saved label: 433_0_gt.jpg	('Li',)
Saved image: ./gt_images_test/images_gt/434_0_gt.jpg
Saved label: 434_0_gt.jpg	('Thi',)
Saved image: ./gt_images_test/images_gt/435_0_gt.jpg
Saved label: 435_0_gt.jpg	('qung',)
Saved image: ./gt_images_test/images_gt/436_0_gt.jpg
Saved label: 436_0_gt.jpg	('xanh',)
Saved image: ./gt_images_test/images_gt/437_0_gt.jpg
Saved label: 437_0_gt.jpg	('bn',)
Saved image: ./gt_images_test/images_gt/438_0_gt.jpg
Saved label: 438_0_gt.jpg	('u',)
Saved image: ./gt_images_test/images_gt/439_0_gt.jpg
Saved label: 439_0_gt.jpg	('MI',)
Saved image: ./gt_images_test/images_gt/440_0_gt.jpg
Saved label: 440_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/441_0_gt.jpg
Saved label: 441_0_gt.jpg	('chnh',)
Saved image: ./gt_images_test/images_gt/442_0_gt.jpg
Saved label: 442_0_gt.jpg	('Li',)
Saved image: ./gt_images_test/images_gt/443_0_gt.jpg
Saved label: 443_0_gt.jpg	('thua',)
Saved image: ./gt_images_test/images_gt/444_0_gt.jpg
Saved label: 444_0_gt.jpg	('Kh',)
Saved image: ./gt_images_test/images_gt/445_0_gt.jpg
Saved label: 445_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/446_0_gt.jpg
Saved label: 446_0_gt.jpg	('ging',)
Saved image: ./gt_images_test/images_gt/447_0_gt.jpg
Saved label: 447_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/448_0_gt.jpg
Saved label: 448_0_gt.jpg	('x',)
Saved image: ./gt_images_test/images_gt/449_0_gt.jpg
Saved label: 449_0_gt.jpg	('Vinh',)
Saved image: ./gt_images_test/images_gt/450_0_gt.jpg
Saved label: 450_0_gt.jpg	('Nguyn',)
Saved image: ./gt_images_test/images_gt/451_0_gt.jpg
Saved label: 451_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/452_0_gt.jpg
Saved label: 452_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/453_0_gt.jpg
Saved label: 453_0_gt.jpg	('d',)
Saved image: ./gt_images_test/images_gt/454_0_gt.jpg
Saved label: 454_0_gt.jpg	('thu',)
Saved image: ./gt_images_test/images_gt/455_0_gt.jpg
Saved label: 455_0_gt.jpg	('nin',)
Saved image: ./gt_images_test/images_gt/456_0_gt.jpg
Saved label: 456_0_gt.jpg	('Khng',)
Saved image: ./gt_images_test/images_gt/457_0_gt.jpg
Saved label: 457_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/458_0_gt.jpg
Saved label: 458_0_gt.jpg	('sen',)
Saved image: ./gt_images_test/images_gt/459_0_gt.jpg
Saved label: 459_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/460_0_gt.jpg
Saved label: 460_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/461_0_gt.jpg
Saved label: 461_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/462_0_gt.jpg
Saved label: 462_0_gt.jpg	('mng',)
Saved image: ./gt_images_test/images_gt/463_0_gt.jpg
Saved label: 463_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/464_0_gt.jpg
Saved label: 464_0_gt.jpg	('sao',)
Saved image: ./gt_images_test/images_gt/465_0_gt.jpg
Saved label: 465_0_gt.jpg	('xuyn',)
Saved image: ./gt_images_test/images_gt/466_0_gt.jpg
Saved label: 466_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/467_0_gt.jpg
Saved label: 467_0_gt.jpg	('Chc',)
Saved image: ./gt_images_test/images_gt/468_0_gt.jpg
Saved label: 468_0_gt.jpg	('Thng',)
Saved image: ./gt_images_test/images_gt/469_0_gt.jpg
Saved label: 469_0_gt.jpg	('cao',)
Saved image: ./gt_images_test/images_gt/470_0_gt.jpg
Saved label: 470_0_gt.jpg	('Pht',)
Saved image: ./gt_images_test/images_gt/471_0_gt.jpg
Saved label: 471_0_gt.jpg	('B',)
Saved image: ./gt_images_test/images_gt/472_0_gt.jpg
Saved label: 472_0_gt.jpg	('Nh',)
Saved image: ./gt_images_test/images_gt/473_0_gt.jpg
Saved label: 473_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/474_0_gt.jpg
Saved label: 474_0_gt.jpg	('chi',)
Saved image: ./gt_images_test/images_gt/475_0_gt.jpg
Saved label: 475_0_gt.jpg	('Kh',)
Saved image: ./gt_images_test/images_gt/476_0_gt.jpg
Saved label: 476_0_gt.jpg	('Kin',)
Saved image: ./gt_images_test/images_gt/477_0_gt.jpg
Saved label: 477_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/478_0_gt.jpg
Saved label: 478_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/479_0_gt.jpg
Saved label: 479_0_gt.jpg	('Hin',)
Saved image: ./gt_images_test/images_gt/480_0_gt.jpg
Saved label: 480_0_gt.jpg	('duynh',)
Saved image: ./gt_images_test/images_gt/481_0_gt.jpg
Saved label: 481_0_gt.jpg	('hai',)
Saved image: ./gt_images_test/images_gt/482_0_gt.jpg
Saved label: 482_0_gt.jpg	('Ta',)
Saved image: ./gt_images_test/images_gt/483_0_gt.jpg
Saved label: 483_0_gt.jpg	('o',)
Saved image: ./gt_images_test/images_gt/484_0_gt.jpg
Saved label: 484_0_gt.jpg	('gia',)
Saved image: ./gt_images_test/images_gt/485_0_gt.jpg
Saved label: 485_0_gt.jpg	('thm',)
Saved image: ./gt_images_test/images_gt/486_0_gt.jpg
Saved label: 486_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/487_0_gt.jpg
Saved label: 487_0_gt.jpg	('Thu',)
Saved image: ./gt_images_test/images_gt/488_0_gt.jpg
Saved label: 488_0_gt.jpg	('tim',)
Saved image: ./gt_images_test/images_gt/489_0_gt.jpg
Saved label: 489_0_gt.jpg	('qun',)
Saved image: ./gt_images_test/images_gt/490_0_gt.jpg
Saved label: 490_0_gt.jpg	('gia',)
Saved image: ./gt_images_test/images_gt/491_0_gt.jpg
Saved label: 491_0_gt.jpg	('ct',)
Saved image: ./gt_images_test/images_gt/492_0_gt.jpg
Saved label: 492_0_gt.jpg	('vng',)
Saved image: ./gt_images_test/images_gt/493_0_gt.jpg
Saved label: 493_0_gt.jpg	('nn',)
Saved image: ./gt_images_test/images_gt/494_0_gt.jpg
Saved label: 494_0_gt.jpg	('cui',)
Saved image: ./gt_images_test/images_gt/495_0_gt.jpg
Saved label: 495_0_gt.jpg	('khng',)
Saved image: ./gt_images_test/images_gt/496_0_gt.jpg
Saved label: 496_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/497_0_gt.jpg
Saved label: 497_0_gt.jpg	('Ho',)
Saved image: ./gt_images_test/images_gt/498_0_gt.jpg
Saved label: 498_0_gt.jpg	('Cng',)
Saved image: ./gt_images_test/images_gt/499_0_gt.jpg
Saved label: 499_0_gt.jpg	('tng',)
Saved image: ./gt_images_test/images_gt/500_0_gt.jpg
Saved label: 500_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/501_0_gt.jpg
Saved label: 501_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/502_0_gt.jpg
Saved label: 502_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/503_0_gt.jpg
Saved label: 503_0_gt.jpg	('Thun',)
Saved image: ./gt_images_test/images_gt/504_0_gt.jpg
Saved label: 504_0_gt.jpg	('Hm',)
Saved image: ./gt_images_test/images_gt/505_0_gt.jpg
Saved label: 505_0_gt.jpg	('Chng',)
Saved image: ./gt_images_test/images_gt/506_0_gt.jpg
Saved label: 506_0_gt.jpg	('tha',)
Saved image: ./gt_images_test/images_gt/507_0_gt.jpg
Saved label: 507_0_gt.jpg	('tinh',)
Saved image: ./gt_images_test/images_gt/508_0_gt.jpg
Saved label: 508_0_gt.jpg	('K',)
Saved image: ./gt_images_test/images_gt/509_0_gt.jpg
Saved label: 509_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/510_0_gt.jpg
Saved label: 510_0_gt.jpg	('cc',)
Saved image: ./gt_images_test/images_gt/511_0_gt.jpg
Saved label: 511_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/512_0_gt.jpg
Saved label: 512_0_gt.jpg	('tin',)
Saved image: ./gt_images_test/images_gt/513_0_gt.jpg
Saved label: 513_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/514_0_gt.jpg
Saved label: 514_0_gt.jpg	('Em',)
Saved image: ./gt_images_test/images_gt/515_0_gt.jpg
Saved label: 515_0_gt.jpg	('Phn',)
Saved image: ./gt_images_test/images_gt/516_0_gt.jpg
Saved label: 516_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/517_0_gt.jpg
Saved label: 517_0_gt.jpg	('khng',)
Saved image: ./gt_images_test/images_gt/518_0_gt.jpg
Saved label: 518_0_gt.jpg	('L',)
Saved image: ./gt_images_test/images_gt/519_0_gt.jpg
Saved label: 519_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/520_0_gt.jpg
Saved label: 520_0_gt.jpg	('ca',)
Saved image: ./gt_images_test/images_gt/521_0_gt.jpg
Saved label: 521_0_gt.jpg	('thi',)
Saved image: ./gt_images_test/images_gt/522_0_gt.jpg
Saved label: 522_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/523_0_gt.jpg
Saved label: 523_0_gt.jpg	('Hnh',)
Saved image: ./gt_images_test/images_gt/524_0_gt.jpg
Saved label: 524_0_gt.jpg	('ph',)
Saved image: ./gt_images_test/images_gt/525_0_gt.jpg
Saved label: 525_0_gt.jpg	('ngn',)
Saved image: ./gt_images_test/images_gt/526_0_gt.jpg
Saved label: 526_0_gt.jpg	('o',)
Saved image: ./gt_images_test/images_gt/527_0_gt.jpg
Saved label: 527_0_gt.jpg	('a',)
Saved image: ./gt_images_test/images_gt/528_0_gt.jpg
Saved label: 528_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/529_0_gt.jpg
Saved label: 529_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/530_0_gt.jpg
Saved label: 530_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/531_0_gt.jpg
Saved label: 531_0_gt.jpg	('ph',)
Saved image: ./gt_images_test/images_gt/532_0_gt.jpg
Saved label: 532_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/533_0_gt.jpg
Saved label: 533_0_gt.jpg	('qung',)
Saved image: ./gt_images_test/images_gt/534_0_gt.jpg
Saved label: 534_0_gt.jpg	('li',)
Saved image: ./gt_images_test/images_gt/535_0_gt.jpg
Saved label: 535_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/536_0_gt.jpg
Saved label: 536_0_gt.jpg	('gi',)
Saved image: ./gt_images_test/images_gt/537_0_gt.jpg
Saved label: 537_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/538_0_gt.jpg
Saved label: 538_0_gt.jpg	('say',)
Saved image: ./gt_images_test/images_gt/539_0_gt.jpg
Saved label: 539_0_gt.jpg	('Mi',)
Saved image: ./gt_images_test/images_gt/540_0_gt.jpg
Saved label: 540_0_gt.jpg	('gia',)
Saved image: ./gt_images_test/images_gt/541_0_gt.jpg
Saved label: 541_0_gt.jpg	('gia',)
Saved image: ./gt_images_test/images_gt/542_0_gt.jpg
Saved label: 542_0_gt.jpg	('g',)
Saved image: ./gt_images_test/images_gt/543_0_gt.jpg
Saved label: 543_0_gt.jpg	('khong',)
Saved image: ./gt_images_test/images_gt/544_0_gt.jpg
Saved label: 544_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/545_0_gt.jpg
Saved label: 545_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/546_0_gt.jpg
Saved label: 546_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/547_0_gt.jpg
Saved label: 547_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/548_0_gt.jpg
Saved label: 548_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/549_0_gt.jpg
Saved label: 549_0_gt.jpg	('3',)
Saved image: ./gt_images_test/images_gt/550_0_gt.jpg
Saved label: 550_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/551_0_gt.jpg
Saved label: 551_0_gt.jpg	('Chi',)
Saved image: ./gt_images_test/images_gt/552_0_gt.jpg
Saved label: 552_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/553_0_gt.jpg
Saved label: 553_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/554_0_gt.jpg
Saved label: 554_0_gt.jpg	('ca',)
Saved image: ./gt_images_test/images_gt/555_0_gt.jpg
Saved label: 555_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/556_0_gt.jpg
Saved label: 556_0_gt.jpg	('nm',)
Saved image: ./gt_images_test/images_gt/557_0_gt.jpg
Saved label: 557_0_gt.jpg	('Mc',)
Saved image: ./gt_images_test/images_gt/558_0_gt.jpg
Saved label: 558_0_gt.jpg	('Ho',)
Saved image: ./gt_images_test/images_gt/559_0_gt.jpg
Saved label: 559_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/560_0_gt.jpg
Saved label: 560_0_gt.jpg	('bng',)
Saved image: ./gt_images_test/images_gt/561_0_gt.jpg
Saved label: 561_0_gt.jpg	('But',)
Saved image: ./gt_images_test/images_gt/562_0_gt.jpg
Saved label: 562_0_gt.jpg	('L',)
Saved image: ./gt_images_test/images_gt/563_0_gt.jpg
Saved label: 563_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/564_0_gt.jpg
Saved label: 564_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/565_0_gt.jpg
Saved label: 565_0_gt.jpg	('Nhn',)
Saved image: ./gt_images_test/images_gt/566_0_gt.jpg
Saved label: 566_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/567_0_gt.jpg
Saved label: 567_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/568_0_gt.jpg
Saved label: 568_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/569_0_gt.jpg
Saved label: 569_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/570_0_gt.jpg
Saved label: 570_0_gt.jpg	('cha',)
Saved image: ./gt_images_test/images_gt/571_0_gt.jpg
Saved label: 571_0_gt.jpg	('yn',)
Saved image: ./gt_images_test/images_gt/572_0_gt.jpg
Saved label: 572_0_gt.jpg	('say',)
Saved image: ./gt_images_test/images_gt/573_0_gt.jpg
Saved label: 573_0_gt.jpg	('Lng',)
Saved image: ./gt_images_test/images_gt/574_0_gt.jpg
Saved label: 574_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/575_0_gt.jpg
Saved label: 575_0_gt.jpg	('ung',)
Saved image: ./gt_images_test/images_gt/576_0_gt.jpg
Saved label: 576_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/577_0_gt.jpg
Saved label: 577_0_gt.jpg	('cnh',)
Saved image: ./gt_images_test/images_gt/578_0_gt.jpg
Saved label: 578_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/579_0_gt.jpg
Saved label: 579_0_gt.jpg	('Tu',)
Saved image: ./gt_images_test/images_gt/580_0_gt.jpg
Saved label: 580_0_gt.jpg	('ting',)
Saved image: ./gt_images_test/images_gt/581_0_gt.jpg
Saved label: 581_0_gt.jpg	('Ct',)
Saved image: ./gt_images_test/images_gt/582_0_gt.jpg
Saved label: 582_0_gt.jpg	('cn',)
Saved image: ./gt_images_test/images_gt/583_0_gt.jpg
Saved label: 583_0_gt.jpg	('bit',)
Saved image: ./gt_images_test/images_gt/584_0_gt.jpg
Saved label: 584_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/585_0_gt.jpg
Saved label: 585_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/586_0_gt.jpg
Saved label: 586_0_gt.jpg	('Tin',)
Saved image: ./gt_images_test/images_gt/587_0_gt.jpg
Saved label: 587_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/588_0_gt.jpg
Saved label: 588_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/589_0_gt.jpg
Saved label: 589_0_gt.jpg	('tng',)
Saved image: ./gt_images_test/images_gt/590_0_gt.jpg
Saved label: 590_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/591_0_gt.jpg
Saved label: 591_0_gt.jpg	('Khng',)
Saved image: ./gt_images_test/images_gt/592_0_gt.jpg
Saved label: 592_0_gt.jpg	('Khc',)
Saved image: ./gt_images_test/images_gt/593_0_gt.jpg
Saved label: 593_0_gt.jpg	('Nguyn',)
Saved image: ./gt_images_test/images_gt/594_0_gt.jpg
Saved label: 594_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/595_0_gt.jpg
Saved label: 595_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/596_0_gt.jpg
Saved label: 596_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/597_0_gt.jpg
Saved label: 597_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/598_0_gt.jpg
Saved label: 598_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/599_0_gt.jpg
Saved label: 599_0_gt.jpg	('ngy',)
Saved image: ./gt_images_test/images_gt/600_0_gt.jpg
Saved label: 600_0_gt.jpg	('sm',)
Saved image: ./gt_images_test/images_gt/601_0_gt.jpg
Saved label: 601_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/602_0_gt.jpg
Saved label: 602_0_gt.jpg	('xa',)
Saved image: ./gt_images_test/images_gt/603_0_gt.jpg
Saved label: 603_0_gt.jpg	('bn',)
Saved image: ./gt_images_test/images_gt/604_0_gt.jpg
Saved label: 604_0_gt.jpg	('Pht',)
Saved image: ./gt_images_test/images_gt/605_0_gt.jpg
Saved label: 605_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/606_0_gt.jpg
Saved label: 606_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/607_0_gt.jpg
Saved label: 607_0_gt.jpg	('Nhn',)
Saved image: ./gt_images_test/images_gt/608_0_gt.jpg
Saved label: 608_0_gt.jpg	('ngh',)
Saved image: ./gt_images_test/images_gt/609_0_gt.jpg
Saved label: 609_0_gt.jpg	('ci',)
Saved image: ./gt_images_test/images_gt/610_0_gt.jpg
Saved label: 610_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/611_0_gt.jpg
Saved label: 611_0_gt.jpg	('nhc',)
Saved image: ./gt_images_test/images_gt/612_0_gt.jpg
Saved label: 612_0_gt.jpg	('tro',)
Saved image: ./gt_images_test/images_gt/613_0_gt.jpg
Saved label: 613_0_gt.jpg	('Kh',)
Saved image: ./gt_images_test/images_gt/614_0_gt.jpg
Saved label: 614_0_gt.jpg	('Tt',)
Saved image: ./gt_images_test/images_gt/615_0_gt.jpg
Saved label: 615_0_gt.jpg	('S',)
Saved image: ./gt_images_test/images_gt/616_0_gt.jpg
Saved label: 616_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/617_0_gt.jpg
Saved label: 617_0_gt.jpg	('An',)
Saved image: ./gt_images_test/images_gt/618_0_gt.jpg
Saved label: 618_0_gt.jpg	('Lc',)
Saved image: ./gt_images_test/images_gt/619_0_gt.jpg
Saved label: 619_0_gt.jpg	('tu',)
Saved image: ./gt_images_test/images_gt/620_0_gt.jpg
Saved label: 620_0_gt.jpg	('An',)
Saved image: ./gt_images_test/images_gt/621_0_gt.jpg
Saved label: 621_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/622_0_gt.jpg
Saved label: 622_0_gt.jpg	('gng',)
Saved image: ./gt_images_test/images_gt/623_0_gt.jpg
Saved label: 623_0_gt.jpg	('An',)
Saved image: ./gt_images_test/images_gt/624_0_gt.jpg
Saved label: 624_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/625_0_gt.jpg
Saved label: 625_0_gt.jpg	('gian',)
Saved image: ./gt_images_test/images_gt/626_0_gt.jpg
Saved label: 626_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/627_0_gt.jpg
Saved label: 627_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/628_0_gt.jpg
Saved label: 628_0_gt.jpg	('Trn',)
Saved image: ./gt_images_test/images_gt/629_0_gt.jpg
Saved label: 629_0_gt.jpg	('Snh',)
Saved image: ./gt_images_test/images_gt/630_0_gt.jpg
Saved label: 630_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/631_0_gt.jpg
Saved label: 631_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/632_0_gt.jpg
Saved label: 632_0_gt.jpg	('Bn',)
Saved image: ./gt_images_test/images_gt/633_0_gt.jpg
Saved label: 633_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/634_0_gt.jpg
Saved label: 634_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/635_0_gt.jpg
Saved label: 635_0_gt.jpg	('Ng',)
Saved image: ./gt_images_test/images_gt/636_0_gt.jpg
Saved label: 636_0_gt.jpg	('diu',)
Saved image: ./gt_images_test/images_gt/637_0_gt.jpg
Saved label: 637_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/638_0_gt.jpg
Saved label: 638_0_gt.jpg	('lm',)
Saved image: ./gt_images_test/images_gt/639_0_gt.jpg
Saved label: 639_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/640_0_gt.jpg
Saved label: 640_0_gt.jpg	('Hiu',)
Saved image: ./gt_images_test/images_gt/641_0_gt.jpg
Saved label: 641_0_gt.jpg	('hoi',)
Saved image: ./gt_images_test/images_gt/642_0_gt.jpg
Saved label: 642_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/643_0_gt.jpg
Saved label: 643_0_gt.jpg	('ma',)
Saved image: ./gt_images_test/images_gt/644_0_gt.jpg
Saved label: 644_0_gt.jpg	('o',)
Saved image: ./gt_images_test/images_gt/645_0_gt.jpg
Saved label: 645_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/646_0_gt.jpg
Saved label: 646_0_gt.jpg	('nhng',)
Saved image: ./gt_images_test/images_gt/647_0_gt.jpg
Saved label: 647_0_gt.jpg	('con',)
Saved image: ./gt_images_test/images_gt/648_0_gt.jpg
Saved label: 648_0_gt.jpg	('li',)
Saved image: ./gt_images_test/images_gt/649_0_gt.jpg
Saved label: 649_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/650_0_gt.jpg
Saved label: 650_0_gt.jpg	('ngha',)
Saved image: ./gt_images_test/images_gt/651_0_gt.jpg
Saved label: 651_0_gt.jpg	('Vinh',)
Saved image: ./gt_images_test/images_gt/652_0_gt.jpg
Saved label: 652_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/653_0_gt.jpg
Saved label: 653_0_gt.jpg	('no',)
Saved image: ./gt_images_test/images_gt/654_0_gt.jpg
Saved label: 654_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/655_0_gt.jpg
Saved label: 655_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/656_0_gt.jpg
Saved label: 656_0_gt.jpg	('thi',)
Saved image: ./gt_images_test/images_gt/657_0_gt.jpg
Saved label: 657_0_gt.jpg	('truyn',)
Saved image: ./gt_images_test/images_gt/658_0_gt.jpg
Saved label: 658_0_gt.jpg	('lan',)
Saved image: ./gt_images_test/images_gt/659_0_gt.jpg
Saved label: 659_0_gt.jpg	('o',)
Saved image: ./gt_images_test/images_gt/660_0_gt.jpg
Saved label: 660_0_gt.jpg	('N',)
Saved image: ./gt_images_test/images_gt/661_0_gt.jpg
Saved label: 661_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/662_0_gt.jpg
Saved label: 662_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/663_0_gt.jpg
Saved label: 663_0_gt.jpg	('yn',)
Saved image: ./gt_images_test/images_gt/664_0_gt.jpg
Saved label: 664_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/665_0_gt.jpg
Saved label: 665_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/666_0_gt.jpg
Saved label: 666_0_gt.jpg	('ci',)
Saved image: ./gt_images_test/images_gt/667_0_gt.jpg
Saved label: 667_0_gt.jpg	('Khan',)
Saved image: ./gt_images_test/images_gt/668_0_gt.jpg
Saved label: 668_0_gt.jpg	('ngt',)
Saved image: ./gt_images_test/images_gt/669_0_gt.jpg
Saved label: 669_0_gt.jpg	('quc',)
Saved image: ./gt_images_test/images_gt/670_0_gt.jpg
Saved label: 670_0_gt.jpg	('Vinh',)
Saved image: ./gt_images_test/images_gt/671_0_gt.jpg
Saved label: 671_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/672_0_gt.jpg
Saved label: 672_0_gt.jpg	('Hn',)
Saved image: ./gt_images_test/images_gt/673_0_gt.jpg
Saved label: 673_0_gt.jpg	('a',)
Saved image: ./gt_images_test/images_gt/674_0_gt.jpg
Saved label: 674_0_gt.jpg	('ph',)
Saved image: ./gt_images_test/images_gt/675_0_gt.jpg
Saved label: 675_0_gt.jpg	('Bnh',)
Saved image: ./gt_images_test/images_gt/676_0_gt.jpg
Saved label: 676_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/677_0_gt.jpg
Saved label: 677_0_gt.jpg	('di',)
Saved image: ./gt_images_test/images_gt/678_0_gt.jpg
Saved label: 678_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/679_0_gt.jpg
Saved label: 679_0_gt.jpg	('Ngha',)
Saved image: ./gt_images_test/images_gt/680_0_gt.jpg
Saved label: 680_0_gt.jpg	('ngp',)
Saved image: ./gt_images_test/images_gt/681_0_gt.jpg
Saved label: 681_0_gt.jpg	('chiu',)
Saved image: ./gt_images_test/images_gt/682_0_gt.jpg
Saved label: 682_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/683_0_gt.jpg
Saved label: 683_0_gt.jpg	('Thi',)
Saved image: ./gt_images_test/images_gt/684_0_gt.jpg
Saved label: 684_0_gt.jpg	('By',)
Saved image: ./gt_images_test/images_gt/685_0_gt.jpg
Saved label: 685_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/686_0_gt.jpg
Saved label: 686_0_gt.jpg	('Thin',)
Saved image: ./gt_images_test/images_gt/687_0_gt.jpg
Saved label: 687_0_gt.jpg	('thin',)
Saved image: ./gt_images_test/images_gt/688_0_gt.jpg
Saved label: 688_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/689_0_gt.jpg
Saved label: 689_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/690_0_gt.jpg
Saved label: 690_0_gt.jpg	('iu',)
Saved image: ./gt_images_test/images_gt/691_0_gt.jpg
Saved label: 691_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/692_0_gt.jpg
Saved label: 692_0_gt.jpg	('gi',)
Saved image: ./gt_images_test/images_gt/693_0_gt.jpg
Saved label: 693_0_gt.jpg	('thn',)
Saved image: ./gt_images_test/images_gt/694_0_gt.jpg
Saved label: 694_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/695_0_gt.jpg
Saved label: 695_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/696_0_gt.jpg
Saved label: 696_0_gt.jpg	('Ti',)
Saved image: ./gt_images_test/images_gt/697_0_gt.jpg
Saved label: 697_0_gt.jpg	('Hng',)
Saved image: ./gt_images_test/images_gt/698_0_gt.jpg
Saved label: 698_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/699_0_gt.jpg
Saved label: 699_0_gt.jpg	('nhng',)
Saved image: ./gt_images_test/images_gt/700_0_gt.jpg
Saved label: 700_0_gt.jpg	('h',)
Saved image: ./gt_images_test/images_gt/701_0_gt.jpg
Saved label: 701_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/702_0_gt.jpg
Saved label: 702_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/703_0_gt.jpg
Saved label: 703_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/704_0_gt.jpg
Saved label: 704_0_gt.jpg	('Hong',)
Saved image: ./gt_images_test/images_gt/705_0_gt.jpg
Saved label: 705_0_gt.jpg	('ci',)
Saved image: ./gt_images_test/images_gt/706_0_gt.jpg
Saved label: 706_0_gt.jpg	('kh',)
Saved image: ./gt_images_test/images_gt/707_0_gt.jpg
Saved label: 707_0_gt.jpg	('no',)
Saved image: ./gt_images_test/images_gt/708_0_gt.jpg
Saved label: 708_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/709_0_gt.jpg
Saved label: 709_0_gt.jpg	('mn',)
Saved image: ./gt_images_test/images_gt/710_0_gt.jpg
Saved label: 710_0_gt.jpg	('ph',)
Saved image: ./gt_images_test/images_gt/711_0_gt.jpg
Saved label: 711_0_gt.jpg	('mng',)
Saved image: ./gt_images_test/images_gt/712_0_gt.jpg
Saved label: 712_0_gt.jpg	('nhnh',)
Saved image: ./gt_images_test/images_gt/713_0_gt.jpg
Saved label: 713_0_gt.jpg	('Vng',)
Saved image: ./gt_images_test/images_gt/714_0_gt.jpg
Saved label: 714_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/715_0_gt.jpg
Saved label: 715_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/716_0_gt.jpg
Saved label: 716_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/717_0_gt.jpg
Saved label: 717_0_gt.jpg	('ai',)
Saved image: ./gt_images_test/images_gt/718_0_gt.jpg
Saved label: 718_0_gt.jpg	('in',)
Saved image: ./gt_images_test/images_gt/719_0_gt.jpg
Saved label: 719_0_gt.jpg	('di',)
Saved image: ./gt_images_test/images_gt/720_0_gt.jpg
Saved label: 720_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/721_0_gt.jpg
Saved label: 721_0_gt.jpg	('y',)
Saved image: ./gt_images_test/images_gt/722_0_gt.jpg
Saved label: 722_0_gt.jpg	('p',)
Saved image: ./gt_images_test/images_gt/723_0_gt.jpg
Saved label: 723_0_gt.jpg	('Ct',)
Saved image: ./gt_images_test/images_gt/724_0_gt.jpg
Saved label: 724_0_gt.jpg	('mong',)
Saved image: ./gt_images_test/images_gt/725_0_gt.jpg
Saved label: 725_0_gt.jpg	('dn',)
Saved image: ./gt_images_test/images_gt/726_0_gt.jpg
Saved label: 726_0_gt.jpg	('Nh',)
Saved image: ./gt_images_test/images_gt/727_0_gt.jpg
Saved label: 727_0_gt.jpg	('phong',)
Saved image: ./gt_images_test/images_gt/728_0_gt.jpg
Saved label: 728_0_gt.jpg	('Nhng',)
Saved image: ./gt_images_test/images_gt/729_0_gt.jpg
Saved label: 729_0_gt.jpg	('ni',)
Saved image: ./gt_images_test/images_gt/730_0_gt.jpg
Saved label: 730_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/731_0_gt.jpg
Saved label: 731_0_gt.jpg	('ph',)
Saved image: ./gt_images_test/images_gt/732_0_gt.jpg
Saved label: 732_0_gt.jpg	('bng',)
Saved image: ./gt_images_test/images_gt/733_0_gt.jpg
Saved label: 733_0_gt.jpg	('d',)
Saved image: ./gt_images_test/images_gt/734_0_gt.jpg
Saved label: 734_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/735_0_gt.jpg
Saved label: 735_0_gt.jpg	('Thnh',)
Saved image: ./gt_images_test/images_gt/736_0_gt.jpg
Saved label: 736_0_gt.jpg	('tc',)
Saved image: ./gt_images_test/images_gt/737_0_gt.jpg
Saved label: 737_0_gt.jpg	('ca',)
Saved image: ./gt_images_test/images_gt/738_0_gt.jpg
Saved label: 738_0_gt.jpg	('mu',)
Saved image: ./gt_images_test/images_gt/739_0_gt.jpg
Saved label: 739_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/740_0_gt.jpg
Saved label: 740_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/741_0_gt.jpg
Saved label: 741_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/742_0_gt.jpg
Saved label: 742_0_gt.jpg	('Tu',)
Saved image: ./gt_images_test/images_gt/743_0_gt.jpg
Saved label: 743_0_gt.jpg	('chng',)
Saved image: ./gt_images_test/images_gt/744_0_gt.jpg
Saved label: 744_0_gt.jpg	('vng',)
Saved image: ./gt_images_test/images_gt/745_0_gt.jpg
Saved label: 745_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/746_0_gt.jpg
Saved label: 746_0_gt.jpg	('Kh',)
Saved image: ./gt_images_test/images_gt/747_0_gt.jpg
Saved label: 747_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/748_0_gt.jpg
Saved label: 748_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/749_0_gt.jpg
Saved label: 749_0_gt.jpg	('cao',)
Saved image: ./gt_images_test/images_gt/750_0_gt.jpg
Saved label: 750_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/751_0_gt.jpg
Saved label: 751_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/752_0_gt.jpg
Saved label: 752_0_gt.jpg	('Khc',)
Saved image: ./gt_images_test/images_gt/753_0_gt.jpg
Saved label: 753_0_gt.jpg	('Lc',)
Saved image: ./gt_images_test/images_gt/754_0_gt.jpg
Saved label: 754_0_gt.jpg	('Tr',)
Saved image: ./gt_images_test/images_gt/755_0_gt.jpg
Saved label: 755_0_gt.jpg	('Trm',)
Saved image: ./gt_images_test/images_gt/756_0_gt.jpg
Saved label: 756_0_gt.jpg	('Ch',)
Saved image: ./gt_images_test/images_gt/757_0_gt.jpg
Saved label: 757_0_gt.jpg	('C',)
Saved image: ./gt_images_test/images_gt/758_0_gt.jpg
Saved label: 758_0_gt.jpg	('gc',)
Saved image: ./gt_images_test/images_gt/759_0_gt.jpg
Saved label: 759_0_gt.jpg	('say',)
Saved image: ./gt_images_test/images_gt/760_0_gt.jpg
Saved label: 760_0_gt.jpg	('p',)
Saved image: ./gt_images_test/images_gt/761_0_gt.jpg
Saved label: 761_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/762_0_gt.jpg
Saved label: 762_0_gt.jpg	('nghe',)
Saved image: ./gt_images_test/images_gt/763_0_gt.jpg
Saved label: 763_0_gt.jpg	('cnh',)
Saved image: ./gt_images_test/images_gt/764_0_gt.jpg
Saved label: 764_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/765_0_gt.jpg
Saved label: 765_0_gt.jpg	('Vng',)
Saved image: ./gt_images_test/images_gt/766_0_gt.jpg
Saved label: 766_0_gt.jpg	('bn',)
Saved image: ./gt_images_test/images_gt/767_0_gt.jpg
Saved label: 767_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/768_0_gt.jpg
Saved label: 768_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/769_0_gt.jpg
Saved label: 769_0_gt.jpg	('C',)
Saved image: ./gt_images_test/images_gt/770_0_gt.jpg
Saved label: 770_0_gt.jpg	('Trng',)
Saved image: ./gt_images_test/images_gt/771_0_gt.jpg
Saved label: 771_0_gt.jpg	('T',)
Saved image: ./gt_images_test/images_gt/772_0_gt.jpg
Saved label: 772_0_gt.jpg	('Tr',)
Saved image: ./gt_images_test/images_gt/773_0_gt.jpg
Saved label: 773_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/774_0_gt.jpg
Saved label: 774_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/775_0_gt.jpg
Saved label: 775_0_gt.jpg	('Ng',)
Saved image: ./gt_images_test/images_gt/776_0_gt.jpg
Saved label: 776_0_gt.jpg	('thnh',)
Saved image: ./gt_images_test/images_gt/777_0_gt.jpg
Saved label: 777_0_gt.jpg	('Pht',)
Saved image: ./gt_images_test/images_gt/778_0_gt.jpg
Saved label: 778_0_gt.jpg	('Cng',)
Saved image: ./gt_images_test/images_gt/779_0_gt.jpg
Saved label: 779_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/780_0_gt.jpg
Saved label: 780_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/781_0_gt.jpg
Saved label: 781_0_gt.jpg	('chuyn',)
Saved image: ./gt_images_test/images_gt/782_0_gt.jpg
Saved label: 782_0_gt.jpg	('Hnh',)
Saved image: ./gt_images_test/images_gt/783_0_gt.jpg
Saved label: 783_0_gt.jpg	('nui',)
Saved image: ./gt_images_test/images_gt/784_0_gt.jpg
Saved label: 784_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/785_0_gt.jpg
Saved label: 785_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/786_0_gt.jpg
Saved label: 786_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/787_0_gt.jpg
Saved label: 787_0_gt.jpg	('thiu',)
Saved image: ./gt_images_test/images_gt/788_0_gt.jpg
Saved label: 788_0_gt.jpg	('Khi',)
Saved image: ./gt_images_test/images_gt/789_0_gt.jpg
Saved label: 789_0_gt.jpg	('Cng',)
Saved image: ./gt_images_test/images_gt/790_0_gt.jpg
Saved label: 790_0_gt.jpg	('Tt',)
Saved image: ./gt_images_test/images_gt/791_0_gt.jpg
Saved label: 791_0_gt.jpg	('Tu',)
Saved image: ./gt_images_test/images_gt/792_0_gt.jpg
Saved label: 792_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/793_0_gt.jpg
Saved label: 793_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/794_0_gt.jpg
Saved label: 794_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/795_0_gt.jpg
Saved label: 795_0_gt.jpg	('Tt',)
Saved image: ./gt_images_test/images_gt/796_0_gt.jpg
Saved label: 796_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/797_0_gt.jpg
Saved label: 797_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/798_0_gt.jpg
Saved label: 798_0_gt.jpg	('hp',)
Saved image: ./gt_images_test/images_gt/799_0_gt.jpg
Saved label: 799_0_gt.jpg	('cho',)
Saved image: ./gt_images_test/images_gt/800_0_gt.jpg
Saved label: 800_0_gt.jpg	('Vui',)
Saved image: ./gt_images_test/images_gt/801_0_gt.jpg
Saved label: 801_0_gt.jpg	('bay',)
Saved image: ./gt_images_test/images_gt/802_0_gt.jpg
Saved label: 802_0_gt.jpg	('cu',)
Saved image: ./gt_images_test/images_gt/803_0_gt.jpg
Saved label: 803_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/804_0_gt.jpg
Saved label: 804_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/805_0_gt.jpg
Saved label: 805_0_gt.jpg	('ci',)
Saved image: ./gt_images_test/images_gt/806_0_gt.jpg
Saved label: 806_0_gt.jpg	('bnh',)
Saved image: ./gt_images_test/images_gt/807_0_gt.jpg
Saved label: 807_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/808_0_gt.jpg
Saved label: 808_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/809_0_gt.jpg
Saved label: 809_0_gt.jpg	('phn',)
Saved image: ./gt_images_test/images_gt/810_0_gt.jpg
Saved label: 810_0_gt.jpg	('nhiu',)
Saved image: ./gt_images_test/images_gt/811_0_gt.jpg
Saved label: 811_0_gt.jpg	('lm',)
Saved image: ./gt_images_test/images_gt/812_0_gt.jpg
Saved label: 812_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/813_0_gt.jpg
Saved label: 813_0_gt.jpg	('gian',)
Saved image: ./gt_images_test/images_gt/814_0_gt.jpg
Saved label: 814_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/815_0_gt.jpg
Saved label: 815_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/816_0_gt.jpg
Saved label: 816_0_gt.jpg	('Lai',)
Saved image: ./gt_images_test/images_gt/817_0_gt.jpg
Saved label: 817_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/818_0_gt.jpg
Saved label: 818_0_gt.jpg	('khng',)
Saved image: ./gt_images_test/images_gt/819_0_gt.jpg
Saved label: 819_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/820_0_gt.jpg
Saved label: 820_0_gt.jpg	('Thun',)
Saved image: ./gt_images_test/images_gt/821_0_gt.jpg
Saved label: 821_0_gt.jpg	('ni',)
Saved image: ./gt_images_test/images_gt/822_0_gt.jpg
Saved label: 822_0_gt.jpg	('no',)
Saved image: ./gt_images_test/images_gt/823_0_gt.jpg
Saved label: 823_0_gt.jpg	('Ty',)
Saved image: ./gt_images_test/images_gt/824_0_gt.jpg
Saved label: 824_0_gt.jpg	('Pht',)
Saved image: ./gt_images_test/images_gt/825_0_gt.jpg
Saved label: 825_0_gt.jpg	('bnh',)
Saved image: ./gt_images_test/images_gt/826_0_gt.jpg
Saved label: 826_0_gt.jpg	('sinh',)
Saved image: ./gt_images_test/images_gt/827_0_gt.jpg
Saved label: 827_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/828_0_gt.jpg
Saved label: 828_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/829_0_gt.jpg
Saved label: 829_0_gt.jpg	('khi',)
Saved image: ./gt_images_test/images_gt/830_0_gt.jpg
Saved label: 830_0_gt.jpg	('bt',)
Saved image: ./gt_images_test/images_gt/831_0_gt.jpg
Saved label: 831_0_gt.jpg	('bn',)
Saved image: ./gt_images_test/images_gt/832_0_gt.jpg
Saved label: 832_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/833_0_gt.jpg
Saved label: 833_0_gt.jpg	('con',)
Saved image: ./gt_images_test/images_gt/834_0_gt.jpg
Saved label: 834_0_gt.jpg	('hin',)
Saved image: ./gt_images_test/images_gt/835_0_gt.jpg
Saved label: 835_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/836_0_gt.jpg
Saved label: 836_0_gt.jpg	('yn',)
Saved image: ./gt_images_test/images_gt/837_0_gt.jpg
Saved label: 837_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/838_0_gt.jpg
Saved label: 838_0_gt.jpg	('sut',)
Saved image: ./gt_images_test/images_gt/839_0_gt.jpg
Saved label: 839_0_gt.jpg	('dng',)
Saved image: ./gt_images_test/images_gt/840_0_gt.jpg
Saved label: 840_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/841_0_gt.jpg
Saved label: 841_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/842_0_gt.jpg
Saved label: 842_0_gt.jpg	('Nhp',)
Saved image: ./gt_images_test/images_gt/843_0_gt.jpg
Saved label: 843_0_gt.jpg	('y',)
Saved image: ./gt_images_test/images_gt/844_0_gt.jpg
Saved label: 844_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/845_0_gt.jpg
Saved label: 845_0_gt.jpg	('Anh',)
Saved image: ./gt_images_test/images_gt/846_0_gt.jpg
Saved label: 846_0_gt.jpg	('long',)
Saved image: ./gt_images_test/images_gt/847_0_gt.jpg
Saved label: 847_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/848_0_gt.jpg
Saved label: 848_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/849_0_gt.jpg
Saved label: 849_0_gt.jpg	('Tt',)
Saved image: ./gt_images_test/images_gt/850_0_gt.jpg
Saved label: 850_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/851_0_gt.jpg
Saved label: 851_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/852_0_gt.jpg
Saved label: 852_0_gt.jpg	('kh',)
Saved image: ./gt_images_test/images_gt/853_0_gt.jpg
Saved label: 853_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/854_0_gt.jpg
Saved label: 854_0_gt.jpg	('H',)
Saved image: ./gt_images_test/images_gt/855_0_gt.jpg
Saved label: 855_0_gt.jpg	('hn',)
Saved image: ./gt_images_test/images_gt/856_0_gt.jpg
Saved label: 856_0_gt.jpg	('chng',)
Saved image: ./gt_images_test/images_gt/857_0_gt.jpg
Saved label: 857_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/858_0_gt.jpg
Saved label: 858_0_gt.jpg	('Mai',)
Saved image: ./gt_images_test/images_gt/859_0_gt.jpg
Saved label: 859_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/860_0_gt.jpg
Saved label: 860_0_gt.jpg	('nng',)
Saved image: ./gt_images_test/images_gt/861_0_gt.jpg
Saved label: 861_0_gt.jpg	('cao',)
Saved image: ./gt_images_test/images_gt/862_0_gt.jpg
Saved label: 862_0_gt.jpg	('hnh',)
Saved image: ./gt_images_test/images_gt/863_0_gt.jpg
Saved label: 863_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/864_0_gt.jpg
Saved label: 864_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/865_0_gt.jpg
Saved label: 865_0_gt.jpg	('Bnh',)
Saved image: ./gt_images_test/images_gt/866_0_gt.jpg
Saved label: 866_0_gt.jpg	('gia',)
Saved image: ./gt_images_test/images_gt/867_0_gt.jpg
Saved label: 867_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/868_0_gt.jpg
Saved label: 868_0_gt.jpg	('Vng',)
Saved image: ./gt_images_test/images_gt/869_0_gt.jpg
Saved label: 869_0_gt.jpg	('bn',)
Saved image: ./gt_images_test/images_gt/870_0_gt.jpg
Saved label: 870_0_gt.jpg	('Xa',)
Saved image: ./gt_images_test/images_gt/871_0_gt.jpg
Saved label: 871_0_gt.jpg	('cha',)
Saved image: ./gt_images_test/images_gt/872_0_gt.jpg
Saved label: 872_0_gt.jpg	('K',)
Saved image: ./gt_images_test/images_gt/873_0_gt.jpg
Saved label: 873_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/874_0_gt.jpg
Saved label: 874_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/875_0_gt.jpg
Saved label: 875_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/876_0_gt.jpg
Saved label: 876_0_gt.jpg	('tru',)
Saved image: ./gt_images_test/images_gt/877_0_gt.jpg
Saved label: 877_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/878_0_gt.jpg
Saved label: 878_0_gt.jpg	('bn',)
Saved image: ./gt_images_test/images_gt/879_0_gt.jpg
Saved label: 879_0_gt.jpg	('mnh',)
Saved image: ./gt_images_test/images_gt/880_0_gt.jpg
Saved label: 880_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/881_0_gt.jpg
Saved label: 881_0_gt.jpg	('vai',)
Saved image: ./gt_images_test/images_gt/882_0_gt.jpg
Saved label: 882_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/883_0_gt.jpg
Saved label: 883_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/884_0_gt.jpg
Saved label: 884_0_gt.jpg	('k',)
Saved image: ./gt_images_test/images_gt/885_0_gt.jpg
Saved label: 885_0_gt.jpg	('ri',)
Saved image: ./gt_images_test/images_gt/886_0_gt.jpg
Saved label: 886_0_gt.jpg	('dy',)
Saved image: ./gt_images_test/images_gt/887_0_gt.jpg
Saved label: 887_0_gt.jpg	('Tt',)
Saved image: ./gt_images_test/images_gt/888_0_gt.jpg
Saved label: 888_0_gt.jpg	('ngn',)
Saved image: ./gt_images_test/images_gt/889_0_gt.jpg
Saved label: 889_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/890_0_gt.jpg
Saved label: 890_0_gt.jpg	('u',)
Saved image: ./gt_images_test/images_gt/891_0_gt.jpg
Saved label: 891_0_gt.jpg	('mng',)
Saved image: ./gt_images_test/images_gt/892_0_gt.jpg
Saved label: 892_0_gt.jpg	('an',)
Saved image: ./gt_images_test/images_gt/893_0_gt.jpg
Saved label: 893_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/894_0_gt.jpg
Saved label: 894_0_gt.jpg	('C',)
Saved image: ./gt_images_test/images_gt/895_0_gt.jpg
Saved label: 895_0_gt.jpg	('tng',)
Saved image: ./gt_images_test/images_gt/896_0_gt.jpg
Saved label: 896_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/897_0_gt.jpg
Saved label: 897_0_gt.jpg	('khng',)
Saved image: ./gt_images_test/images_gt/898_0_gt.jpg
Saved label: 898_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/899_0_gt.jpg
Saved label: 899_0_gt.jpg	('cy',)
Saved image: ./gt_images_test/images_gt/900_0_gt.jpg
Saved label: 900_0_gt.jpg	('p',)
Saved image: ./gt_images_test/images_gt/901_0_gt.jpg
Saved label: 901_0_gt.jpg	('nhng',)
Saved image: ./gt_images_test/images_gt/902_0_gt.jpg
Saved label: 902_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/903_0_gt.jpg
Saved label: 903_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/904_0_gt.jpg
Saved label: 904_0_gt.jpg	('Tt',)
Saved image: ./gt_images_test/images_gt/905_0_gt.jpg
Saved label: 905_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/906_0_gt.jpg
Saved label: 906_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/907_0_gt.jpg
Saved label: 907_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/908_0_gt.jpg
Saved label: 908_0_gt.jpg	('Ha',)
Saved image: ./gt_images_test/images_gt/909_0_gt.jpg
Saved label: 909_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/910_0_gt.jpg
Saved label: 910_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/911_0_gt.jpg
Saved label: 911_0_gt.jpg	('trm',)
Saved image: ./gt_images_test/images_gt/912_0_gt.jpg
Saved label: 912_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/913_0_gt.jpg
Saved label: 913_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/914_0_gt.jpg
Saved label: 914_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/915_0_gt.jpg
Saved label: 915_0_gt.jpg	('S',)
Saved image: ./gt_images_test/images_gt/916_0_gt.jpg
Saved label: 916_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/917_0_gt.jpg
Saved label: 917_0_gt.jpg	('Vo',)
Saved image: ./gt_images_test/images_gt/918_0_gt.jpg
Saved label: 918_0_gt.jpg	('thm',)
Saved image: ./gt_images_test/images_gt/919_0_gt.jpg
Saved label: 919_0_gt.jpg	('gt',)
Saved image: ./gt_images_test/images_gt/920_0_gt.jpg
Saved label: 920_0_gt.jpg	('Ngm',)
Saved image: ./gt_images_test/images_gt/921_0_gt.jpg
Saved label: 921_0_gt.jpg	('ngc',)
Saved image: ./gt_images_test/images_gt/922_0_gt.jpg
Saved label: 922_0_gt.jpg	('mu',)
Saved image: ./gt_images_test/images_gt/923_0_gt.jpg
Saved label: 923_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/924_0_gt.jpg
Saved label: 924_0_gt.jpg	('Bc',)
Saved image: ./gt_images_test/images_gt/925_0_gt.jpg
Saved label: 925_0_gt.jpg	('y',)
Saved image: ./gt_images_test/images_gt/926_0_gt.jpg
Saved label: 926_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/927_0_gt.jpg
Saved label: 927_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/928_0_gt.jpg
Saved label: 928_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/929_0_gt.jpg
Saved label: 929_0_gt.jpg	('thy',)
Saved image: ./gt_images_test/images_gt/930_0_gt.jpg
Saved label: 930_0_gt.jpg	('mai',)
Saved image: ./gt_images_test/images_gt/931_0_gt.jpg
Saved label: 931_0_gt.jpg	('Lc',)
Saved image: ./gt_images_test/images_gt/932_0_gt.jpg
Saved label: 932_0_gt.jpg	('mai',)
Saved image: ./gt_images_test/images_gt/933_0_gt.jpg
Saved label: 933_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/934_0_gt.jpg
Saved label: 934_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/935_0_gt.jpg
Saved label: 935_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/936_0_gt.jpg
Saved label: 936_0_gt.jpg	('thun',)
Saved image: ./gt_images_test/images_gt/937_0_gt.jpg
Saved label: 937_0_gt.jpg	('Pht',)
Saved image: ./gt_images_test/images_gt/938_0_gt.jpg
Saved label: 938_0_gt.jpg	('g',)
Saved image: ./gt_images_test/images_gt/939_0_gt.jpg
Saved label: 939_0_gt.jpg	('Sum',)
Saved image: ./gt_images_test/images_gt/940_0_gt.jpg
Saved label: 940_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/941_0_gt.jpg
Saved label: 941_0_gt.jpg	('Ta',)
Saved image: ./gt_images_test/images_gt/942_0_gt.jpg
Saved label: 942_0_gt.jpg	('phi',)
Saved image: ./gt_images_test/images_gt/943_0_gt.jpg
Saved label: 943_0_gt.jpg	('nng',)
Saved image: ./gt_images_test/images_gt/944_0_gt.jpg
Saved label: 944_0_gt.jpg	('S',)
Saved image: ./gt_images_test/images_gt/945_0_gt.jpg
Saved label: 945_0_gt.jpg	('leo',)
Saved image: ./gt_images_test/images_gt/946_0_gt.jpg
Saved label: 946_0_gt.jpg	('ngy',)
Saved image: ./gt_images_test/images_gt/947_0_gt.jpg
Saved label: 947_0_gt.jpg	('Hiu',)
Saved image: ./gt_images_test/images_gt/948_0_gt.jpg
Saved label: 948_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/949_0_gt.jpg
Saved label: 949_0_gt.jpg	('li',)
Saved image: ./gt_images_test/images_gt/950_0_gt.jpg
Saved label: 950_0_gt.jpg	('Ph',)
Saved image: ./gt_images_test/images_gt/951_0_gt.jpg
Saved label: 951_0_gt.jpg	('nhiu',)
Saved image: ./gt_images_test/images_gt/952_0_gt.jpg
Saved label: 952_0_gt.jpg	('Nin',)
Saved image: ./gt_images_test/images_gt/953_0_gt.jpg
Saved label: 953_0_gt.jpg	('o',)
Saved image: ./gt_images_test/images_gt/954_0_gt.jpg
Saved label: 954_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/955_0_gt.jpg
Saved label: 955_0_gt.jpg	('N',)
Saved image: ./gt_images_test/images_gt/956_0_gt.jpg
Saved label: 956_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/957_0_gt.jpg
Saved label: 957_0_gt.jpg	('nguyt',)
Saved image: ./gt_images_test/images_gt/958_0_gt.jpg
Saved label: 958_0_gt.jpg	('im',)
Saved image: ./gt_images_test/images_gt/959_0_gt.jpg
Saved label: 959_0_gt.jpg	('y',)
Saved image: ./gt_images_test/images_gt/960_0_gt.jpg
Saved label: 960_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/961_0_gt.jpg
Saved label: 961_0_gt.jpg	('khang',)
Saved image: ./gt_images_test/images_gt/962_0_gt.jpg
Saved label: 962_0_gt.jpg	('ang',)
Saved image: ./gt_images_test/images_gt/963_0_gt.jpg
Saved label: 963_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/964_0_gt.jpg
Saved label: 964_0_gt.jpg	('ht',)
Saved image: ./gt_images_test/images_gt/965_0_gt.jpg
Saved label: 965_0_gt.jpg	('dng',)
Saved image: ./gt_images_test/images_gt/966_0_gt.jpg
Saved label: 966_0_gt.jpg	('theo',)
Saved image: ./gt_images_test/images_gt/967_0_gt.jpg
Saved label: 967_0_gt.jpg	('treo',)
Saved image: ./gt_images_test/images_gt/968_0_gt.jpg
Saved label: 968_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/969_0_gt.jpg
Saved label: 969_0_gt.jpg	('Si',)
Saved image: ./gt_images_test/images_gt/970_0_gt.jpg
Saved label: 970_0_gt.jpg	('con',)
Saved image: ./gt_images_test/images_gt/971_0_gt.jpg
Saved label: 971_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/972_0_gt.jpg
Saved label: 972_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/973_0_gt.jpg
Saved label: 973_0_gt.jpg	('Hu',)
Saved image: ./gt_images_test/images_gt/974_0_gt.jpg
Saved label: 974_0_gt.jpg	('Tnh',)
Saved image: ./gt_images_test/images_gt/975_0_gt.jpg
Saved label: 975_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/976_0_gt.jpg
Saved label: 976_0_gt.jpg	('Anh',)
Saved image: ./gt_images_test/images_gt/977_0_gt.jpg
Saved label: 977_0_gt.jpg	('cha',)
Saved image: ./gt_images_test/images_gt/978_0_gt.jpg
Saved label: 978_0_gt.jpg	('hng',)
Saved image: ./gt_images_test/images_gt/979_0_gt.jpg
Saved label: 979_0_gt.jpg	('gia',)
Saved image: ./gt_images_test/images_gt/980_0_gt.jpg
Saved label: 980_0_gt.jpg	('Bi',)
Saved image: ./gt_images_test/images_gt/981_0_gt.jpg
Saved label: 981_0_gt.jpg	('Ru',)
Saved image: ./gt_images_test/images_gt/982_0_gt.jpg
Saved label: 982_0_gt.jpg	('bnh',)
Saved image: ./gt_images_test/images_gt/983_0_gt.jpg
Saved label: 983_0_gt.jpg	('Mun',)
Saved image: ./gt_images_test/images_gt/984_0_gt.jpg
Saved label: 984_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/985_0_gt.jpg
Saved label: 985_0_gt.jpg	('cm',)
Saved image: ./gt_images_test/images_gt/986_0_gt.jpg
Saved label: 986_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/987_0_gt.jpg
Saved label: 987_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/988_0_gt.jpg
Saved label: 988_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/989_0_gt.jpg
Saved label: 989_0_gt.jpg	('Kh',)
Saved image: ./gt_images_test/images_gt/990_0_gt.jpg
Saved label: 990_0_gt.jpg	('hy',)
Saved image: ./gt_images_test/images_gt/991_0_gt.jpg
Saved label: 991_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/992_0_gt.jpg
Saved label: 992_0_gt.jpg	('nghe',)
Saved image: ./gt_images_test/images_gt/993_0_gt.jpg
Saved label: 993_0_gt.jpg	('hn',)
Saved image: ./gt_images_test/images_gt/994_0_gt.jpg
Saved label: 994_0_gt.jpg	('cao',)
Saved image: ./gt_images_test/images_gt/995_0_gt.jpg
Saved label: 995_0_gt.jpg	('khoe',)
Saved image: ./gt_images_test/images_gt/996_0_gt.jpg
Saved label: 996_0_gt.jpg	('thua',)
Saved image: ./gt_images_test/images_gt/997_0_gt.jpg
Saved label: 997_0_gt.jpg	('Mng',)
Saved image: ./gt_images_test/images_gt/998_0_gt.jpg
Saved label: 998_0_gt.jpg	('Vit',)
Saved image: ./gt_images_test/images_gt/999_0_gt.jpg
Saved label: 999_0_gt.jpg	('Vng',)
Saved image: ./gt_images_test/images_gt/1000_0_gt.jpg
Saved label: 1000_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/1001_0_gt.jpg
Saved label: 1001_0_gt.jpg	('bi',)
Saved image: ./gt_images_test/images_gt/1002_0_gt.jpg
Saved label: 1002_0_gt.jpg	('Nht',)
Saved image: ./gt_images_test/images_gt/1003_0_gt.jpg
Saved label: 1003_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/1004_0_gt.jpg
Saved label: 1004_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/1005_0_gt.jpg
Saved label: 1005_0_gt.jpg	('bun',)
Saved image: ./gt_images_test/images_gt/1006_0_gt.jpg
Saved label: 1006_0_gt.jpg	('ngun',)
Saved image: ./gt_images_test/images_gt/1007_0_gt.jpg
Saved label: 1007_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/1008_0_gt.jpg
Saved label: 1008_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/1009_0_gt.jpg
Saved label: 1009_0_gt.jpg	('Ph',)
Saved image: ./gt_images_test/images_gt/1010_0_gt.jpg
Saved label: 1010_0_gt.jpg	('gia',)
Saved image: ./gt_images_test/images_gt/1011_0_gt.jpg
Saved label: 1011_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1012_0_gt.jpg
Saved label: 1012_0_gt.jpg	('trm',)
Saved image: ./gt_images_test/images_gt/1013_0_gt.jpg
Saved label: 1013_0_gt.jpg	('k',)
Saved image: ./gt_images_test/images_gt/1014_0_gt.jpg
Saved label: 1014_0_gt.jpg	('ri',)
Saved image: ./gt_images_test/images_gt/1015_0_gt.jpg
Saved label: 1015_0_gt.jpg	('hanh',)
Saved image: ./gt_images_test/images_gt/1016_0_gt.jpg
Saved label: 1016_0_gt.jpg	('Vui',)
Saved image: ./gt_images_test/images_gt/1017_0_gt.jpg
Saved label: 1017_0_gt.jpg	('Ng',)
Saved image: ./gt_images_test/images_gt/1018_0_gt.jpg
Saved label: 1018_0_gt.jpg	('vai',)
Saved image: ./gt_images_test/images_gt/1019_0_gt.jpg
Saved label: 1019_0_gt.jpg	('chng',)
Saved image: ./gt_images_test/images_gt/1020_0_gt.jpg
Saved label: 1020_0_gt.jpg	('Thng',)
Saved image: ./gt_images_test/images_gt/1021_0_gt.jpg
Saved label: 1021_0_gt.jpg	('nin',)
Saved image: ./gt_images_test/images_gt/1022_0_gt.jpg
Saved label: 1022_0_gt.jpg	('chc',)
Saved image: ./gt_images_test/images_gt/1023_0_gt.jpg
Saved label: 1023_0_gt.jpg	('Tr',)
Saved image: ./gt_images_test/images_gt/1024_0_gt.jpg
Saved label: 1024_0_gt.jpg	('Vinh',)
Saved image: ./gt_images_test/images_gt/1025_0_gt.jpg
Saved label: 1025_0_gt.jpg	('hiu',)
Saved image: ./gt_images_test/images_gt/1026_0_gt.jpg
Saved label: 1026_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/1027_0_gt.jpg
Saved label: 1027_0_gt.jpg	('hn',)
Saved image: ./gt_images_test/images_gt/1028_0_gt.jpg
Saved label: 1028_0_gt.jpg	('ph',)
Saved image: ./gt_images_test/images_gt/1029_0_gt.jpg
Saved label: 1029_0_gt.jpg	('cao',)
Saved image: ./gt_images_test/images_gt/1030_0_gt.jpg
Saved label: 1030_0_gt.jpg	('dng',)
Saved image: ./gt_images_test/images_gt/1031_0_gt.jpg
Saved label: 1031_0_gt.jpg	('trung',)
Saved image: ./gt_images_test/images_gt/1032_0_gt.jpg
Saved label: 1032_0_gt.jpg	('gn',)
Saved image: ./gt_images_test/images_gt/1033_0_gt.jpg
Saved label: 1033_0_gt.jpg	('tng',)
Saved image: ./gt_images_test/images_gt/1034_0_gt.jpg
Saved label: 1034_0_gt.jpg	('Tc',)
Saved image: ./gt_images_test/images_gt/1035_0_gt.jpg
Saved label: 1035_0_gt.jpg	('Ba',)
Saved image: ./gt_images_test/images_gt/1036_0_gt.jpg
Saved label: 1036_0_gt.jpg	('Ma',)
Saved image: ./gt_images_test/images_gt/1037_0_gt.jpg
Saved label: 1037_0_gt.jpg	('nhng',)
Saved image: ./gt_images_test/images_gt/1038_0_gt.jpg
Saved label: 1038_0_gt.jpg	('thnh',)
Saved image: ./gt_images_test/images_gt/1039_0_gt.jpg
Saved label: 1039_0_gt.jpg	('Hng',)
Saved image: ./gt_images_test/images_gt/1040_0_gt.jpg
Saved label: 1040_0_gt.jpg	('Dng',)
Saved image: ./gt_images_test/images_gt/1041_0_gt.jpg
Saved label: 1041_0_gt.jpg	('inh',)
Saved image: ./gt_images_test/images_gt/1042_0_gt.jpg
Saved label: 1042_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/1043_0_gt.jpg
Saved label: 1043_0_gt.jpg	('chng',)
Saved image: ./gt_images_test/images_gt/1044_0_gt.jpg
Saved label: 1044_0_gt.jpg	('ma',)
Saved image: ./gt_images_test/images_gt/1045_0_gt.jpg
Saved label: 1045_0_gt.jpg	('Con',)
Saved image: ./gt_images_test/images_gt/1046_0_gt.jpg
Saved label: 1046_0_gt.jpg	('Rng',)
Saved image: ./gt_images_test/images_gt/1047_0_gt.jpg
Saved label: 1047_0_gt.jpg	('sinh',)
Saved image: ./gt_images_test/images_gt/1048_0_gt.jpg
Saved label: 1048_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/1049_0_gt.jpg
Saved label: 1049_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/1050_0_gt.jpg
Saved label: 1050_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/1051_0_gt.jpg
Saved label: 1051_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/1052_0_gt.jpg
Saved label: 1052_0_gt.jpg	('php',)
Saved image: ./gt_images_test/images_gt/1053_0_gt.jpg
Saved label: 1053_0_gt.jpg	('Tu',)
Saved image: ./gt_images_test/images_gt/1054_0_gt.jpg
Saved label: 1054_0_gt.jpg	('Bch',)
Saved image: ./gt_images_test/images_gt/1055_0_gt.jpg
Saved label: 1055_0_gt.jpg	('thn',)
Saved image: ./gt_images_test/images_gt/1056_0_gt.jpg
Saved label: 1056_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/1057_0_gt.jpg
Saved label: 1057_0_gt.jpg	('s',)
Saved image: ./gt_images_test/images_gt/1058_0_gt.jpg
Saved label: 1058_0_gt.jpg	('Lm',)
Saved image: ./gt_images_test/images_gt/1059_0_gt.jpg
Saved label: 1059_0_gt.jpg	('di',)
Saved image: ./gt_images_test/images_gt/1060_0_gt.jpg
Saved label: 1060_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/1061_0_gt.jpg
Saved label: 1061_0_gt.jpg	('nhnh',)
Saved image: ./gt_images_test/images_gt/1062_0_gt.jpg
Saved label: 1062_0_gt.jpg	('vinh',)
Saved image: ./gt_images_test/images_gt/1063_0_gt.jpg
Saved label: 1063_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1064_0_gt.jpg
Saved label: 1064_0_gt.jpg	('hnh',)
Saved image: ./gt_images_test/images_gt/1065_0_gt.jpg
Saved label: 1065_0_gt.jpg	('nhau',)
Saved image: ./gt_images_test/images_gt/1066_0_gt.jpg
Saved label: 1066_0_gt.jpg	('cu',)
Saved image: ./gt_images_test/images_gt/1067_0_gt.jpg
Saved label: 1067_0_gt.jpg	('hoa',)
Saved image: ./gt_images_test/images_gt/1068_0_gt.jpg
Saved label: 1068_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/1069_0_gt.jpg
Saved label: 1069_0_gt.jpg	('tng',)
Saved image: ./gt_images_test/images_gt/1070_0_gt.jpg
Saved label: 1070_0_gt.jpg	('ong',)
Saved image: ./gt_images_test/images_gt/1071_0_gt.jpg
Saved label: 1071_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/1072_0_gt.jpg
Saved label: 1072_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/1073_0_gt.jpg
Saved label: 1073_0_gt.jpg	('vic',)
Saved image: ./gt_images_test/images_gt/1074_0_gt.jpg
Saved label: 1074_0_gt.jpg	('Thy',)
Saved image: ./gt_images_test/images_gt/1075_0_gt.jpg
Saved label: 1075_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/1076_0_gt.jpg
Saved label: 1076_0_gt.jpg	('nm',)
Saved image: ./gt_images_test/images_gt/1077_0_gt.jpg
Saved label: 1077_0_gt.jpg	('mn',)
Saved image: ./gt_images_test/images_gt/1078_0_gt.jpg
Saved label: 1078_0_gt.jpg	('Vin',)
Saved image: ./gt_images_test/images_gt/1079_0_gt.jpg
Saved label: 1079_0_gt.jpg	('bch',)
Saved image: ./gt_images_test/images_gt/1080_0_gt.jpg
Saved label: 1080_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/1081_0_gt.jpg
Saved label: 1081_0_gt.jpg	('Thm',)
Saved image: ./gt_images_test/images_gt/1082_0_gt.jpg
Saved label: 1082_0_gt.jpg	('Ti',)
Saved image: ./gt_images_test/images_gt/1083_0_gt.jpg
Saved label: 1083_0_gt.jpg	('vt',)
Saved image: ./gt_images_test/images_gt/1084_0_gt.jpg
Saved label: 1084_0_gt.jpg	('Bt',)
Saved image: ./gt_images_test/images_gt/1085_0_gt.jpg
Saved label: 1085_0_gt.jpg	('Cng',)
Saved image: ./gt_images_test/images_gt/1086_0_gt.jpg
Saved label: 1086_0_gt.jpg	('H',)
Saved image: ./gt_images_test/images_gt/1087_0_gt.jpg
Saved label: 1087_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/1088_0_gt.jpg
Saved label: 1088_0_gt.jpg	('bp',)
Saved image: ./gt_images_test/images_gt/1089_0_gt.jpg
Saved label: 1089_0_gt.jpg	('Luyn',)
Saved image: ./gt_images_test/images_gt/1090_0_gt.jpg
Saved label: 1090_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/1091_0_gt.jpg
Saved label: 1091_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/1092_0_gt.jpg
Saved label: 1092_0_gt.jpg	('thi',)
Saved image: ./gt_images_test/images_gt/1093_0_gt.jpg
Saved label: 1093_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/1094_0_gt.jpg
Saved label: 1094_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/1095_0_gt.jpg
Saved label: 1095_0_gt.jpg	('o',)
Saved image: ./gt_images_test/images_gt/1096_0_gt.jpg
Saved label: 1096_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1097_0_gt.jpg
Saved label: 1097_0_gt.jpg	('bm',)
Saved image: ./gt_images_test/images_gt/1098_0_gt.jpg
Saved label: 1098_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/1099_0_gt.jpg
Saved label: 1099_0_gt.jpg	('chng',)
Saved image: ./gt_images_test/images_gt/1100_0_gt.jpg
Saved label: 1100_0_gt.jpg	('trong',)
Saved image: ./gt_images_test/images_gt/1101_0_gt.jpg
Saved label: 1101_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/1102_0_gt.jpg
Saved label: 1102_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/1103_0_gt.jpg
Saved label: 1103_0_gt.jpg	('Ti',)
Saved image: ./gt_images_test/images_gt/1104_0_gt.jpg
Saved label: 1104_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/1105_0_gt.jpg
Saved label: 1105_0_gt.jpg	('mn',)
Saved image: ./gt_images_test/images_gt/1106_0_gt.jpg
Saved label: 1106_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/1107_0_gt.jpg
Saved label: 1107_0_gt.jpg	('rn',)
Saved image: ./gt_images_test/images_gt/1108_0_gt.jpg
Saved label: 1108_0_gt.jpg	('nin',)
Saved image: ./gt_images_test/images_gt/1109_0_gt.jpg
Saved label: 1109_0_gt.jpg	('Tt',)
Saved image: ./gt_images_test/images_gt/1110_0_gt.jpg
Saved label: 1110_0_gt.jpg	('Ca',)
Saved image: ./gt_images_test/images_gt/1111_0_gt.jpg
Saved label: 1111_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/1112_0_gt.jpg
Saved label: 1112_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/1113_0_gt.jpg
Saved label: 1113_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1114_0_gt.jpg
Saved label: 1114_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/1115_0_gt.jpg
Saved label: 1115_0_gt.jpg	('danh',)
Saved image: ./gt_images_test/images_gt/1116_0_gt.jpg
Saved label: 1116_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/1117_0_gt.jpg
Saved label: 1117_0_gt.jpg	('hnh',)
Saved image: ./gt_images_test/images_gt/1118_0_gt.jpg
Saved label: 1118_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/1119_0_gt.jpg
Saved label: 1119_0_gt.jpg	('Nhn',)
Saved image: ./gt_images_test/images_gt/1120_0_gt.jpg
Saved label: 1120_0_gt.jpg	('ai',)
Saved image: ./gt_images_test/images_gt/1121_0_gt.jpg
Saved label: 1121_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/1122_0_gt.jpg
Saved label: 1122_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/1123_0_gt.jpg
Saved label: 1123_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/1124_0_gt.jpg
Saved label: 1124_0_gt.jpg	('tu',)
Saved image: ./gt_images_test/images_gt/1125_0_gt.jpg
Saved label: 1125_0_gt.jpg	('non',)
Saved image: ./gt_images_test/images_gt/1126_0_gt.jpg
Saved label: 1126_0_gt.jpg	('thnh',)
Saved image: ./gt_images_test/images_gt/1127_0_gt.jpg
Saved label: 1127_0_gt.jpg	('Thip',)
Saved image: ./gt_images_test/images_gt/1128_0_gt.jpg
Saved label: 1128_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/1129_0_gt.jpg
Saved label: 1129_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/1130_0_gt.jpg
Saved label: 1130_0_gt.jpg	('sn',)
Saved image: ./gt_images_test/images_gt/1131_0_gt.jpg
Saved label: 1131_0_gt.jpg	('gc',)
Saved image: ./gt_images_test/images_gt/1132_0_gt.jpg
Saved label: 1132_0_gt.jpg	('bng',)
Saved image: ./gt_images_test/images_gt/1133_0_gt.jpg
Saved label: 1133_0_gt.jpg	('ho',)
Saved image: ./gt_images_test/images_gt/1134_0_gt.jpg
Saved label: 1134_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/1135_0_gt.jpg
Saved label: 1135_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1136_0_gt.jpg
Saved label: 1136_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1137_0_gt.jpg
Saved label: 1137_0_gt.jpg	('Ln',)
Saved image: ./gt_images_test/images_gt/1138_0_gt.jpg
Saved label: 1138_0_gt.jpg	('xng',)
Saved image: ./gt_images_test/images_gt/1139_0_gt.jpg
Saved label: 1139_0_gt.jpg	('Ty',)
Saved image: ./gt_images_test/images_gt/1140_0_gt.jpg
Saved label: 1140_0_gt.jpg	('thnh',)
Saved image: ./gt_images_test/images_gt/1141_0_gt.jpg
Saved label: 1141_0_gt.jpg	('nu',)
Saved image: ./gt_images_test/images_gt/1142_0_gt.jpg
Saved label: 1142_0_gt.jpg	('Hnh',)
Saved image: ./gt_images_test/images_gt/1143_0_gt.jpg
Saved label: 1143_0_gt.jpg	('thi',)
Saved image: ./gt_images_test/images_gt/1144_0_gt.jpg
Saved label: 1144_0_gt.jpg	('Phng',)
Saved image: ./gt_images_test/images_gt/1145_0_gt.jpg
Saved label: 1145_0_gt.jpg	('tui',)
Saved image: ./gt_images_test/images_gt/1146_0_gt.jpg
Saved label: 1146_0_gt.jpg	('S',)
Saved image: ./gt_images_test/images_gt/1147_0_gt.jpg
Saved label: 1147_0_gt.jpg	('Vit',)
Saved image: ./gt_images_test/images_gt/1148_0_gt.jpg
Saved label: 1148_0_gt.jpg	('sn',)
Saved image: ./gt_images_test/images_gt/1149_0_gt.jpg
Saved label: 1149_0_gt.jpg	('s',)
Saved image: ./gt_images_test/images_gt/1150_0_gt.jpg
Saved label: 1150_0_gt.jpg	('H',)
Saved image: ./gt_images_test/images_gt/1151_0_gt.jpg
Saved label: 1151_0_gt.jpg	('gio',)
Saved image: ./gt_images_test/images_gt/1152_0_gt.jpg
Saved label: 1152_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/1153_0_gt.jpg
Saved label: 1153_0_gt.jpg	('non',)
Saved image: ./gt_images_test/images_gt/1154_0_gt.jpg
Saved label: 1154_0_gt.jpg	('C',)
Saved image: ./gt_images_test/images_gt/1155_0_gt.jpg
Saved label: 1155_0_gt.jpg	('Li',)
Saved image: ./gt_images_test/images_gt/1156_0_gt.jpg
Saved label: 1156_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/1157_0_gt.jpg
Saved label: 1157_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/1158_0_gt.jpg
Saved label: 1158_0_gt.jpg	('Gi',)
Saved image: ./gt_images_test/images_gt/1159_0_gt.jpg
Saved label: 1159_0_gt.jpg	('Tp',)
Saved image: ./gt_images_test/images_gt/1160_0_gt.jpg
Saved label: 1160_0_gt.jpg	('Pht',)
Saved image: ./gt_images_test/images_gt/1161_0_gt.jpg
Saved label: 1161_0_gt.jpg	('Dong',)
Saved image: ./gt_images_test/images_gt/1162_0_gt.jpg
Saved label: 1162_0_gt.jpg	('non',)
Saved image: ./gt_images_test/images_gt/1163_0_gt.jpg
Saved label: 1163_0_gt.jpg	('vic',)
Saved image: ./gt_images_test/images_gt/1164_0_gt.jpg
Saved label: 1164_0_gt.jpg	('nhp',)
Saved image: ./gt_images_test/images_gt/1165_0_gt.jpg
Saved label: 1165_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/1166_0_gt.jpg
Saved label: 1166_0_gt.jpg	('thu',)
Saved image: ./gt_images_test/images_gt/1167_0_gt.jpg
Saved label: 1167_0_gt.jpg	('Hiu',)
Saved image: ./gt_images_test/images_gt/1168_0_gt.jpg
Saved label: 1168_0_gt.jpg	('mn',)
Saved image: ./gt_images_test/images_gt/1169_0_gt.jpg
Saved label: 1169_0_gt.jpg	('trt',)
Saved image: ./gt_images_test/images_gt/1170_0_gt.jpg
Saved label: 1170_0_gt.jpg	('tin',)
Saved image: ./gt_images_test/images_gt/1171_0_gt.jpg
Saved label: 1171_0_gt.jpg	('Con',)
Saved image: ./gt_images_test/images_gt/1172_0_gt.jpg
Saved label: 1172_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/1173_0_gt.jpg
Saved label: 1173_0_gt.jpg	('ging',)
Saved image: ./gt_images_test/images_gt/1174_0_gt.jpg
Saved label: 1174_0_gt.jpg	('man',)
Saved image: ./gt_images_test/images_gt/1175_0_gt.jpg
Saved label: 1175_0_gt.jpg	('o',)
Saved image: ./gt_images_test/images_gt/1176_0_gt.jpg
Saved label: 1176_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/1177_0_gt.jpg
Saved label: 1177_0_gt.jpg	('lu',)
Saved image: ./gt_images_test/images_gt/1178_0_gt.jpg
Saved label: 1178_0_gt.jpg	('Ngi',)
Saved image: ./gt_images_test/images_gt/1179_0_gt.jpg
Saved label: 1179_0_gt.jpg	('Vn',)
Saved image: ./gt_images_test/images_gt/1180_0_gt.jpg
Saved label: 1180_0_gt.jpg	('ni',)
Saved image: ./gt_images_test/images_gt/1181_0_gt.jpg
Saved label: 1181_0_gt.jpg	('Con',)
Saved image: ./gt_images_test/images_gt/1182_0_gt.jpg
Saved label: 1182_0_gt.jpg	('bn',)
Saved image: ./gt_images_test/images_gt/1183_0_gt.jpg
Saved label: 1183_0_gt.jpg	('C',)
Saved image: ./gt_images_test/images_gt/1184_0_gt.jpg
Saved label: 1184_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/1185_0_gt.jpg
Saved label: 1185_0_gt.jpg	('Hy',)
Saved image: ./gt_images_test/images_gt/1186_0_gt.jpg
Saved label: 1186_0_gt.jpg	('b',)
Saved image: ./gt_images_test/images_gt/1187_0_gt.jpg
Saved label: 1187_0_gt.jpg	('Thn',)
Saved image: ./gt_images_test/images_gt/1188_0_gt.jpg
Saved label: 1188_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/1189_0_gt.jpg
Saved label: 1189_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1190_0_gt.jpg
Saved label: 1190_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1191_0_gt.jpg
Saved label: 1191_0_gt.jpg	('Cha',)
Saved image: ./gt_images_test/images_gt/1192_0_gt.jpg
Saved label: 1192_0_gt.jpg	('minh',)
Saved image: ./gt_images_test/images_gt/1193_0_gt.jpg
Saved label: 1193_0_gt.jpg	('Kit',)
Saved image: ./gt_images_test/images_gt/1194_0_gt.jpg
Saved label: 1194_0_gt.jpg	('vo',)
Saved image: ./gt_images_test/images_gt/1195_0_gt.jpg
Saved label: 1195_0_gt.jpg	('ging',)
Saved image: ./gt_images_test/images_gt/1196_0_gt.jpg
Saved label: 1196_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/1197_0_gt.jpg
Saved label: 1197_0_gt.jpg	('ly',)
Saved image: ./gt_images_test/images_gt/1198_0_gt.jpg
Saved label: 1198_0_gt.jpg	('sum',)
Saved image: ./gt_images_test/images_gt/1199_0_gt.jpg
Saved label: 1199_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/1200_0_gt.jpg
Saved label: 1200_0_gt.jpg	('ngha',)
Saved image: ./gt_images_test/images_gt/1201_0_gt.jpg
Saved label: 1201_0_gt.jpg	('ngng',)
Saved image: ./gt_images_test/images_gt/1202_0_gt.jpg
Saved label: 1202_0_gt.jpg	('nhp',)
Saved image: ./gt_images_test/images_gt/1203_0_gt.jpg
Saved label: 1203_0_gt.jpg	('chung',)
Saved image: ./gt_images_test/images_gt/1204_0_gt.jpg
Saved label: 1204_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/1205_0_gt.jpg
Saved label: 1205_0_gt.jpg	('Nguyn',)
Saved image: ./gt_images_test/images_gt/1206_0_gt.jpg
Saved label: 1206_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1207_0_gt.jpg
Saved label: 1207_0_gt.jpg	('ni',)
Saved image: ./gt_images_test/images_gt/1208_0_gt.jpg
Saved label: 1208_0_gt.jpg	('Trng',)
Saved image: ./gt_images_test/images_gt/1209_0_gt.jpg
Saved label: 1209_0_gt.jpg	('S',)
Saved image: ./gt_images_test/images_gt/1210_0_gt.jpg
Saved label: 1210_0_gt.jpg	('trm',)
Saved image: ./gt_images_test/images_gt/1211_0_gt.jpg
Saved label: 1211_0_gt.jpg	('phn',)
Saved image: ./gt_images_test/images_gt/1212_0_gt.jpg
Saved label: 1212_0_gt.jpg	('thiu',)
Saved image: ./gt_images_test/images_gt/1213_0_gt.jpg
Saved label: 1213_0_gt.jpg	('vit',)
Saved image: ./gt_images_test/images_gt/1214_0_gt.jpg
Saved label: 1214_0_gt.jpg	('p',)
Saved image: ./gt_images_test/images_gt/1215_0_gt.jpg
Saved label: 1215_0_gt.jpg	('C',)
Saved image: ./gt_images_test/images_gt/1216_0_gt.jpg
Saved label: 1216_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/1217_0_gt.jpg
Saved label: 1217_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1218_0_gt.jpg
Saved label: 1218_0_gt.jpg	('tip',)
Saved image: ./gt_images_test/images_gt/1219_0_gt.jpg
Saved label: 1219_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/1220_0_gt.jpg
Saved label: 1220_0_gt.jpg	('ln',)
Saved image: ./gt_images_test/images_gt/1221_0_gt.jpg
Saved label: 1221_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/1222_0_gt.jpg
Saved label: 1222_0_gt.jpg	('Sn',)
Saved image: ./gt_images_test/images_gt/1223_0_gt.jpg
Saved label: 1223_0_gt.jpg	('cho',)
Saved image: ./gt_images_test/images_gt/1224_0_gt.jpg
Saved label: 1224_0_gt.jpg	('hin',)
Saved image: ./gt_images_test/images_gt/1225_0_gt.jpg
Saved label: 1225_0_gt.jpg	('mng',)
Saved image: ./gt_images_test/images_gt/1226_0_gt.jpg
Saved label: 1226_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/1227_0_gt.jpg
Saved label: 1227_0_gt.jpg	('Nh',)
Saved image: ./gt_images_test/images_gt/1228_0_gt.jpg
Saved label: 1228_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/1229_0_gt.jpg
Saved label: 1229_0_gt.jpg	('nn',)
Saved image: ./gt_images_test/images_gt/1230_0_gt.jpg
Saved label: 1230_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/1231_0_gt.jpg
Saved label: 1231_0_gt.jpg	('Ph',)
Saved image: ./gt_images_test/images_gt/1232_0_gt.jpg
Saved label: 1232_0_gt.jpg	('phng',)
Saved image: ./gt_images_test/images_gt/1233_0_gt.jpg
Saved label: 1233_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/1234_0_gt.jpg
Saved label: 1234_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/1235_0_gt.jpg
Saved label: 1235_0_gt.jpg	('b',)
Saved image: ./gt_images_test/images_gt/1236_0_gt.jpg
Saved label: 1236_0_gt.jpg	('Lc',)
Saved image: ./gt_images_test/images_gt/1237_0_gt.jpg
Saved label: 1237_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/1238_0_gt.jpg
Saved label: 1238_0_gt.jpg	('Thnh',)
Saved image: ./gt_images_test/images_gt/1239_0_gt.jpg
Saved label: 1239_0_gt.jpg	('Hu',)
Saved image: ./gt_images_test/images_gt/1240_0_gt.jpg
Saved label: 1240_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/1241_0_gt.jpg
Saved label: 1241_0_gt.jpg	('hng',)
Saved image: ./gt_images_test/images_gt/1242_0_gt.jpg
Saved label: 1242_0_gt.jpg	('z',)
Saved image: ./gt_images_test/images_gt/1243_0_gt.jpg
Saved label: 1243_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/1244_0_gt.jpg
Saved label: 1244_0_gt.jpg	('ci',)
Saved image: ./gt_images_test/images_gt/1245_0_gt.jpg
Saved label: 1245_0_gt.jpg	('hp',)
Saved image: ./gt_images_test/images_gt/1246_0_gt.jpg
Saved label: 1246_0_gt.jpg	('ra',)
Saved image: ./gt_images_test/images_gt/1247_0_gt.jpg
Saved label: 1247_0_gt.jpg	('Thng',)
Saved image: ./gt_images_test/images_gt/1248_0_gt.jpg
Saved label: 1248_0_gt.jpg	('nm',)
Saved image: ./gt_images_test/images_gt/1249_0_gt.jpg
Saved label: 1249_0_gt.jpg	('ngang',)
Saved image: ./gt_images_test/images_gt/1250_0_gt.jpg
Saved label: 1250_0_gt.jpg	('Hy',)
Saved image: ./gt_images_test/images_gt/1251_0_gt.jpg
Saved label: 1251_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/1252_0_gt.jpg
Saved label: 1252_0_gt.jpg	('bn',)
Saved image: ./gt_images_test/images_gt/1253_0_gt.jpg
Saved label: 1253_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/1254_0_gt.jpg
Saved label: 1254_0_gt.jpg	('quay',)
Saved image: ./gt_images_test/images_gt/1255_0_gt.jpg
Saved label: 1255_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/1256_0_gt.jpg
Saved label: 1256_0_gt.jpg	('qua',)
Saved image: ./gt_images_test/images_gt/1257_0_gt.jpg
Saved label: 1257_0_gt.jpg	('Nghim',)
Saved image: ./gt_images_test/images_gt/1258_0_gt.jpg
Saved label: 1258_0_gt.jpg	('nim',)
Saved image: ./gt_images_test/images_gt/1259_0_gt.jpg
Saved label: 1259_0_gt.jpg	('chc',)
Saved image: ./gt_images_test/images_gt/1260_0_gt.jpg
Saved label: 1260_0_gt.jpg	('ngui',)
Saved image: ./gt_images_test/images_gt/1261_0_gt.jpg
Saved label: 1261_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/1262_0_gt.jpg
Saved label: 1262_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/1263_0_gt.jpg
Saved label: 1263_0_gt.jpg	('T',)
Saved image: ./gt_images_test/images_gt/1264_0_gt.jpg
Saved label: 1264_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/1265_0_gt.jpg
Saved label: 1265_0_gt.jpg	('Tt',)
Saved image: ./gt_images_test/images_gt/1266_0_gt.jpg
Saved label: 1266_0_gt.jpg	('Lt',)
Saved image: ./gt_images_test/images_gt/1267_0_gt.jpg
Saved label: 1267_0_gt.jpg	('vang',)
Saved image: ./gt_images_test/images_gt/1268_0_gt.jpg
Saved label: 1268_0_gt.jpg	('che',)
Saved image: ./gt_images_test/images_gt/1269_0_gt.jpg
Saved label: 1269_0_gt.jpg	('bung',)
Saved image: ./gt_images_test/images_gt/1270_0_gt.jpg
Saved label: 1270_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/1271_0_gt.jpg
Saved label: 1271_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/1272_0_gt.jpg
Saved label: 1272_0_gt.jpg	('nim',)
Saved image: ./gt_images_test/images_gt/1273_0_gt.jpg
Saved label: 1273_0_gt.jpg	('thm',)
Saved image: ./gt_images_test/images_gt/1274_0_gt.jpg
Saved label: 1274_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/1275_0_gt.jpg
Saved label: 1275_0_gt.jpg	('ln',)
Saved image: ./gt_images_test/images_gt/1276_0_gt.jpg
Saved label: 1276_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/1277_0_gt.jpg
Saved label: 1277_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1278_0_gt.jpg
Saved label: 1278_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/1279_0_gt.jpg
Saved label: 1279_0_gt.jpg	('xao',)
Saved image: ./gt_images_test/images_gt/1280_0_gt.jpg
Saved label: 1280_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1281_0_gt.jpg
Saved label: 1281_0_gt.jpg	('thn',)
Saved image: ./gt_images_test/images_gt/1282_0_gt.jpg
Saved label: 1282_0_gt.jpg	('nan',)
Saved image: ./gt_images_test/images_gt/1283_0_gt.jpg
Saved label: 1283_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/1284_0_gt.jpg
Saved label: 1284_0_gt.jpg	('vi',)
Saved image: ./gt_images_test/images_gt/1285_0_gt.jpg
Saved label: 1285_0_gt.jpg	('tay',)
Saved image: ./gt_images_test/images_gt/1286_0_gt.jpg
Saved label: 1286_0_gt.jpg	('Mo',)
Saved image: ./gt_images_test/images_gt/1287_0_gt.jpg
Saved label: 1287_0_gt.jpg	('trc',)
Saved image: ./gt_images_test/images_gt/1288_0_gt.jpg
Saved label: 1288_0_gt.jpg	('y',)
Saved image: ./gt_images_test/images_gt/1289_0_gt.jpg
Saved label: 1289_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/1290_0_gt.jpg
Saved label: 1290_0_gt.jpg	('Bnh',)
Saved image: ./gt_images_test/images_gt/1291_0_gt.jpg
Saved label: 1291_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/1292_0_gt.jpg
Saved label: 1292_0_gt.jpg	('Thin',)
Saved image: ./gt_images_test/images_gt/1293_0_gt.jpg
Saved label: 1293_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/1294_0_gt.jpg
Saved label: 1294_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/1295_0_gt.jpg
Saved label: 1295_0_gt.jpg	('mang',)
Saved image: ./gt_images_test/images_gt/1296_0_gt.jpg
Saved label: 1296_0_gt.jpg	('gy',)
Saved image: ./gt_images_test/images_gt/1297_0_gt.jpg
Saved label: 1297_0_gt.jpg	('ci',)
Saved image: ./gt_images_test/images_gt/1298_0_gt.jpg
Saved label: 1298_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/1299_0_gt.jpg
Saved label: 1299_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/1300_0_gt.jpg
Saved label: 1300_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/1301_0_gt.jpg
Saved label: 1301_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/1302_0_gt.jpg
Saved label: 1302_0_gt.jpg	('Soi',)
Saved image: ./gt_images_test/images_gt/1303_0_gt.jpg
Saved label: 1303_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/1304_0_gt.jpg
Saved label: 1304_0_gt.jpg	('y',)
Saved image: ./gt_images_test/images_gt/1305_0_gt.jpg
Saved label: 1305_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/1306_0_gt.jpg
Saved label: 1306_0_gt.jpg	('cnh',)
Saved image: ./gt_images_test/images_gt/1307_0_gt.jpg
Saved label: 1307_0_gt.jpg	('nui',)
Saved image: ./gt_images_test/images_gt/1308_0_gt.jpg
Saved label: 1308_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/1309_0_gt.jpg
Saved label: 1309_0_gt.jpg	('Ph',)
Saved image: ./gt_images_test/images_gt/1310_0_gt.jpg
Saved label: 1310_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/1311_0_gt.jpg
Saved label: 1311_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1312_0_gt.jpg
Saved label: 1312_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/1313_0_gt.jpg
Saved label: 1313_0_gt.jpg	('d',)
Saved image: ./gt_images_test/images_gt/1314_0_gt.jpg
Saved label: 1314_0_gt.jpg	('Thnh',)
Saved image: ./gt_images_test/images_gt/1315_0_gt.jpg
Saved label: 1315_0_gt.jpg	('Nhn',)
Saved image: ./gt_images_test/images_gt/1316_0_gt.jpg
Saved label: 1316_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/1317_0_gt.jpg
Saved label: 1317_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/1318_0_gt.jpg
Saved label: 1318_0_gt.jpg	('ta',)
Saved image: ./gt_images_test/images_gt/1319_0_gt.jpg
Saved label: 1319_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/1320_0_gt.jpg
Saved label: 1320_0_gt.jpg	('du',)
Saved image: ./gt_images_test/images_gt/1321_0_gt.jpg
Saved label: 1321_0_gt.jpg	('ru',)
Saved image: ./gt_images_test/images_gt/1322_0_gt.jpg
Saved label: 1322_0_gt.jpg	('ngy',)
Saved image: ./gt_images_test/images_gt/1323_0_gt.jpg
Saved label: 1323_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/1324_0_gt.jpg
Saved label: 1324_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/1325_0_gt.jpg
Saved label: 1325_0_gt.jpg	('xa',)
Saved image: ./gt_images_test/images_gt/1326_0_gt.jpg
Saved label: 1326_0_gt.jpg	('Tr',)
Saved image: ./gt_images_test/images_gt/1327_0_gt.jpg
Saved label: 1327_0_gt.jpg	('a',)
Saved image: ./gt_images_test/images_gt/1328_0_gt.jpg
Saved label: 1328_0_gt.jpg	('thu',)
Saved image: ./gt_images_test/images_gt/1329_0_gt.jpg
Saved label: 1329_0_gt.jpg	('ngy',)
Saved image: ./gt_images_test/images_gt/1330_0_gt.jpg
Saved label: 1330_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/1331_0_gt.jpg
Saved label: 1331_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1332_0_gt.jpg
Saved label: 1332_0_gt.jpg	('thanh',)
Saved image: ./gt_images_test/images_gt/1333_0_gt.jpg
Saved label: 1333_0_gt.jpg	('Si',)
Saved image: ./gt_images_test/images_gt/1334_0_gt.jpg
Saved label: 1334_0_gt.jpg	('hng',)
Saved image: ./gt_images_test/images_gt/1335_0_gt.jpg
Saved label: 1335_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1336_0_gt.jpg
Saved label: 1336_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/1337_0_gt.jpg
Saved label: 1337_0_gt.jpg	('s',)
Saved image: ./gt_images_test/images_gt/1338_0_gt.jpg
Saved label: 1338_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/1339_0_gt.jpg
Saved label: 1339_0_gt.jpg	('Thin',)
Saved image: ./gt_images_test/images_gt/1340_0_gt.jpg
Saved label: 1340_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/1341_0_gt.jpg
Saved label: 1341_0_gt.jpg	('hin',)
Saved image: ./gt_images_test/images_gt/1342_0_gt.jpg
Saved label: 1342_0_gt.jpg	('V',)
Saved image: ./gt_images_test/images_gt/1343_0_gt.jpg
Saved label: 1343_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1344_0_gt.jpg
Saved label: 1344_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/1345_0_gt.jpg
Saved label: 1345_0_gt.jpg	('Kip',)
Saved image: ./gt_images_test/images_gt/1346_0_gt.jpg
Saved label: 1346_0_gt.jpg	('thua',)
Saved image: ./gt_images_test/images_gt/1347_0_gt.jpg
Saved label: 1347_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/1348_0_gt.jpg
Saved label: 1348_0_gt.jpg	('ra',)
Saved image: ./gt_images_test/images_gt/1349_0_gt.jpg
Saved label: 1349_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/1350_0_gt.jpg
Saved label: 1350_0_gt.jpg	('trm',)
Saved image: ./gt_images_test/images_gt/1351_0_gt.jpg
Saved label: 1351_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/1352_0_gt.jpg
Saved label: 1352_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/1353_0_gt.jpg
Saved label: 1353_0_gt.jpg	('bnh',)
Saved image: ./gt_images_test/images_gt/1354_0_gt.jpg
Saved label: 1354_0_gt.jpg	('phch',)
Saved image: ./gt_images_test/images_gt/1355_0_gt.jpg
Saved label: 1355_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1356_0_gt.jpg
Saved label: 1356_0_gt.jpg	('Say',)
Saved image: ./gt_images_test/images_gt/1357_0_gt.jpg
Saved label: 1357_0_gt.jpg	('mn',)
Saved image: ./gt_images_test/images_gt/1358_0_gt.jpg
Saved label: 1358_0_gt.jpg	('ngy',)
Saved image: ./gt_images_test/images_gt/1359_0_gt.jpg
Saved label: 1359_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/1360_0_gt.jpg
Saved label: 1360_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/1361_0_gt.jpg
Saved label: 1361_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/1362_0_gt.jpg
Saved label: 1362_0_gt.jpg	('Tr',)
Saved image: ./gt_images_test/images_gt/1363_0_gt.jpg
Saved label: 1363_0_gt.jpg	('dng',)
Saved image: ./gt_images_test/images_gt/1364_0_gt.jpg
Saved label: 1364_0_gt.jpg	('Nhn',)
Saved image: ./gt_images_test/images_gt/1365_0_gt.jpg
Saved label: 1365_0_gt.jpg	('bc',)
Saved image: ./gt_images_test/images_gt/1366_0_gt.jpg
Saved label: 1366_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1367_0_gt.jpg
Saved label: 1367_0_gt.jpg	('ngn',)
Saved image: ./gt_images_test/images_gt/1368_0_gt.jpg
Saved label: 1368_0_gt.jpg	('Ta',)
Saved image: ./gt_images_test/images_gt/1369_0_gt.jpg
Saved label: 1369_0_gt.jpg	('bin',)
Saved image: ./gt_images_test/images_gt/1370_0_gt.jpg
Saved label: 1370_0_gt.jpg	('hng',)
Saved image: ./gt_images_test/images_gt/1371_0_gt.jpg
Saved label: 1371_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/1372_0_gt.jpg
Saved label: 1372_0_gt.jpg	('Hng',)
Saved image: ./gt_images_test/images_gt/1373_0_gt.jpg
Saved label: 1373_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/1374_0_gt.jpg
Saved label: 1374_0_gt.jpg	('con',)
Saved image: ./gt_images_test/images_gt/1375_0_gt.jpg
Saved label: 1375_0_gt.jpg	('tng',)
Saved image: ./gt_images_test/images_gt/1376_0_gt.jpg
Saved label: 1376_0_gt.jpg	('Tt',)
Saved image: ./gt_images_test/images_gt/1377_0_gt.jpg
Saved label: 1377_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/1378_0_gt.jpg
Saved label: 1378_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/1379_0_gt.jpg
Saved label: 1379_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/1380_0_gt.jpg
Saved label: 1380_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/1381_0_gt.jpg
Saved label: 1381_0_gt.jpg	('Chu',)
Saved image: ./gt_images_test/images_gt/1382_0_gt.jpg
Saved label: 1382_0_gt.jpg	('Kn',)
Saved image: ./gt_images_test/images_gt/1383_0_gt.jpg
Saved label: 1383_0_gt.jpg	('hin',)
Saved image: ./gt_images_test/images_gt/1384_0_gt.jpg
Saved label: 1384_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/1385_0_gt.jpg
Saved label: 1385_0_gt.jpg	('Gia',)
Saved image: ./gt_images_test/images_gt/1386_0_gt.jpg
Saved label: 1386_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/1387_0_gt.jpg
Saved label: 1387_0_gt.jpg	('T',)
Saved image: ./gt_images_test/images_gt/1388_0_gt.jpg
Saved label: 1388_0_gt.jpg	('hng',)
Saved image: ./gt_images_test/images_gt/1389_0_gt.jpg
Saved label: 1389_0_gt.jpg	('nha',)
Saved image: ./gt_images_test/images_gt/1390_0_gt.jpg
Saved label: 1390_0_gt.jpg	('h',)
Saved image: ./gt_images_test/images_gt/1391_0_gt.jpg
Saved label: 1391_0_gt.jpg	('Vn',)
Saved image: ./gt_images_test/images_gt/1392_0_gt.jpg
Saved label: 1392_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/1393_0_gt.jpg
Saved label: 1393_0_gt.jpg	('Bi',)
Saved image: ./gt_images_test/images_gt/1394_0_gt.jpg
Saved label: 1394_0_gt.jpg	('Mai',)
Saved image: ./gt_images_test/images_gt/1395_0_gt.jpg
Saved label: 1395_0_gt.jpg	('nguyt',)
Saved image: ./gt_images_test/images_gt/1396_0_gt.jpg
Saved label: 1396_0_gt.jpg	('H',)
Saved image: ./gt_images_test/images_gt/1397_0_gt.jpg
Saved label: 1397_0_gt.jpg	('T',)
Saved image: ./gt_images_test/images_gt/1398_0_gt.jpg
Saved label: 1398_0_gt.jpg	('nhiu',)
Saved image: ./gt_images_test/images_gt/1399_0_gt.jpg
Saved label: 1399_0_gt.jpg	('Vin',)
Saved image: ./gt_images_test/images_gt/1400_0_gt.jpg
Saved label: 1400_0_gt.jpg	('chung',)
Saved image: ./gt_images_test/images_gt/1401_0_gt.jpg
Saved label: 1401_0_gt.jpg	('thanh',)
Saved image: ./gt_images_test/images_gt/1402_0_gt.jpg
Saved label: 1402_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1403_0_gt.jpg
Saved label: 1403_0_gt.jpg	('hoa',)
Saved image: ./gt_images_test/images_gt/1404_0_gt.jpg
Saved label: 1404_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/1405_0_gt.jpg
Saved label: 1405_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/1406_0_gt.jpg
Saved label: 1406_0_gt.jpg	('Dn',)
Saved image: ./gt_images_test/images_gt/1407_0_gt.jpg
Saved label: 1407_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/1408_0_gt.jpg
Saved label: 1408_0_gt.jpg	('Dit',)
Saved image: ./gt_images_test/images_gt/1409_0_gt.jpg
Saved label: 1409_0_gt.jpg	('Nm',)
Saved image: ./gt_images_test/images_gt/1410_0_gt.jpg
Saved label: 1410_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/1411_0_gt.jpg
Saved label: 1411_0_gt.jpg	('khc',)
Saved image: ./gt_images_test/images_gt/1412_0_gt.jpg
Saved label: 1412_0_gt.jpg	('hoa',)
Saved image: ./gt_images_test/images_gt/1413_0_gt.jpg
Saved label: 1413_0_gt.jpg	('Ng',)
Saved image: ./gt_images_test/images_gt/1414_0_gt.jpg
Saved label: 1414_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1415_0_gt.jpg
Saved label: 1415_0_gt.jpg	('an',)
Saved image: ./gt_images_test/images_gt/1416_0_gt.jpg
Saved label: 1416_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/1417_0_gt.jpg
Saved label: 1417_0_gt.jpg	('gng',)
Saved image: ./gt_images_test/images_gt/1418_0_gt.jpg
Saved label: 1418_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/1419_0_gt.jpg
Saved label: 1419_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1420_0_gt.jpg
Saved label: 1420_0_gt.jpg	('nghip',)
Saved image: ./gt_images_test/images_gt/1421_0_gt.jpg
Saved label: 1421_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/1422_0_gt.jpg
Saved label: 1422_0_gt.jpg	('Hc',)
Saved image: ./gt_images_test/images_gt/1423_0_gt.jpg
Saved label: 1423_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/1424_0_gt.jpg
Saved label: 1424_0_gt.jpg	('Xun',)
Saved image: ./gt_images_test/images_gt/1425_0_gt.jpg
Saved label: 1425_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/1426_0_gt.jpg
Saved label: 1426_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1427_0_gt.jpg
Saved label: 1427_0_gt.jpg	('gic',)
Saved image: ./gt_images_test/images_gt/1428_0_gt.jpg
Saved label: 1428_0_gt.jpg	('Thu',)
Saved image: ./gt_images_test/images_gt/1429_0_gt.jpg
Saved label: 1429_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/1430_0_gt.jpg
Saved label: 1430_0_gt.jpg	('vui',)
Saved image: ./gt_images_test/images_gt/1431_0_gt.jpg
Saved label: 1431_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/1432_0_gt.jpg
Saved label: 1432_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1433_0_gt.jpg
Saved label: 1433_0_gt.jpg	('Lc',)
Saved image: ./gt_images_test/images_gt/1434_0_gt.jpg
Saved label: 1434_0_gt.jpg	('nc',)
Saved image: ./gt_images_test/images_gt/1435_0_gt.jpg
Saved label: 1435_0_gt.jpg	('gi',)
Saved image: ./gt_images_test/images_gt/1436_0_gt.jpg
Saved label: 1436_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/1437_0_gt.jpg
Saved label: 1437_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/1438_0_gt.jpg
Saved label: 1438_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/1439_0_gt.jpg
Saved label: 1439_0_gt.jpg	('bnh',)
Saved image: ./gt_images_test/images_gt/1440_0_gt.jpg
Saved label: 1440_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1441_0_gt.jpg
Saved label: 1441_0_gt.jpg	('NM',)
Saved image: ./gt_images_test/images_gt/1442_0_gt.jpg
Saved label: 1442_0_gt.jpg	('D',)
Saved image: ./gt_images_test/images_gt/1443_0_gt.jpg
Saved label: 1443_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/1444_0_gt.jpg
Saved label: 1444_0_gt.jpg	('quy',)
Saved image: ./gt_images_test/images_gt/1445_0_gt.jpg
Saved label: 1445_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/1446_0_gt.jpg
Saved label: 1446_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/1447_0_gt.jpg
Saved label: 1447_0_gt.jpg	('h',)
Saved image: ./gt_images_test/images_gt/1448_0_gt.jpg
Saved label: 1448_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/1449_0_gt.jpg
Saved label: 1449_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1450_0_gt.jpg
Saved label: 1450_0_gt.jpg	('to',)
Saved image: ./gt_images_test/images_gt/1451_0_gt.jpg
Saved label: 1451_0_gt.jpg	('ri',)
Saved image: ./gt_images_test/images_gt/1452_0_gt.jpg
Saved label: 1452_0_gt.jpg	('ni',)
Saved image: ./gt_images_test/images_gt/1453_0_gt.jpg
Saved label: 1453_0_gt.jpg	('Nh',)
Saved image: ./gt_images_test/images_gt/1454_0_gt.jpg
Saved label: 1454_0_gt.jpg	('Lng',)
Saved image: ./gt_images_test/images_gt/1455_0_gt.jpg
Saved label: 1455_0_gt.jpg	('Trc',)
Saved image: ./gt_images_test/images_gt/1456_0_gt.jpg
Saved label: 1456_0_gt.jpg	('D',)
Saved image: ./gt_images_test/images_gt/1457_0_gt.jpg
Saved label: 1457_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/1458_0_gt.jpg
Saved label: 1458_0_gt.jpg	('y',)
Saved image: ./gt_images_test/images_gt/1459_0_gt.jpg
Saved label: 1459_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/1460_0_gt.jpg
Saved label: 1460_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/1461_0_gt.jpg
Saved label: 1461_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/1462_0_gt.jpg
Saved label: 1462_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1463_0_gt.jpg
Saved label: 1463_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/1464_0_gt.jpg
Saved label: 1464_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/1465_0_gt.jpg
Saved label: 1465_0_gt.jpg	('hoi',)
Saved image: ./gt_images_test/images_gt/1466_0_gt.jpg
Saved label: 1466_0_gt.jpg	('Yu',)
Saved image: ./gt_images_test/images_gt/1467_0_gt.jpg
Saved label: 1467_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/1468_0_gt.jpg
Saved label: 1468_0_gt.jpg	('dong',)
Saved image: ./gt_images_test/images_gt/1469_0_gt.jpg
Saved label: 1469_0_gt.jpg	('chiu',)
Saved image: ./gt_images_test/images_gt/1470_0_gt.jpg
Saved label: 1470_0_gt.jpg	('gi',)
Saved image: ./gt_images_test/images_gt/1471_0_gt.jpg
Saved label: 1471_0_gt.jpg	('thm',)
Saved image: ./gt_images_test/images_gt/1472_0_gt.jpg
Saved label: 1472_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/1473_0_gt.jpg
Saved label: 1473_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/1474_0_gt.jpg
Saved label: 1474_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/1475_0_gt.jpg
Saved label: 1475_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/1476_0_gt.jpg
Saved label: 1476_0_gt.jpg	('thnh',)
Saved image: ./gt_images_test/images_gt/1477_0_gt.jpg
Saved label: 1477_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/1478_0_gt.jpg
Saved label: 1478_0_gt.jpg	('sao',)
Saved image: ./gt_images_test/images_gt/1479_0_gt.jpg
Saved label: 1479_0_gt.jpg	('Bt',)
Saved image: ./gt_images_test/images_gt/1480_0_gt.jpg
Saved label: 1480_0_gt.jpg	('Ly',)
Saved image: ./gt_images_test/images_gt/1481_0_gt.jpg
Saved label: 1481_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/1482_0_gt.jpg
Saved label: 1482_0_gt.jpg	('va',)
Saved image: ./gt_images_test/images_gt/1483_0_gt.jpg
Saved label: 1483_0_gt.jpg	('la',)
Saved image: ./gt_images_test/images_gt/1484_0_gt.jpg
Saved label: 1484_0_gt.jpg	('g',)
Saved image: ./gt_images_test/images_gt/1485_0_gt.jpg
Saved label: 1485_0_gt.jpg	('hiu',)
Saved image: ./gt_images_test/images_gt/1486_0_gt.jpg
Saved label: 1486_0_gt.jpg	('Quyn',)
Saved image: ./gt_images_test/images_gt/1487_0_gt.jpg
Saved label: 1487_0_gt.jpg	('Ngn',)
Saved image: ./gt_images_test/images_gt/1488_0_gt.jpg
Saved label: 1488_0_gt.jpg	('thin',)
Saved image: ./gt_images_test/images_gt/1489_0_gt.jpg
Saved label: 1489_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/1490_0_gt.jpg
Saved label: 1490_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/1491_0_gt.jpg
Saved label: 1491_0_gt.jpg	('Thnh',)
Saved image: ./gt_images_test/images_gt/1492_0_gt.jpg
Saved label: 1492_0_gt.jpg	('nghip',)
Saved image: ./gt_images_test/images_gt/1493_0_gt.jpg
Saved label: 1493_0_gt.jpg	('inh',)
Saved image: ./gt_images_test/images_gt/1494_0_gt.jpg
Saved label: 1494_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/1495_0_gt.jpg
Saved label: 1495_0_gt.jpg	('Trng',)
Saved image: ./gt_images_test/images_gt/1496_0_gt.jpg
Saved label: 1496_0_gt.jpg	('trong',)
Saved image: ./gt_images_test/images_gt/1497_0_gt.jpg
Saved label: 1497_0_gt.jpg	('Tng',)
Saved image: ./gt_images_test/images_gt/1498_0_gt.jpg
Saved label: 1498_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/1499_0_gt.jpg
Saved label: 1499_0_gt.jpg	('hy',)
Saved image: ./gt_images_test/images_gt/1500_0_gt.jpg
Saved label: 1500_0_gt.jpg	('trc',)
Saved image: ./gt_images_test/images_gt/1501_0_gt.jpg
Saved label: 1501_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/1502_0_gt.jpg
Saved label: 1502_0_gt.jpg	('Cha',)
Saved image: ./gt_images_test/images_gt/1503_0_gt.jpg
Saved label: 1503_0_gt.jpg	('xinh',)
Saved image: ./gt_images_test/images_gt/1504_0_gt.jpg
Saved label: 1504_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/1505_0_gt.jpg
Saved label: 1505_0_gt.jpg	('thin',)
Saved image: ./gt_images_test/images_gt/1506_0_gt.jpg
Saved label: 1506_0_gt.jpg	('chng',)
Saved image: ./gt_images_test/images_gt/1507_0_gt.jpg
Saved label: 1507_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1508_0_gt.jpg
Saved label: 1508_0_gt.jpg	('An',)
Saved image: ./gt_images_test/images_gt/1509_0_gt.jpg
Saved label: 1509_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/1510_0_gt.jpg
Saved label: 1510_0_gt.jpg	('bn',)
Saved image: ./gt_images_test/images_gt/1511_0_gt.jpg
Saved label: 1511_0_gt.jpg	('ni',)
Saved image: ./gt_images_test/images_gt/1512_0_gt.jpg
Saved label: 1512_0_gt.jpg	('vng',)
Saved image: ./gt_images_test/images_gt/1513_0_gt.jpg
Saved label: 1513_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/1514_0_gt.jpg
Saved label: 1514_0_gt.jpg	('6',)
Saved image: ./gt_images_test/images_gt/1515_0_gt.jpg
Saved label: 1515_0_gt.jpg	('nim',)
Saved image: ./gt_images_test/images_gt/1516_0_gt.jpg
Saved label: 1516_0_gt.jpg	('ph',)
Saved image: ./gt_images_test/images_gt/1517_0_gt.jpg
Saved label: 1517_0_gt.jpg	('vua',)
Saved image: ./gt_images_test/images_gt/1518_0_gt.jpg
Saved label: 1518_0_gt.jpg	('L',)
Saved image: ./gt_images_test/images_gt/1519_0_gt.jpg
Saved label: 1519_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/1520_0_gt.jpg
Saved label: 1520_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/1521_0_gt.jpg
Saved label: 1521_0_gt.jpg	('khc',)
Saved image: ./gt_images_test/images_gt/1522_0_gt.jpg
Saved label: 1522_0_gt.jpg	('Mc',)
Saved image: ./gt_images_test/images_gt/1523_0_gt.jpg
Saved label: 1523_0_gt.jpg	('heo',)
Saved image: ./gt_images_test/images_gt/1524_0_gt.jpg
Saved label: 1524_0_gt.jpg	('Cha',)
Saved image: ./gt_images_test/images_gt/1525_0_gt.jpg
Saved label: 1525_0_gt.jpg	('nht',)
Saved image: ./gt_images_test/images_gt/1526_0_gt.jpg
Saved label: 1526_0_gt.jpg	('Vt',)
Saved image: ./gt_images_test/images_gt/1527_0_gt.jpg
Saved label: 1527_0_gt.jpg	('chy',)
Saved image: ./gt_images_test/images_gt/1528_0_gt.jpg
Saved label: 1528_0_gt.jpg	('Trm',)
Saved image: ./gt_images_test/images_gt/1529_0_gt.jpg
Saved label: 1529_0_gt.jpg	('Vit',)
Saved image: ./gt_images_test/images_gt/1530_0_gt.jpg
Saved label: 1530_0_gt.jpg	('Hi',)
Saved image: ./gt_images_test/images_gt/1531_0_gt.jpg
Saved label: 1531_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/1532_0_gt.jpg
Saved label: 1532_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/1533_0_gt.jpg
Saved label: 1533_0_gt.jpg	('bay',)
Saved image: ./gt_images_test/images_gt/1534_0_gt.jpg
Saved label: 1534_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/1535_0_gt.jpg
Saved label: 1535_0_gt.jpg	('s',)
Saved image: ./gt_images_test/images_gt/1536_0_gt.jpg
Saved label: 1536_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1537_0_gt.jpg
Saved label: 1537_0_gt.jpg	('tu',)
Saved image: ./gt_images_test/images_gt/1538_0_gt.jpg
Saved label: 1538_0_gt.jpg	('hoa',)
Saved image: ./gt_images_test/images_gt/1539_0_gt.jpg
Saved label: 1539_0_gt.jpg	('vng',)
Saved image: ./gt_images_test/images_gt/1540_0_gt.jpg
Saved label: 1540_0_gt.jpg	('chc',)
Saved image: ./gt_images_test/images_gt/1541_0_gt.jpg
Saved label: 1541_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/1542_0_gt.jpg
Saved label: 1542_0_gt.jpg	('Vin',)
Saved image: ./gt_images_test/images_gt/1543_0_gt.jpg
Saved label: 1543_0_gt.jpg	('Thc',)
Saved image: ./gt_images_test/images_gt/1544_0_gt.jpg
Saved label: 1544_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/1545_0_gt.jpg
Saved label: 1545_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1546_0_gt.jpg
Saved label: 1546_0_gt.jpg	('Thng',)
Saved image: ./gt_images_test/images_gt/1547_0_gt.jpg
Saved label: 1547_0_gt.jpg	('ghen',)
Saved image: ./gt_images_test/images_gt/1548_0_gt.jpg
Saved label: 1548_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/1549_0_gt.jpg
Saved label: 1549_0_gt.jpg	('Vn',)
Saved image: ./gt_images_test/images_gt/1550_0_gt.jpg
Saved label: 1550_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/1551_0_gt.jpg
Saved label: 1551_0_gt.jpg	('sao',)
Saved image: ./gt_images_test/images_gt/1552_0_gt.jpg
Saved label: 1552_0_gt.jpg	('Vng',)
Saved image: ./gt_images_test/images_gt/1553_0_gt.jpg
Saved label: 1553_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/1554_0_gt.jpg
Saved label: 1554_0_gt.jpg	('Tnh',)
Saved image: ./gt_images_test/images_gt/1555_0_gt.jpg
Saved label: 1555_0_gt.jpg	('Bt',)
Saved image: ./gt_images_test/images_gt/1556_0_gt.jpg
Saved label: 1556_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/1557_0_gt.jpg
Saved label: 1557_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1558_0_gt.jpg
Saved label: 1558_0_gt.jpg	('bm',)
Saved image: ./gt_images_test/images_gt/1559_0_gt.jpg
Saved label: 1559_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/1560_0_gt.jpg
Saved label: 1560_0_gt.jpg	('vui',)
Saved image: ./gt_images_test/images_gt/1561_0_gt.jpg
Saved label: 1561_0_gt.jpg	('Ct',)
Saved image: ./gt_images_test/images_gt/1562_0_gt.jpg
Saved label: 1562_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/1563_0_gt.jpg
Saved label: 1563_0_gt.jpg	('Hiu',)
Saved image: ./gt_images_test/images_gt/1564_0_gt.jpg
Saved label: 1564_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/1565_0_gt.jpg
Saved label: 1565_0_gt.jpg	('ct',)
Saved image: ./gt_images_test/images_gt/1566_0_gt.jpg
Saved label: 1566_0_gt.jpg	('nc',)
Saved image: ./gt_images_test/images_gt/1567_0_gt.jpg
Saved label: 1567_0_gt.jpg	('mng',)
Saved image: ./gt_images_test/images_gt/1568_0_gt.jpg
Saved label: 1568_0_gt.jpg	('tng',)
Saved image: ./gt_images_test/images_gt/1569_0_gt.jpg
Saved label: 1569_0_gt.jpg	('Nm',)
Saved image: ./gt_images_test/images_gt/1570_0_gt.jpg
Saved label: 1570_0_gt.jpg	('Hnh',)
Saved image: ./gt_images_test/images_gt/1571_0_gt.jpg
Saved label: 1571_0_gt.jpg	('minh',)
Saved image: ./gt_images_test/images_gt/1572_0_gt.jpg
Saved label: 1572_0_gt.jpg	('chuyn',)
Saved image: ./gt_images_test/images_gt/1573_0_gt.jpg
Saved label: 1573_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/1574_0_gt.jpg
Saved label: 1574_0_gt.jpg	('ngay',)
Saved image: ./gt_images_test/images_gt/1575_0_gt.jpg
Saved label: 1575_0_gt.jpg	('nng',)
Saved image: ./gt_images_test/images_gt/1576_0_gt.jpg
Saved label: 1576_0_gt.jpg	('gi',)
Saved image: ./gt_images_test/images_gt/1577_0_gt.jpg
Saved label: 1577_0_gt.jpg	('Tc',)
Saved image: ./gt_images_test/images_gt/1578_0_gt.jpg
Saved label: 1578_0_gt.jpg	('ni',)
Saved image: ./gt_images_test/images_gt/1579_0_gt.jpg
Saved label: 1579_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/1580_0_gt.jpg
Saved label: 1580_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/1581_0_gt.jpg
Saved label: 1581_0_gt.jpg	('u',)
Saved image: ./gt_images_test/images_gt/1582_0_gt.jpg
Saved label: 1582_0_gt.jpg	('Thi',)
Saved image: ./gt_images_test/images_gt/1583_0_gt.jpg
Saved label: 1583_0_gt.jpg	('Gii',)
Saved image: ./gt_images_test/images_gt/1584_0_gt.jpg
Saved label: 1584_0_gt.jpg	('rng',)
Saved image: ./gt_images_test/images_gt/1585_0_gt.jpg
Saved label: 1585_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1586_0_gt.jpg
Saved label: 1586_0_gt.jpg	('Phng',)
Saved image: ./gt_images_test/images_gt/1587_0_gt.jpg
Saved label: 1587_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1588_0_gt.jpg
Saved label: 1588_0_gt.jpg	('Chia',)
Saved image: ./gt_images_test/images_gt/1589_0_gt.jpg
Saved label: 1589_0_gt.jpg	('Vi',)
Saved image: ./gt_images_test/images_gt/1590_0_gt.jpg
Saved label: 1590_0_gt.jpg	('thi',)
Saved image: ./gt_images_test/images_gt/1591_0_gt.jpg
Saved label: 1591_0_gt.jpg	('s',)
Saved image: ./gt_images_test/images_gt/1592_0_gt.jpg
Saved label: 1592_0_gt.jpg	('cho',)
Saved image: ./gt_images_test/images_gt/1593_0_gt.jpg
Saved label: 1593_0_gt.jpg	('Mi',)
Saved image: ./gt_images_test/images_gt/1594_0_gt.jpg
Saved label: 1594_0_gt.jpg	('mai',)
Saved image: ./gt_images_test/images_gt/1595_0_gt.jpg
Saved label: 1595_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1596_0_gt.jpg
Saved label: 1596_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/1597_0_gt.jpg
Saved label: 1597_0_gt.jpg	('Pht',)
Saved image: ./gt_images_test/images_gt/1598_0_gt.jpg
Saved label: 1598_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/1599_0_gt.jpg
Saved label: 1599_0_gt.jpg	('chc',)
Saved image: ./gt_images_test/images_gt/1600_0_gt.jpg
Saved label: 1600_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1601_0_gt.jpg
Saved label: 1601_0_gt.jpg	('Rng',)
Saved image: ./gt_images_test/images_gt/1602_0_gt.jpg
Saved label: 1602_0_gt.jpg	('e',)
Saved image: ./gt_images_test/images_gt/1603_0_gt.jpg
Saved label: 1603_0_gt.jpg	('rn',)
Saved image: ./gt_images_test/images_gt/1604_0_gt.jpg
Saved label: 1604_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1605_0_gt.jpg
Saved label: 1605_0_gt.jpg	('Cng',)
Saved image: ./gt_images_test/images_gt/1606_0_gt.jpg
Saved label: 1606_0_gt.jpg	('Gia',)
Saved image: ./gt_images_test/images_gt/1607_0_gt.jpg
Saved label: 1607_0_gt.jpg	('git',)
Saved image: ./gt_images_test/images_gt/1608_0_gt.jpg
Saved label: 1608_0_gt.jpg	('Trung',)
Saved image: ./gt_images_test/images_gt/1609_0_gt.jpg
Saved label: 1609_0_gt.jpg	('Vn',)
Saved image: ./gt_images_test/images_gt/1610_0_gt.jpg
Saved label: 1610_0_gt.jpg	('Hn',)
Saved image: ./gt_images_test/images_gt/1611_0_gt.jpg
Saved label: 1611_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/1612_0_gt.jpg
Saved label: 1612_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/1613_0_gt.jpg
Saved label: 1613_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/1614_0_gt.jpg
Saved label: 1614_0_gt.jpg	('qua',)
Saved image: ./gt_images_test/images_gt/1615_0_gt.jpg
Saved label: 1615_0_gt.jpg	('S',)
Saved image: ./gt_images_test/images_gt/1616_0_gt.jpg
Saved label: 1616_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/1617_0_gt.jpg
Saved label: 1617_0_gt.jpg	('thnh',)
Saved image: ./gt_images_test/images_gt/1618_0_gt.jpg
Saved label: 1618_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/1619_0_gt.jpg
Saved label: 1619_0_gt.jpg	('Vn',)
Saved image: ./gt_images_test/images_gt/1620_0_gt.jpg
Saved label: 1620_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/1621_0_gt.jpg
Saved label: 1621_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/1622_0_gt.jpg
Saved label: 1622_0_gt.jpg	('Soi',)
Saved image: ./gt_images_test/images_gt/1623_0_gt.jpg
Saved label: 1623_0_gt.jpg	('hin',)
Saved image: ./gt_images_test/images_gt/1624_0_gt.jpg
Saved label: 1624_0_gt.jpg	('snh',)
Saved image: ./gt_images_test/images_gt/1625_0_gt.jpg
Saved label: 1625_0_gt.jpg	('nit',)
Saved image: ./gt_images_test/images_gt/1626_0_gt.jpg
Saved label: 1626_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/1627_0_gt.jpg
Saved label: 1627_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/1628_0_gt.jpg
Saved label: 1628_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1629_0_gt.jpg
Saved label: 1629_0_gt.jpg	('huyn',)
Saved image: ./gt_images_test/images_gt/1630_0_gt.jpg
Saved label: 1630_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/1631_0_gt.jpg
Saved label: 1631_0_gt.jpg	('gi',)
Saved image: ./gt_images_test/images_gt/1632_0_gt.jpg
Saved label: 1632_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/1633_0_gt.jpg
Saved label: 1633_0_gt.jpg	('nhiu',)
Saved image: ./gt_images_test/images_gt/1634_0_gt.jpg
Saved label: 1634_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/1635_0_gt.jpg
Saved label: 1635_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/1636_0_gt.jpg
Saved label: 1636_0_gt.jpg	('Ph',)
Saved image: ./gt_images_test/images_gt/1637_0_gt.jpg
Saved label: 1637_0_gt.jpg	('chi',)
Saved image: ./gt_images_test/images_gt/1638_0_gt.jpg
Saved label: 1638_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/1639_0_gt.jpg
Saved label: 1639_0_gt.jpg	('Bo',)
Saved image: ./gt_images_test/images_gt/1640_0_gt.jpg
Saved label: 1640_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/1641_0_gt.jpg
Saved label: 1641_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/1642_0_gt.jpg
Saved label: 1642_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/1643_0_gt.jpg
Saved label: 1643_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/1644_0_gt.jpg
Saved label: 1644_0_gt.jpg	('tin',)
Saved image: ./gt_images_test/images_gt/1645_0_gt.jpg
Saved label: 1645_0_gt.jpg	('cy',)
Saved image: ./gt_images_test/images_gt/1646_0_gt.jpg
Saved label: 1646_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/1647_0_gt.jpg
Saved label: 1647_0_gt.jpg	('vic',)
Saved image: ./gt_images_test/images_gt/1648_0_gt.jpg
Saved label: 1648_0_gt.jpg	('Khang',)
Saved image: ./gt_images_test/images_gt/1649_0_gt.jpg
Saved label: 1649_0_gt.jpg	('xun',)
Saved image: ./gt_images_test/images_gt/1650_0_gt.jpg
Saved label: 1650_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/1651_0_gt.jpg
Saved label: 1651_0_gt.jpg	('ngang',)
Saved image: ./gt_images_test/images_gt/1652_0_gt.jpg
Saved label: 1652_0_gt.jpg	('tng',)
Saved image: ./gt_images_test/images_gt/1653_0_gt.jpg
Saved label: 1653_0_gt.jpg	('Thi',)
Saved image: ./gt_images_test/images_gt/1654_0_gt.jpg
Saved label: 1654_0_gt.jpg	('tha',)
Saved image: ./gt_images_test/images_gt/1655_0_gt.jpg
Saved label: 1655_0_gt.jpg	('Ho',)
Saved image: ./gt_images_test/images_gt/1656_0_gt.jpg
Saved label: 1656_0_gt.jpg	('nghip',)
Saved image: ./gt_images_test/images_gt/1657_0_gt.jpg
Saved label: 1657_0_gt.jpg	('tin',)
Saved image: ./gt_images_test/images_gt/1658_0_gt.jpg
Saved label: 1658_0_gt.jpg	('thnh',)
Saved image: ./gt_images_test/images_gt/1659_0_gt.jpg
Saved label: 1659_0_gt.jpg	('Bi',)
Saved image: ./gt_images_test/images_gt/1660_0_gt.jpg
Saved label: 1660_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1661_0_gt.jpg
Saved label: 1661_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/1662_0_gt.jpg
Saved label: 1662_0_gt.jpg	('Nm',)
Saved image: ./gt_images_test/images_gt/1663_0_gt.jpg
Saved label: 1663_0_gt.jpg	('git',)
Saved image: ./gt_images_test/images_gt/1664_0_gt.jpg
Saved label: 1664_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/1665_0_gt.jpg
Saved label: 1665_0_gt.jpg	('ngy',)
Saved image: ./gt_images_test/images_gt/1666_0_gt.jpg
Saved label: 1666_0_gt.jpg	('hm',)
Saved image: ./gt_images_test/images_gt/1667_0_gt.jpg
Saved label: 1667_0_gt.jpg	('p',)
Saved image: ./gt_images_test/images_gt/1668_0_gt.jpg
Saved label: 1668_0_gt.jpg	('thm',)
Saved image: ./gt_images_test/images_gt/1669_0_gt.jpg
Saved label: 1669_0_gt.jpg	('Thi',)
Saved image: ./gt_images_test/images_gt/1670_0_gt.jpg
Saved label: 1670_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/1671_0_gt.jpg
Saved label: 1671_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/1672_0_gt.jpg
Saved label: 1672_0_gt.jpg	('gn',)
Saved image: ./gt_images_test/images_gt/1673_0_gt.jpg
Saved label: 1673_0_gt.jpg	('non',)
Saved image: ./gt_images_test/images_gt/1674_0_gt.jpg
Saved label: 1674_0_gt.jpg	('cu',)
Saved image: ./gt_images_test/images_gt/1675_0_gt.jpg
Saved label: 1675_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/1676_0_gt.jpg
Saved label: 1676_0_gt.jpg	('bn',)
Saved image: ./gt_images_test/images_gt/1677_0_gt.jpg
Saved label: 1677_0_gt.jpg	('bung',)
Saved image: ./gt_images_test/images_gt/1678_0_gt.jpg
Saved label: 1678_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/1679_0_gt.jpg
Saved label: 1679_0_gt.jpg	('hn',)
Saved image: ./gt_images_test/images_gt/1680_0_gt.jpg
Saved label: 1680_0_gt.jpg	('Tnh',)
Saved image: ./gt_images_test/images_gt/1681_0_gt.jpg
Saved label: 1681_0_gt.jpg	('Tho',)
Saved image: ./gt_images_test/images_gt/1682_0_gt.jpg
Saved label: 1682_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1683_0_gt.jpg
Saved label: 1683_0_gt.jpg	('Thng',)
Saved image: ./gt_images_test/images_gt/1684_0_gt.jpg
Saved label: 1684_0_gt.jpg	('Thc',)
Saved image: ./gt_images_test/images_gt/1685_0_gt.jpg
Saved label: 1685_0_gt.jpg	('Sinh',)
Saved image: ./gt_images_test/images_gt/1686_0_gt.jpg
Saved label: 1686_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/1687_0_gt.jpg
Saved label: 1687_0_gt.jpg	('ha',)
Saved image: ./gt_images_test/images_gt/1688_0_gt.jpg
Saved label: 1688_0_gt.jpg	('vi',)
Saved image: ./gt_images_test/images_gt/1689_0_gt.jpg
Saved label: 1689_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1690_0_gt.jpg
Saved label: 1690_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/1691_0_gt.jpg
Saved label: 1691_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1692_0_gt.jpg
Saved label: 1692_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/1693_0_gt.jpg
Saved label: 1693_0_gt.jpg	('yn',)
Saved image: ./gt_images_test/images_gt/1694_0_gt.jpg
Saved label: 1694_0_gt.jpg	('Lc',)
Saved image: ./gt_images_test/images_gt/1695_0_gt.jpg
Saved label: 1695_0_gt.jpg	('Khng',)
Saved image: ./gt_images_test/images_gt/1696_0_gt.jpg
Saved label: 1696_0_gt.jpg	('sinh',)
Saved image: ./gt_images_test/images_gt/1697_0_gt.jpg
Saved label: 1697_0_gt.jpg	('Lc',)
Saved image: ./gt_images_test/images_gt/1698_0_gt.jpg
Saved label: 1698_0_gt.jpg	('thi',)
Saved image: ./gt_images_test/images_gt/1699_0_gt.jpg
Saved label: 1699_0_gt.jpg	('gin',)
Saved image: ./gt_images_test/images_gt/1700_0_gt.jpg
Saved label: 1700_0_gt.jpg	('kh',)
Saved image: ./gt_images_test/images_gt/1701_0_gt.jpg
Saved label: 1701_0_gt.jpg	('gnh',)
Saved image: ./gt_images_test/images_gt/1702_0_gt.jpg
Saved label: 1702_0_gt.jpg	('na',)
Saved image: ./gt_images_test/images_gt/1703_0_gt.jpg
Saved label: 1703_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/1704_0_gt.jpg
Saved label: 1704_0_gt.jpg	('gia',)
Saved image: ./gt_images_test/images_gt/1705_0_gt.jpg
Saved label: 1705_0_gt.jpg	('mnh',)
Saved image: ./gt_images_test/images_gt/1706_0_gt.jpg
Saved label: 1706_0_gt.jpg	('Gt',)
Saved image: ./gt_images_test/images_gt/1707_0_gt.jpg
Saved label: 1707_0_gt.jpg	('Ph',)
Saved image: ./gt_images_test/images_gt/1708_0_gt.jpg
Saved label: 1708_0_gt.jpg	('thi',)
Saved image: ./gt_images_test/images_gt/1709_0_gt.jpg
Saved label: 1709_0_gt.jpg	('ht',)
Saved image: ./gt_images_test/images_gt/1710_0_gt.jpg
Saved label: 1710_0_gt.jpg	('Tng',)
Saved image: ./gt_images_test/images_gt/1711_0_gt.jpg
Saved label: 1711_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1712_0_gt.jpg
Saved label: 1712_0_gt.jpg	('ph',)
Saved image: ./gt_images_test/images_gt/1713_0_gt.jpg
Saved label: 1713_0_gt.jpg	('Vn',)
Saved image: ./gt_images_test/images_gt/1714_0_gt.jpg
Saved label: 1714_0_gt.jpg	('Hiu',)
Saved image: ./gt_images_test/images_gt/1715_0_gt.jpg
Saved label: 1715_0_gt.jpg	('An',)
Saved image: ./gt_images_test/images_gt/1716_0_gt.jpg
Saved label: 1716_0_gt.jpg	('Khch',)
Saved image: ./gt_images_test/images_gt/1717_0_gt.jpg
Saved label: 1717_0_gt.jpg	('thu',)
Saved image: ./gt_images_test/images_gt/1718_0_gt.jpg
Saved label: 1718_0_gt.jpg	('S',)
Saved image: ./gt_images_test/images_gt/1719_0_gt.jpg
Saved label: 1719_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/1720_0_gt.jpg
Saved label: 1720_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/1721_0_gt.jpg
Saved label: 1721_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/1722_0_gt.jpg
Saved label: 1722_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1723_0_gt.jpg
Saved label: 1723_0_gt.jpg	('ung',)
Saved image: ./gt_images_test/images_gt/1724_0_gt.jpg
Saved label: 1724_0_gt.jpg	('Chn',)
Saved image: ./gt_images_test/images_gt/1725_0_gt.jpg
Saved label: 1725_0_gt.jpg	('thy',)
Saved image: ./gt_images_test/images_gt/1726_0_gt.jpg
Saved label: 1726_0_gt.jpg	('thc',)
Saved image: ./gt_images_test/images_gt/1727_0_gt.jpg
Saved label: 1727_0_gt.jpg	('khang',)
Saved image: ./gt_images_test/images_gt/1728_0_gt.jpg
Saved label: 1728_0_gt.jpg	('thun',)
Saved image: ./gt_images_test/images_gt/1729_0_gt.jpg
Saved label: 1729_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/1730_0_gt.jpg
Saved label: 1730_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/1731_0_gt.jpg
Saved label: 1731_0_gt.jpg	('hay',)
Saved image: ./gt_images_test/images_gt/1732_0_gt.jpg
Saved label: 1732_0_gt.jpg	('gian',)
Saved image: ./gt_images_test/images_gt/1733_0_gt.jpg
Saved label: 1733_0_gt.jpg	('ri',)
Saved image: ./gt_images_test/images_gt/1734_0_gt.jpg
Saved label: 1734_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/1735_0_gt.jpg
Saved label: 1735_0_gt.jpg	('Hng',)
Saved image: ./gt_images_test/images_gt/1736_0_gt.jpg
Saved label: 1736_0_gt.jpg	('ta',)
Saved image: ./gt_images_test/images_gt/1737_0_gt.jpg
Saved label: 1737_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1738_0_gt.jpg
Saved label: 1738_0_gt.jpg	('em',)
Saved image: ./gt_images_test/images_gt/1739_0_gt.jpg
Saved label: 1739_0_gt.jpg	('rng',)
Saved image: ./gt_images_test/images_gt/1740_0_gt.jpg
Saved label: 1740_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/1741_0_gt.jpg
Saved label: 1741_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/1742_0_gt.jpg
Saved label: 1742_0_gt.jpg	('Lin',)
Saved image: ./gt_images_test/images_gt/1743_0_gt.jpg
Saved label: 1743_0_gt.jpg	('a',)
Saved image: ./gt_images_test/images_gt/1744_0_gt.jpg
Saved label: 1744_0_gt.jpg	('lm',)
Saved image: ./gt_images_test/images_gt/1745_0_gt.jpg
Saved label: 1745_0_gt.jpg	('Nm',)
Saved image: ./gt_images_test/images_gt/1746_0_gt.jpg
Saved label: 1746_0_gt.jpg	('trc',)
Saved image: ./gt_images_test/images_gt/1747_0_gt.jpg
Saved label: 1747_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/1748_0_gt.jpg
Saved label: 1748_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/1749_0_gt.jpg
Saved label: 1749_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/1750_0_gt.jpg
Saved label: 1750_0_gt.jpg	('Nng',)
Saved image: ./gt_images_test/images_gt/1751_0_gt.jpg
Saved label: 1751_0_gt.jpg	('hng',)
Saved image: ./gt_images_test/images_gt/1752_0_gt.jpg
Saved label: 1752_0_gt.jpg	('phi',)
Saved image: ./gt_images_test/images_gt/1753_0_gt.jpg
Saved label: 1753_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/1754_0_gt.jpg
Saved label: 1754_0_gt.jpg	('nng',)
Saved image: ./gt_images_test/images_gt/1755_0_gt.jpg
Saved label: 1755_0_gt.jpg	('Anh',)
Saved image: ./gt_images_test/images_gt/1756_0_gt.jpg
Saved label: 1756_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/1757_0_gt.jpg
Saved label: 1757_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/1758_0_gt.jpg
Saved label: 1758_0_gt.jpg	('minh',)
Saved image: ./gt_images_test/images_gt/1759_0_gt.jpg
Saved label: 1759_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/1760_0_gt.jpg
Saved label: 1760_0_gt.jpg	('h',)
Saved image: ./gt_images_test/images_gt/1761_0_gt.jpg
Saved label: 1761_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/1762_0_gt.jpg
Saved label: 1762_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/1763_0_gt.jpg
Saved label: 1763_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/1764_0_gt.jpg
Saved label: 1764_0_gt.jpg	('gian',)
Saved image: ./gt_images_test/images_gt/1765_0_gt.jpg
Saved label: 1765_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/1766_0_gt.jpg
Saved label: 1766_0_gt.jpg	('ta',)
Saved image: ./gt_images_test/images_gt/1767_0_gt.jpg
Saved label: 1767_0_gt.jpg	('qun',)
Saved image: ./gt_images_test/images_gt/1768_0_gt.jpg
Saved label: 1768_0_gt.jpg	('Ni',)
Saved image: ./gt_images_test/images_gt/1769_0_gt.jpg
Saved label: 1769_0_gt.jpg	('bc',)
Saved image: ./gt_images_test/images_gt/1770_0_gt.jpg
Saved label: 1770_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/1771_0_gt.jpg
Saved label: 1771_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/1772_0_gt.jpg
Saved label: 1772_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/1773_0_gt.jpg
Saved label: 1773_0_gt.jpg	('Mun',)
Saved image: ./gt_images_test/images_gt/1774_0_gt.jpg
Saved label: 1774_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/1775_0_gt.jpg
Saved label: 1775_0_gt.jpg	('qun',)
Saved image: ./gt_images_test/images_gt/1776_0_gt.jpg
Saved label: 1776_0_gt.jpg	('ty',)
Saved image: ./gt_images_test/images_gt/1777_0_gt.jpg
Saved label: 1777_0_gt.jpg	('mng',)
Saved image: ./gt_images_test/images_gt/1778_0_gt.jpg
Saved label: 1778_0_gt.jpg	('S',)
Saved image: ./gt_images_test/images_gt/1779_0_gt.jpg
Saved label: 1779_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/1780_0_gt.jpg
Saved label: 1780_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/1781_0_gt.jpg
Saved label: 1781_0_gt.jpg	('qua',)
Saved image: ./gt_images_test/images_gt/1782_0_gt.jpg
Saved label: 1782_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/1783_0_gt.jpg
Saved label: 1783_0_gt.jpg	('np',)
Saved image: ./gt_images_test/images_gt/1784_0_gt.jpg
Saved label: 1784_0_gt.jpg	('Lnh',)
Saved image: ./gt_images_test/images_gt/1785_0_gt.jpg
Saved label: 1785_0_gt.jpg	('bi',)
Saved image: ./gt_images_test/images_gt/1786_0_gt.jpg
Saved label: 1786_0_gt.jpg	('thin',)
Saved image: ./gt_images_test/images_gt/1787_0_gt.jpg
Saved label: 1787_0_gt.jpg	('gt',)
Saved image: ./gt_images_test/images_gt/1788_0_gt.jpg
Saved label: 1788_0_gt.jpg	('gian',)
Saved image: ./gt_images_test/images_gt/1789_0_gt.jpg
Saved label: 1789_0_gt.jpg	('hung',)
Saved image: ./gt_images_test/images_gt/1790_0_gt.jpg
Saved label: 1790_0_gt.jpg	('lai',)
Saved image: ./gt_images_test/images_gt/1791_0_gt.jpg
Saved label: 1791_0_gt.jpg	('ngang',)
Saved image: ./gt_images_test/images_gt/1792_0_gt.jpg
Saved label: 1792_0_gt.jpg	('Lan',)
Saved image: ./gt_images_test/images_gt/1793_0_gt.jpg
Saved label: 1793_0_gt.jpg	('Kh',)
Saved image: ./gt_images_test/images_gt/1794_0_gt.jpg
Saved label: 1794_0_gt.jpg	('chng',)
Saved image: ./gt_images_test/images_gt/1795_0_gt.jpg
Saved label: 1795_0_gt.jpg	('bng',)
Saved image: ./gt_images_test/images_gt/1796_0_gt.jpg
Saved label: 1796_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/1797_0_gt.jpg
Saved label: 1797_0_gt.jpg	('vng',)
Saved image: ./gt_images_test/images_gt/1798_0_gt.jpg
Saved label: 1798_0_gt.jpg	('hiu',)
Saved image: ./gt_images_test/images_gt/1799_0_gt.jpg
Saved label: 1799_0_gt.jpg	('sm',)
Saved image: ./gt_images_test/images_gt/1800_0_gt.jpg
Saved label: 1800_0_gt.jpg	('tp',)
Saved image: ./gt_images_test/images_gt/1801_0_gt.jpg
Saved label: 1801_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/1802_0_gt.jpg
Saved label: 1802_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/1803_0_gt.jpg
Saved label: 1803_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/1804_0_gt.jpg
Saved label: 1804_0_gt.jpg	('Khu',)
Saved image: ./gt_images_test/images_gt/1805_0_gt.jpg
Saved label: 1805_0_gt.jpg	('thnh',)
Saved image: ./gt_images_test/images_gt/1806_0_gt.jpg
Saved label: 1806_0_gt.jpg	('hiu',)
Saved image: ./gt_images_test/images_gt/1807_0_gt.jpg
Saved label: 1807_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/1808_0_gt.jpg
Saved label: 1808_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/1809_0_gt.jpg
Saved label: 1809_0_gt.jpg	('Quyn',)
Saved image: ./gt_images_test/images_gt/1810_0_gt.jpg
Saved label: 1810_0_gt.jpg	('bin',)
Saved image: ./gt_images_test/images_gt/1811_0_gt.jpg
Saved label: 1811_0_gt.jpg	('nng',)
Saved image: ./gt_images_test/images_gt/1812_0_gt.jpg
Saved label: 1812_0_gt.jpg	('trang',)
Saved image: ./gt_images_test/images_gt/1813_0_gt.jpg
Saved label: 1813_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/1814_0_gt.jpg
Saved label: 1814_0_gt.jpg	('Dung',)
Saved image: ./gt_images_test/images_gt/1815_0_gt.jpg
Saved label: 1815_0_gt.jpg	('o',)
Saved image: ./gt_images_test/images_gt/1816_0_gt.jpg
Saved label: 1816_0_gt.jpg	('nm',)
Saved image: ./gt_images_test/images_gt/1817_0_gt.jpg
Saved label: 1817_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/1818_0_gt.jpg
Saved label: 1818_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/1819_0_gt.jpg
Saved label: 1819_0_gt.jpg	('thch',)
Saved image: ./gt_images_test/images_gt/1820_0_gt.jpg
Saved label: 1820_0_gt.jpg	('si',)
Saved image: ./gt_images_test/images_gt/1821_0_gt.jpg
Saved label: 1821_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/1822_0_gt.jpg
Saved label: 1822_0_gt.jpg	('nay',)
Saved image: ./gt_images_test/images_gt/1823_0_gt.jpg
Saved label: 1823_0_gt.jpg	('Khng',)
Saved image: ./gt_images_test/images_gt/1824_0_gt.jpg
Saved label: 1824_0_gt.jpg	('o',)
Saved image: ./gt_images_test/images_gt/1825_0_gt.jpg
Saved label: 1825_0_gt.jpg	('kh',)
Saved image: ./gt_images_test/images_gt/1826_0_gt.jpg
Saved label: 1826_0_gt.jpg	('My',)
Saved image: ./gt_images_test/images_gt/1827_0_gt.jpg
Saved label: 1827_0_gt.jpg	('du',)
Saved image: ./gt_images_test/images_gt/1828_0_gt.jpg
Saved label: 1828_0_gt.jpg	('chung',)
Saved image: ./gt_images_test/images_gt/1829_0_gt.jpg
Saved label: 1829_0_gt.jpg	('Tr',)
Saved image: ./gt_images_test/images_gt/1830_0_gt.jpg
Saved label: 1830_0_gt.jpg	('ru',)
Saved image: ./gt_images_test/images_gt/1831_0_gt.jpg
Saved label: 1831_0_gt.jpg	('Ch',)
Saved image: ./gt_images_test/images_gt/1832_0_gt.jpg
Saved label: 1832_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1833_0_gt.jpg
Saved label: 1833_0_gt.jpg	('H',)
Saved image: ./gt_images_test/images_gt/1834_0_gt.jpg
Saved label: 1834_0_gt.jpg	('xui',)
Saved image: ./gt_images_test/images_gt/1835_0_gt.jpg
Saved label: 1835_0_gt.jpg	('ci',)
Saved image: ./gt_images_test/images_gt/1836_0_gt.jpg
Saved label: 1836_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/1837_0_gt.jpg
Saved label: 1837_0_gt.jpg	('Sch',)
Saved image: ./gt_images_test/images_gt/1838_0_gt.jpg
Saved label: 1838_0_gt.jpg	('K',)
Saved image: ./gt_images_test/images_gt/1839_0_gt.jpg
Saved label: 1839_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/1840_0_gt.jpg
Saved label: 1840_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/1841_0_gt.jpg
Saved label: 1841_0_gt.jpg	('nui',)
Saved image: ./gt_images_test/images_gt/1842_0_gt.jpg
Saved label: 1842_0_gt.jpg	('gian',)
Saved image: ./gt_images_test/images_gt/1843_0_gt.jpg
Saved label: 1843_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/1844_0_gt.jpg
Saved label: 1844_0_gt.jpg	('S',)
Saved image: ./gt_images_test/images_gt/1845_0_gt.jpg
Saved label: 1845_0_gt.jpg	('s',)
Saved image: ./gt_images_test/images_gt/1846_0_gt.jpg
Saved label: 1846_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/1847_0_gt.jpg
Saved label: 1847_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/1848_0_gt.jpg
Saved label: 1848_0_gt.jpg	('li',)
Saved image: ./gt_images_test/images_gt/1849_0_gt.jpg
Saved label: 1849_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/1850_0_gt.jpg
Saved label: 1850_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/1851_0_gt.jpg
Saved label: 1851_0_gt.jpg	('thy',)
Saved image: ./gt_images_test/images_gt/1852_0_gt.jpg
Saved label: 1852_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/1853_0_gt.jpg
Saved label: 1853_0_gt.jpg	('tc',)
Saved image: ./gt_images_test/images_gt/1854_0_gt.jpg
Saved label: 1854_0_gt.jpg	('du',)
Saved image: ./gt_images_test/images_gt/1855_0_gt.jpg
Saved label: 1855_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1856_0_gt.jpg
Saved label: 1856_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1857_0_gt.jpg
Saved label: 1857_0_gt.jpg	('bin',)
Saved image: ./gt_images_test/images_gt/1858_0_gt.jpg
Saved label: 1858_0_gt.jpg	('ngy',)
Saved image: ./gt_images_test/images_gt/1859_0_gt.jpg
Saved label: 1859_0_gt.jpg	('xun',)
Saved image: ./gt_images_test/images_gt/1860_0_gt.jpg
Saved label: 1860_0_gt.jpg	('qunh',)
Saved image: ./gt_images_test/images_gt/1861_0_gt.jpg
Saved label: 1861_0_gt.jpg	('Bc',)
Saved image: ./gt_images_test/images_gt/1862_0_gt.jpg
Saved label: 1862_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/1863_0_gt.jpg
Saved label: 1863_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/1864_0_gt.jpg
Saved label: 1864_0_gt.jpg	('y',)
Saved image: ./gt_images_test/images_gt/1865_0_gt.jpg
Saved label: 1865_0_gt.jpg	('ma',)
Saved image: ./gt_images_test/images_gt/1866_0_gt.jpg
Saved label: 1866_0_gt.jpg	('thm',)
Saved image: ./gt_images_test/images_gt/1867_0_gt.jpg
Saved label: 1867_0_gt.jpg	('An',)
Saved image: ./gt_images_test/images_gt/1868_0_gt.jpg
Saved label: 1868_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/1869_0_gt.jpg
Saved label: 1869_0_gt.jpg	('bay',)
Saved image: ./gt_images_test/images_gt/1870_0_gt.jpg
Saved label: 1870_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/1871_0_gt.jpg
Saved label: 1871_0_gt.jpg	('d',)
Saved image: ./gt_images_test/images_gt/1872_0_gt.jpg
Saved label: 1872_0_gt.jpg	('ngoi',)
Saved image: ./gt_images_test/images_gt/1873_0_gt.jpg
Saved label: 1873_0_gt.jpg	('xao',)
Saved image: ./gt_images_test/images_gt/1874_0_gt.jpg
Saved label: 1874_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/1875_0_gt.jpg
Saved label: 1875_0_gt.jpg	('qua',)
Saved image: ./gt_images_test/images_gt/1876_0_gt.jpg
Saved label: 1876_0_gt.jpg	('vi',)
Saved image: ./gt_images_test/images_gt/1877_0_gt.jpg
Saved label: 1877_0_gt.jpg	('ba',)
Saved image: ./gt_images_test/images_gt/1878_0_gt.jpg
Saved label: 1878_0_gt.jpg	('vy',)
Saved image: ./gt_images_test/images_gt/1879_0_gt.jpg
Saved label: 1879_0_gt.jpg	('gy',)
Saved image: ./gt_images_test/images_gt/1880_0_gt.jpg
Saved label: 1880_0_gt.jpg	('gia',)
Saved image: ./gt_images_test/images_gt/1881_0_gt.jpg
Saved label: 1881_0_gt.jpg	('Sc',)
Saved image: ./gt_images_test/images_gt/1882_0_gt.jpg
Saved label: 1882_0_gt.jpg	('thoi',)
Saved image: ./gt_images_test/images_gt/1883_0_gt.jpg
Saved label: 1883_0_gt.jpg	('kin',)
Saved image: ./gt_images_test/images_gt/1884_0_gt.jpg
Saved label: 1884_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/1885_0_gt.jpg
Saved label: 1885_0_gt.jpg	('Thnh',)
Saved image: ./gt_images_test/images_gt/1886_0_gt.jpg
Saved label: 1886_0_gt.jpg	('chuc',)
Saved image: ./gt_images_test/images_gt/1887_0_gt.jpg
Saved label: 1887_0_gt.jpg	('Chn',)
Saved image: ./gt_images_test/images_gt/1888_0_gt.jpg
Saved label: 1888_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/1889_0_gt.jpg
Saved label: 1889_0_gt.jpg	('bnh',)
Saved image: ./gt_images_test/images_gt/1890_0_gt.jpg
Saved label: 1890_0_gt.jpg	('T',)
Saved image: ./gt_images_test/images_gt/1891_0_gt.jpg
Saved label: 1891_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/1892_0_gt.jpg
Saved label: 1892_0_gt.jpg	('Tr',)
Saved image: ./gt_images_test/images_gt/1893_0_gt.jpg
Saved label: 1893_0_gt.jpg	('Knh',)
Saved image: ./gt_images_test/images_gt/1894_0_gt.jpg
Saved label: 1894_0_gt.jpg	('ngy',)
Saved image: ./gt_images_test/images_gt/1895_0_gt.jpg
Saved label: 1895_0_gt.jpg	('Hoa',)
Saved image: ./gt_images_test/images_gt/1896_0_gt.jpg
Saved label: 1896_0_gt.jpg	('Vit',)
Saved image: ./gt_images_test/images_gt/1897_0_gt.jpg
Saved label: 1897_0_gt.jpg	('p',)
Saved image: ./gt_images_test/images_gt/1898_0_gt.jpg
Saved label: 1898_0_gt.jpg	('s',)
Saved image: ./gt_images_test/images_gt/1899_0_gt.jpg
Saved label: 1899_0_gt.jpg	('Quc',)
Saved image: ./gt_images_test/images_gt/1900_0_gt.jpg
Saved label: 1900_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/1901_0_gt.jpg
Saved label: 1901_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1902_0_gt.jpg
Saved label: 1902_0_gt.jpg	('Cn',)
Saved image: ./gt_images_test/images_gt/1903_0_gt.jpg
Saved label: 1903_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/1904_0_gt.jpg
Saved label: 1904_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1905_0_gt.jpg
Saved label: 1905_0_gt.jpg	('C',)
Saved image: ./gt_images_test/images_gt/1906_0_gt.jpg
Saved label: 1906_0_gt.jpg	('yn',)
Saved image: ./gt_images_test/images_gt/1907_0_gt.jpg
Saved label: 1907_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/1908_0_gt.jpg
Saved label: 1908_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/1909_0_gt.jpg
Saved label: 1909_0_gt.jpg	('hon',)
Saved image: ./gt_images_test/images_gt/1910_0_gt.jpg
Saved label: 1910_0_gt.jpg	('Lng',)
Saved image: ./gt_images_test/images_gt/1911_0_gt.jpg
Saved label: 1911_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/1912_0_gt.jpg
Saved label: 1912_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/1913_0_gt.jpg
Saved label: 1913_0_gt.jpg	('H',)
Saved image: ./gt_images_test/images_gt/1914_0_gt.jpg
Saved label: 1914_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/1915_0_gt.jpg
Saved label: 1915_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/1916_0_gt.jpg
Saved label: 1916_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/1917_0_gt.jpg
Saved label: 1917_0_gt.jpg	('Sau',)
Saved image: ./gt_images_test/images_gt/1918_0_gt.jpg
Saved label: 1918_0_gt.jpg	('ni',)
Saved image: ./gt_images_test/images_gt/1919_0_gt.jpg
Saved label: 1919_0_gt.jpg	('Kin',)
Saved image: ./gt_images_test/images_gt/1920_0_gt.jpg
Saved label: 1920_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/1921_0_gt.jpg
Saved label: 1921_0_gt.jpg	('pha',)
Saved image: ./gt_images_test/images_gt/1922_0_gt.jpg
Saved label: 1922_0_gt.jpg	('bun',)
Saved image: ./gt_images_test/images_gt/1923_0_gt.jpg
Saved label: 1923_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/1924_0_gt.jpg
Saved label: 1924_0_gt.jpg	('trong',)
Saved image: ./gt_images_test/images_gt/1925_0_gt.jpg
Saved label: 1925_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/1926_0_gt.jpg
Saved label: 1926_0_gt.jpg	('V',)
Saved image: ./gt_images_test/images_gt/1927_0_gt.jpg
Saved label: 1927_0_gt.jpg	('Vn',)
Saved image: ./gt_images_test/images_gt/1928_0_gt.jpg
Saved label: 1928_0_gt.jpg	('ha',)
Saved image: ./gt_images_test/images_gt/1929_0_gt.jpg
Saved label: 1929_0_gt.jpg	('Thnh',)
Saved image: ./gt_images_test/images_gt/1930_0_gt.jpg
Saved label: 1930_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/1931_0_gt.jpg
Saved label: 1931_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/1932_0_gt.jpg
Saved label: 1932_0_gt.jpg	('Rng',)
Saved image: ./gt_images_test/images_gt/1933_0_gt.jpg
Saved label: 1933_0_gt.jpg	('thanh',)
Saved image: ./gt_images_test/images_gt/1934_0_gt.jpg
Saved label: 1934_0_gt.jpg	('im',)
Saved image: ./gt_images_test/images_gt/1935_0_gt.jpg
Saved label: 1935_0_gt.jpg	('km',)
Saved image: ./gt_images_test/images_gt/1936_0_gt.jpg
Saved label: 1936_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/1937_0_gt.jpg
Saved label: 1937_0_gt.jpg	('Ln',)
Saved image: ./gt_images_test/images_gt/1938_0_gt.jpg
Saved label: 1938_0_gt.jpg	('Thin',)
Saved image: ./gt_images_test/images_gt/1939_0_gt.jpg
Saved label: 1939_0_gt.jpg	('bng',)
Saved image: ./gt_images_test/images_gt/1940_0_gt.jpg
Saved label: 1940_0_gt.jpg	('Ty',)
Saved image: ./gt_images_test/images_gt/1941_0_gt.jpg
Saved label: 1941_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/1942_0_gt.jpg
Saved label: 1942_0_gt.jpg	('Mn',)
Saved image: ./gt_images_test/images_gt/1943_0_gt.jpg
Saved label: 1943_0_gt.jpg	('cht',)
Saved image: ./gt_images_test/images_gt/1944_0_gt.jpg
Saved label: 1944_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/1945_0_gt.jpg
Saved label: 1945_0_gt.jpg	('ca',)
Saved image: ./gt_images_test/images_gt/1946_0_gt.jpg
Saved label: 1946_0_gt.jpg	('coi',)
Saved image: ./gt_images_test/images_gt/1947_0_gt.jpg
Saved label: 1947_0_gt.jpg	('ngun',)
Saved image: ./gt_images_test/images_gt/1948_0_gt.jpg
Saved label: 1948_0_gt.jpg	('Mun',)
Saved image: ./gt_images_test/images_gt/1949_0_gt.jpg
Saved label: 1949_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/1950_0_gt.jpg
Saved label: 1950_0_gt.jpg	('nhin',)
Saved image: ./gt_images_test/images_gt/1951_0_gt.jpg
Saved label: 1951_0_gt.jpg	('ngn',)
Saved image: ./gt_images_test/images_gt/1952_0_gt.jpg
Saved label: 1952_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/1953_0_gt.jpg
Saved label: 1953_0_gt.jpg	('ni',)
Saved image: ./gt_images_test/images_gt/1954_0_gt.jpg
Saved label: 1954_0_gt.jpg	('mn',)
Saved image: ./gt_images_test/images_gt/1955_0_gt.jpg
Saved label: 1955_0_gt.jpg	('vt',)
Saved image: ./gt_images_test/images_gt/1956_0_gt.jpg
Saved label: 1956_0_gt.jpg	('Khoe',)
Saved image: ./gt_images_test/images_gt/1957_0_gt.jpg
Saved label: 1957_0_gt.jpg	('trong',)
Saved image: ./gt_images_test/images_gt/1958_0_gt.jpg
Saved label: 1958_0_gt.jpg	('bt',)
Saved image: ./gt_images_test/images_gt/1959_0_gt.jpg
Saved label: 1959_0_gt.jpg	('tanh',)
Saved image: ./gt_images_test/images_gt/1960_0_gt.jpg
Saved label: 1960_0_gt.jpg	('S',)
Saved image: ./gt_images_test/images_gt/1961_0_gt.jpg
Saved label: 1961_0_gt.jpg	('Toi',)
Saved image: ./gt_images_test/images_gt/1962_0_gt.jpg
Saved label: 1962_0_gt.jpg	('oai',)
Saved image: ./gt_images_test/images_gt/1963_0_gt.jpg
Saved label: 1963_0_gt.jpg	('Ta',)
Saved image: ./gt_images_test/images_gt/1964_0_gt.jpg
Saved label: 1964_0_gt.jpg	('y',)
Saved image: ./gt_images_test/images_gt/1965_0_gt.jpg
Saved label: 1965_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/1966_0_gt.jpg
Saved label: 1966_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/1967_0_gt.jpg
Saved label: 1967_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/1968_0_gt.jpg
Saved label: 1968_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/1969_0_gt.jpg
Saved label: 1969_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/1970_0_gt.jpg
Saved label: 1970_0_gt.jpg	('T',)
Saved image: ./gt_images_test/images_gt/1971_0_gt.jpg
Saved label: 1971_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/1972_0_gt.jpg
Saved label: 1972_0_gt.jpg	('L',)
Saved image: ./gt_images_test/images_gt/1973_0_gt.jpg
Saved label: 1973_0_gt.jpg	('S',)
Saved image: ./gt_images_test/images_gt/1974_0_gt.jpg
Saved label: 1974_0_gt.jpg	('Vn',)
Saved image: ./gt_images_test/images_gt/1975_0_gt.jpg
Saved label: 1975_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/1976_0_gt.jpg
Saved label: 1976_0_gt.jpg	('sinh',)
Saved image: ./gt_images_test/images_gt/1977_0_gt.jpg
Saved label: 1977_0_gt.jpg	('Hin',)
Saved image: ./gt_images_test/images_gt/1978_0_gt.jpg
Saved label: 1978_0_gt.jpg	('hiu',)
Saved image: ./gt_images_test/images_gt/1979_0_gt.jpg
Saved label: 1979_0_gt.jpg	('Cha',)
Saved image: ./gt_images_test/images_gt/1980_0_gt.jpg
Saved label: 1980_0_gt.jpg	('xa',)
Saved image: ./gt_images_test/images_gt/1981_0_gt.jpg
Saved label: 1981_0_gt.jpg	('bn',)
Saved image: ./gt_images_test/images_gt/1982_0_gt.jpg
Saved label: 1982_0_gt.jpg	('ngm',)
Saved image: ./gt_images_test/images_gt/1983_0_gt.jpg
Saved label: 1983_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/1984_0_gt.jpg
Saved label: 1984_0_gt.jpg	('kip',)
Saved image: ./gt_images_test/images_gt/1985_0_gt.jpg
Saved label: 1985_0_gt.jpg	('o',)
Saved image: ./gt_images_test/images_gt/1986_0_gt.jpg
Saved label: 1986_0_gt.jpg	('hn',)
Saved image: ./gt_images_test/images_gt/1987_0_gt.jpg
Saved label: 1987_0_gt.jpg	('Tui',)
Saved image: ./gt_images_test/images_gt/1988_0_gt.jpg
Saved label: 1988_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/1989_0_gt.jpg
Saved label: 1989_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/1990_0_gt.jpg
Saved label: 1990_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/1991_0_gt.jpg
Saved label: 1991_0_gt.jpg	('thy',)
Saved image: ./gt_images_test/images_gt/1992_0_gt.jpg
Saved label: 1992_0_gt.jpg	('Xun',)
Saved image: ./gt_images_test/images_gt/1993_0_gt.jpg
Saved label: 1993_0_gt.jpg	('hng',)
Saved image: ./gt_images_test/images_gt/1994_0_gt.jpg
Saved label: 1994_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/1995_0_gt.jpg
Saved label: 1995_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/1996_0_gt.jpg
Saved label: 1996_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/1997_0_gt.jpg
Saved label: 1997_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/1998_0_gt.jpg
Saved label: 1998_0_gt.jpg	('php',)
Saved image: ./gt_images_test/images_gt/1999_0_gt.jpg
Saved label: 1999_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/2000_0_gt.jpg
Saved label: 2000_0_gt.jpg	('mnh',)
Saved image: ./gt_images_test/images_gt/2001_0_gt.jpg
Saved label: 2001_0_gt.jpg	('canh',)
Saved image: ./gt_images_test/images_gt/2002_0_gt.jpg
Saved label: 2002_0_gt.jpg	('nghin',)
Saved image: ./gt_images_test/images_gt/2003_0_gt.jpg
Saved label: 2003_0_gt.jpg	('sinh',)
Saved image: ./gt_images_test/images_gt/2004_0_gt.jpg
Saved label: 2004_0_gt.jpg	('Ph',)
Saved image: ./gt_images_test/images_gt/2005_0_gt.jpg
Saved label: 2005_0_gt.jpg	('hoa',)
Saved image: ./gt_images_test/images_gt/2006_0_gt.jpg
Saved label: 2006_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/2007_0_gt.jpg
Saved label: 2007_0_gt.jpg	('ci',)
Saved image: ./gt_images_test/images_gt/2008_0_gt.jpg
Saved label: 2008_0_gt.jpg	('sen',)
Saved image: ./gt_images_test/images_gt/2009_0_gt.jpg
Saved label: 2009_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/2010_0_gt.jpg
Saved label: 2010_0_gt.jpg	('Nam',)
Saved image: ./gt_images_test/images_gt/2011_0_gt.jpg
Saved label: 2011_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/2012_0_gt.jpg
Saved label: 2012_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2013_0_gt.jpg
Saved label: 2013_0_gt.jpg	('Hnh',)
Saved image: ./gt_images_test/images_gt/2014_0_gt.jpg
Saved label: 2014_0_gt.jpg	('tra',)
Saved image: ./gt_images_test/images_gt/2015_0_gt.jpg
Saved label: 2015_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/2016_0_gt.jpg
Saved label: 2016_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/2017_0_gt.jpg
Saved label: 2017_0_gt.jpg	('cht',)
Saved image: ./gt_images_test/images_gt/2018_0_gt.jpg
Saved label: 2018_0_gt.jpg	('Xuyn',)
Saved image: ./gt_images_test/images_gt/2019_0_gt.jpg
Saved label: 2019_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/2020_0_gt.jpg
Saved label: 2020_0_gt.jpg	('nm',)
Saved image: ./gt_images_test/images_gt/2021_0_gt.jpg
Saved label: 2021_0_gt.jpg	('o',)
Saved image: ./gt_images_test/images_gt/2022_0_gt.jpg
Saved label: 2022_0_gt.jpg	('Long',)
Saved image: ./gt_images_test/images_gt/2023_0_gt.jpg
Saved label: 2023_0_gt.jpg	('Vin',)
Saved image: ./gt_images_test/images_gt/2024_0_gt.jpg
Saved label: 2024_0_gt.jpg	('Ln',)
Saved image: ./gt_images_test/images_gt/2025_0_gt.jpg
Saved label: 2025_0_gt.jpg	('S',)
Saved image: ./gt_images_test/images_gt/2026_0_gt.jpg
Saved label: 2026_0_gt.jpg	('tranh',)
Saved image: ./gt_images_test/images_gt/2027_0_gt.jpg
Saved label: 2027_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/2028_0_gt.jpg
Saved label: 2028_0_gt.jpg	('A',)
Saved image: ./gt_images_test/images_gt/2029_0_gt.jpg
Saved label: 2029_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/2030_0_gt.jpg
Saved label: 2030_0_gt.jpg	('Dng',)
Saved image: ./gt_images_test/images_gt/2031_0_gt.jpg
Saved label: 2031_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/2032_0_gt.jpg
Saved label: 2032_0_gt.jpg	('trung',)
Saved image: ./gt_images_test/images_gt/2033_0_gt.jpg
Saved label: 2033_0_gt.jpg	('tay',)
Saved image: ./gt_images_test/images_gt/2034_0_gt.jpg
Saved label: 2034_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/2035_0_gt.jpg
Saved label: 2035_0_gt.jpg	('Nh',)
Saved image: ./gt_images_test/images_gt/2036_0_gt.jpg
Saved label: 2036_0_gt.jpg	('chuyn',)
Saved image: ./gt_images_test/images_gt/2037_0_gt.jpg
Saved label: 2037_0_gt.jpg	('Minh',)
Saved image: ./gt_images_test/images_gt/2038_0_gt.jpg
Saved label: 2038_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/2039_0_gt.jpg
Saved label: 2039_0_gt.jpg	('gin',)
Saved image: ./gt_images_test/images_gt/2040_0_gt.jpg
Saved label: 2040_0_gt.jpg	('Tam',)
Saved image: ./gt_images_test/images_gt/2041_0_gt.jpg
Saved label: 2041_0_gt.jpg	('Trnh',)
Saved image: ./gt_images_test/images_gt/2042_0_gt.jpg
Saved label: 2042_0_gt.jpg	('thin',)
Saved image: ./gt_images_test/images_gt/2043_0_gt.jpg
Saved label: 2043_0_gt.jpg	('bng',)
Saved image: ./gt_images_test/images_gt/2044_0_gt.jpg
Saved label: 2044_0_gt.jpg	('thong',)
Saved image: ./gt_images_test/images_gt/2045_0_gt.jpg
Saved label: 2045_0_gt.jpg	('hi',)
Saved image: ./gt_images_test/images_gt/2046_0_gt.jpg
Saved label: 2046_0_gt.jpg	('vi',)
Saved image: ./gt_images_test/images_gt/2047_0_gt.jpg
Saved label: 2047_0_gt.jpg	('bin',)
Saved image: ./gt_images_test/images_gt/2048_0_gt.jpg
Saved label: 2048_0_gt.jpg	('vinh',)
Saved image: ./gt_images_test/images_gt/2049_0_gt.jpg
Saved label: 2049_0_gt.jpg	('vng',)
Saved image: ./gt_images_test/images_gt/2050_0_gt.jpg
Saved label: 2050_0_gt.jpg	('Thin',)
Saved image: ./gt_images_test/images_gt/2051_0_gt.jpg
Saved label: 2051_0_gt.jpg	('Tht',)
Saved image: ./gt_images_test/images_gt/2052_0_gt.jpg
Saved label: 2052_0_gt.jpg	('chi',)
Saved image: ./gt_images_test/images_gt/2053_0_gt.jpg
Saved label: 2053_0_gt.jpg	('Gia',)
Saved image: ./gt_images_test/images_gt/2054_0_gt.jpg
Saved label: 2054_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/2055_0_gt.jpg
Saved label: 2055_0_gt.jpg	('tui',)
Saved image: ./gt_images_test/images_gt/2056_0_gt.jpg
Saved label: 2056_0_gt.jpg	('V',)
Saved image: ./gt_images_test/images_gt/2057_0_gt.jpg
Saved label: 2057_0_gt.jpg	('bin',)
Saved image: ./gt_images_test/images_gt/2058_0_gt.jpg
Saved label: 2058_0_gt.jpg	('V',)
Saved image: ./gt_images_test/images_gt/2059_0_gt.jpg
Saved label: 2059_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/2060_0_gt.jpg
Saved label: 2060_0_gt.jpg	('S',)
Saved image: ./gt_images_test/images_gt/2061_0_gt.jpg
Saved label: 2061_0_gt.jpg	('Ph',)
Saved image: ./gt_images_test/images_gt/2062_0_gt.jpg
Saved label: 2062_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/2063_0_gt.jpg
Saved label: 2063_0_gt.jpg	('Quang',)
Saved image: ./gt_images_test/images_gt/2064_0_gt.jpg
Saved label: 2064_0_gt.jpg	('tc',)
Saved image: ./gt_images_test/images_gt/2065_0_gt.jpg
Saved label: 2065_0_gt.jpg	('hoa',)
Saved image: ./gt_images_test/images_gt/2066_0_gt.jpg
Saved label: 2066_0_gt.jpg	('Lai',)
Saved image: ./gt_images_test/images_gt/2067_0_gt.jpg
Saved label: 2067_0_gt.jpg	('Mng',)
Saved image: ./gt_images_test/images_gt/2068_0_gt.jpg
Saved label: 2068_0_gt.jpg	('Thank',)
Saved image: ./gt_images_test/images_gt/2069_0_gt.jpg
Saved label: 2069_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2070_0_gt.jpg
Saved label: 2070_0_gt.jpg	('duyn',)
Saved image: ./gt_images_test/images_gt/2071_0_gt.jpg
Saved label: 2071_0_gt.jpg	('du',)
Saved image: ./gt_images_test/images_gt/2072_0_gt.jpg
Saved label: 2072_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/2073_0_gt.jpg
Saved label: 2073_0_gt.jpg	('trong',)
Saved image: ./gt_images_test/images_gt/2074_0_gt.jpg
Saved label: 2074_0_gt.jpg	('nin',)
Saved image: ./gt_images_test/images_gt/2075_0_gt.jpg
Saved label: 2075_0_gt.jpg	('on',)
Saved image: ./gt_images_test/images_gt/2076_0_gt.jpg
Saved label: 2076_0_gt.jpg	('p',)
Saved image: ./gt_images_test/images_gt/2077_0_gt.jpg
Saved label: 2077_0_gt.jpg	('An',)
Saved image: ./gt_images_test/images_gt/2078_0_gt.jpg
Saved label: 2078_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/2079_0_gt.jpg
Saved label: 2079_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/2080_0_gt.jpg
Saved label: 2080_0_gt.jpg	('mng',)
Saved image: ./gt_images_test/images_gt/2081_0_gt.jpg
Saved label: 2081_0_gt.jpg	('nhc',)
Saved image: ./gt_images_test/images_gt/2082_0_gt.jpg
Saved label: 2082_0_gt.jpg	('bm',)
Saved image: ./gt_images_test/images_gt/2083_0_gt.jpg
Saved label: 2083_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/2084_0_gt.jpg
Saved label: 2084_0_gt.jpg	('S',)
Saved image: ./gt_images_test/images_gt/2085_0_gt.jpg
Saved label: 2085_0_gt.jpg	('b',)
Saved image: ./gt_images_test/images_gt/2086_0_gt.jpg
Saved label: 2086_0_gt.jpg	('khit',)
Saved image: ./gt_images_test/images_gt/2087_0_gt.jpg
Saved label: 2087_0_gt.jpg	('tng',)
Saved image: ./gt_images_test/images_gt/2088_0_gt.jpg
Saved label: 2088_0_gt.jpg	('tng',)
Saved image: ./gt_images_test/images_gt/2089_0_gt.jpg
Saved label: 2089_0_gt.jpg	('Bnh',)
Saved image: ./gt_images_test/images_gt/2090_0_gt.jpg
Saved label: 2090_0_gt.jpg	('phi',)
Saved image: ./gt_images_test/images_gt/2091_0_gt.jpg
Saved label: 2091_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/2092_0_gt.jpg
Saved label: 2092_0_gt.jpg	('Cng',)
Saved image: ./gt_images_test/images_gt/2093_0_gt.jpg
Saved label: 2093_0_gt.jpg	('Lun',)
Saved image: ./gt_images_test/images_gt/2094_0_gt.jpg
Saved label: 2094_0_gt.jpg	('vi',)
Saved image: ./gt_images_test/images_gt/2095_0_gt.jpg
Saved label: 2095_0_gt.jpg	('anh',)
Saved image: ./gt_images_test/images_gt/2096_0_gt.jpg
Saved label: 2096_0_gt.jpg	('trong',)
Saved image: ./gt_images_test/images_gt/2097_0_gt.jpg
Saved label: 2097_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/2098_0_gt.jpg
Saved label: 2098_0_gt.jpg	('hi',)
Saved image: ./gt_images_test/images_gt/2099_0_gt.jpg
Saved label: 2099_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/2100_0_gt.jpg
Saved label: 2100_0_gt.jpg	('Nhn',)
Saved image: ./gt_images_test/images_gt/2101_0_gt.jpg
Saved label: 2101_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/2102_0_gt.jpg
Saved label: 2102_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/2103_0_gt.jpg
Saved label: 2103_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/2104_0_gt.jpg
Saved label: 2104_0_gt.jpg	('Tam',)
Saved image: ./gt_images_test/images_gt/2105_0_gt.jpg
Saved label: 2105_0_gt.jpg	('Mi',)
Saved image: ./gt_images_test/images_gt/2106_0_gt.jpg
Saved label: 2106_0_gt.jpg	('Nh',)
Saved image: ./gt_images_test/images_gt/2107_0_gt.jpg
Saved label: 2107_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2108_0_gt.jpg
Saved label: 2108_0_gt.jpg	('bin',)
Saved image: ./gt_images_test/images_gt/2109_0_gt.jpg
Saved label: 2109_0_gt.jpg	('Hp',)
Saved image: ./gt_images_test/images_gt/2110_0_gt.jpg
Saved label: 2110_0_gt.jpg	('gi',)
Saved image: ./gt_images_test/images_gt/2111_0_gt.jpg
Saved label: 2111_0_gt.jpg	('Cha',)
Saved image: ./gt_images_test/images_gt/2112_0_gt.jpg
Saved label: 2112_0_gt.jpg	('ngy',)
Saved image: ./gt_images_test/images_gt/2113_0_gt.jpg
Saved label: 2113_0_gt.jpg	('con',)
Saved image: ./gt_images_test/images_gt/2114_0_gt.jpg
Saved label: 2114_0_gt.jpg	('khn',)
Saved image: ./gt_images_test/images_gt/2115_0_gt.jpg
Saved label: 2115_0_gt.jpg	('s',)
Saved image: ./gt_images_test/images_gt/2116_0_gt.jpg
Saved label: 2116_0_gt.jpg	('gian',)
Saved image: ./gt_images_test/images_gt/2117_0_gt.jpg
Saved label: 2117_0_gt.jpg	('chi',)
Saved image: ./gt_images_test/images_gt/2118_0_gt.jpg
Saved label: 2118_0_gt.jpg	('Nh',)
Saved image: ./gt_images_test/images_gt/2119_0_gt.jpg
Saved label: 2119_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/2120_0_gt.jpg
Saved label: 2120_0_gt.jpg	('Bu',)
Saved image: ./gt_images_test/images_gt/2121_0_gt.jpg
Saved label: 2121_0_gt.jpg	('kn',)
Saved image: ./gt_images_test/images_gt/2122_0_gt.jpg
Saved label: 2122_0_gt.jpg	('dng',)
Saved image: ./gt_images_test/images_gt/2123_0_gt.jpg
Saved label: 2123_0_gt.jpg	('bc',)
Saved image: ./gt_images_test/images_gt/2124_0_gt.jpg
Saved label: 2124_0_gt.jpg	('Mng',)
Saved image: ./gt_images_test/images_gt/2125_0_gt.jpg
Saved label: 2125_0_gt.jpg	('Hm',)
Saved image: ./gt_images_test/images_gt/2126_0_gt.jpg
Saved label: 2126_0_gt.jpg	('thip',)
Saved image: ./gt_images_test/images_gt/2127_0_gt.jpg
Saved label: 2127_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/2128_0_gt.jpg
Saved label: 2128_0_gt.jpg	('p',)
Saved image: ./gt_images_test/images_gt/2129_0_gt.jpg
Saved label: 2129_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2130_0_gt.jpg
Saved label: 2130_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/2131_0_gt.jpg
Saved label: 2131_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2132_0_gt.jpg
Saved label: 2132_0_gt.jpg	('git',)
Saved image: ./gt_images_test/images_gt/2133_0_gt.jpg
Saved label: 2133_0_gt.jpg	('Mn',)
Saved image: ./gt_images_test/images_gt/2134_0_gt.jpg
Saved label: 2134_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/2135_0_gt.jpg
Saved label: 2135_0_gt.jpg	('Trit',)
Saved image: ./gt_images_test/images_gt/2136_0_gt.jpg
Saved label: 2136_0_gt.jpg	('ct',)
Saved image: ./gt_images_test/images_gt/2137_0_gt.jpg
Saved label: 2137_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/2138_0_gt.jpg
Saved label: 2138_0_gt.jpg	('Tng',)
Saved image: ./gt_images_test/images_gt/2139_0_gt.jpg
Saved label: 2139_0_gt.jpg	('o',)
Saved image: ./gt_images_test/images_gt/2140_0_gt.jpg
Saved label: 2140_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/2141_0_gt.jpg
Saved label: 2141_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/2142_0_gt.jpg
Saved label: 2142_0_gt.jpg	('khung',)
Saved image: ./gt_images_test/images_gt/2143_0_gt.jpg
Saved label: 2143_0_gt.jpg	('dong',)
Saved image: ./gt_images_test/images_gt/2144_0_gt.jpg
Saved label: 2144_0_gt.jpg	('Hi',)
Saved image: ./gt_images_test/images_gt/2145_0_gt.jpg
Saved label: 2145_0_gt.jpg	('qun',)
Saved image: ./gt_images_test/images_gt/2146_0_gt.jpg
Saved label: 2146_0_gt.jpg	('Trang',)
Saved image: ./gt_images_test/images_gt/2147_0_gt.jpg
Saved label: 2147_0_gt.jpg	('Tng',)
Saved image: ./gt_images_test/images_gt/2148_0_gt.jpg
Saved label: 2148_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/2149_0_gt.jpg
Saved label: 2149_0_gt.jpg	('Nc',)
Saved image: ./gt_images_test/images_gt/2150_0_gt.jpg
Saved label: 2150_0_gt.jpg	('ging',)
Saved image: ./gt_images_test/images_gt/2151_0_gt.jpg
Saved label: 2151_0_gt.jpg	('Vua',)
Saved image: ./gt_images_test/images_gt/2152_0_gt.jpg
Saved label: 2152_0_gt.jpg	('C',)
Saved image: ./gt_images_test/images_gt/2153_0_gt.jpg
Saved label: 2153_0_gt.jpg	('C',)
Saved image: ./gt_images_test/images_gt/2154_0_gt.jpg
Saved label: 2154_0_gt.jpg	('ngun',)
Saved image: ./gt_images_test/images_gt/2155_0_gt.jpg
Saved label: 2155_0_gt.jpg	('vui',)
Saved image: ./gt_images_test/images_gt/2156_0_gt.jpg
Saved label: 2156_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/2157_0_gt.jpg
Saved label: 2157_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/2158_0_gt.jpg
Saved label: 2158_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/2159_0_gt.jpg
Saved label: 2159_0_gt.jpg	('an',)
Saved image: ./gt_images_test/images_gt/2160_0_gt.jpg
Saved label: 2160_0_gt.jpg	('rng',)
Saved image: ./gt_images_test/images_gt/2161_0_gt.jpg
Saved label: 2161_0_gt.jpg	('ngy',)
Saved image: ./gt_images_test/images_gt/2162_0_gt.jpg
Saved label: 2162_0_gt.jpg	('u',)
Saved image: ./gt_images_test/images_gt/2163_0_gt.jpg
Saved label: 2163_0_gt.jpg	('Ta',)
Saved image: ./gt_images_test/images_gt/2164_0_gt.jpg
Saved label: 2164_0_gt.jpg	('thn',)
Saved image: ./gt_images_test/images_gt/2165_0_gt.jpg
Saved label: 2165_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/2166_0_gt.jpg
Saved label: 2166_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/2167_0_gt.jpg
Saved label: 2167_0_gt.jpg	('cho',)
Saved image: ./gt_images_test/images_gt/2168_0_gt.jpg
Saved label: 2168_0_gt.jpg	('o',)
Saved image: ./gt_images_test/images_gt/2169_0_gt.jpg
Saved label: 2169_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/2170_0_gt.jpg
Saved label: 2170_0_gt.jpg	('Ti',)
Saved image: ./gt_images_test/images_gt/2171_0_gt.jpg
Saved label: 2171_0_gt.jpg	('Vit',)
Saved image: ./gt_images_test/images_gt/2172_0_gt.jpg
Saved label: 2172_0_gt.jpg	('cn',)
Saved image: ./gt_images_test/images_gt/2173_0_gt.jpg
Saved label: 2173_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/2174_0_gt.jpg
Saved label: 2174_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/2175_0_gt.jpg
Saved label: 2175_0_gt.jpg	('vi',)
Saved image: ./gt_images_test/images_gt/2176_0_gt.jpg
Saved label: 2176_0_gt.jpg	('la',)
Saved image: ./gt_images_test/images_gt/2177_0_gt.jpg
Saved label: 2177_0_gt.jpg	('men',)
Saved image: ./gt_images_test/images_gt/2178_0_gt.jpg
Saved label: 2178_0_gt.jpg	('nm',)
Saved image: ./gt_images_test/images_gt/2179_0_gt.jpg
Saved label: 2179_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2180_0_gt.jpg
Saved label: 2180_0_gt.jpg	('Chnh',)
Saved image: ./gt_images_test/images_gt/2181_0_gt.jpg
Saved label: 2181_0_gt.jpg	('bnh',)
Saved image: ./gt_images_test/images_gt/2182_0_gt.jpg
Saved label: 2182_0_gt.jpg	('Xun',)
Saved image: ./gt_images_test/images_gt/2183_0_gt.jpg
Saved label: 2183_0_gt.jpg	('nht',)
Saved image: ./gt_images_test/images_gt/2184_0_gt.jpg
Saved label: 2184_0_gt.jpg	('rong',)
Saved image: ./gt_images_test/images_gt/2185_0_gt.jpg
Saved label: 2185_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/2186_0_gt.jpg
Saved label: 2186_0_gt.jpg	('to',)
Saved image: ./gt_images_test/images_gt/2187_0_gt.jpg
Saved label: 2187_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/2188_0_gt.jpg
Saved label: 2188_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/2189_0_gt.jpg
Saved label: 2189_0_gt.jpg	('cha',)
Saved image: ./gt_images_test/images_gt/2190_0_gt.jpg
Saved label: 2190_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/2191_0_gt.jpg
Saved label: 2191_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/2192_0_gt.jpg
Saved label: 2192_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/2193_0_gt.jpg
Saved label: 2193_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/2194_0_gt.jpg
Saved label: 2194_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/2195_0_gt.jpg
Saved label: 2195_0_gt.jpg	('nhng',)
Saved image: ./gt_images_test/images_gt/2196_0_gt.jpg
Saved label: 2196_0_gt.jpg	('duyn',)
Saved image: ./gt_images_test/images_gt/2197_0_gt.jpg
Saved label: 2197_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/2198_0_gt.jpg
Saved label: 2198_0_gt.jpg	('Nhn',)
Saved image: ./gt_images_test/images_gt/2199_0_gt.jpg
Saved label: 2199_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/2200_0_gt.jpg
Saved label: 2200_0_gt.jpg	('thu',)
Saved image: ./gt_images_test/images_gt/2201_0_gt.jpg
Saved label: 2201_0_gt.jpg	('sc',)
Saved image: ./gt_images_test/images_gt/2202_0_gt.jpg
Saved label: 2202_0_gt.jpg	('ngha',)
Saved image: ./gt_images_test/images_gt/2203_0_gt.jpg
Saved label: 2203_0_gt.jpg	('hay',)
Saved image: ./gt_images_test/images_gt/2204_0_gt.jpg
Saved label: 2204_0_gt.jpg	('Hnh',)
Saved image: ./gt_images_test/images_gt/2205_0_gt.jpg
Saved label: 2205_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2206_0_gt.jpg
Saved label: 2206_0_gt.jpg	('bi',)
Saved image: ./gt_images_test/images_gt/2207_0_gt.jpg
Saved label: 2207_0_gt.jpg	('hi',)
Saved image: ./gt_images_test/images_gt/2208_0_gt.jpg
Saved label: 2208_0_gt.jpg	('cht',)
Saved image: ./gt_images_test/images_gt/2209_0_gt.jpg
Saved label: 2209_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/2210_0_gt.jpg
Saved label: 2210_0_gt.jpg	('thiu',)
Saved image: ./gt_images_test/images_gt/2211_0_gt.jpg
Saved label: 2211_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/2212_0_gt.jpg
Saved label: 2212_0_gt.jpg	('Lc',)
Saved image: ./gt_images_test/images_gt/2213_0_gt.jpg
Saved label: 2213_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/2214_0_gt.jpg
Saved label: 2214_0_gt.jpg	('chiu',)
Saved image: ./gt_images_test/images_gt/2215_0_gt.jpg
Saved label: 2215_0_gt.jpg	('Cm',)
Saved image: ./gt_images_test/images_gt/2216_0_gt.jpg
Saved label: 2216_0_gt.jpg	('phng',)
Saved image: ./gt_images_test/images_gt/2217_0_gt.jpg
Saved label: 2217_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2218_0_gt.jpg
Saved label: 2218_0_gt.jpg	('Vn',)
Saved image: ./gt_images_test/images_gt/2219_0_gt.jpg
Saved label: 2219_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/2220_0_gt.jpg
Saved label: 2220_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/2221_0_gt.jpg
Saved label: 2221_0_gt.jpg	('hng',)
Saved image: ./gt_images_test/images_gt/2222_0_gt.jpg
Saved label: 2222_0_gt.jpg	('cn',)
Saved image: ./gt_images_test/images_gt/2223_0_gt.jpg
Saved label: 2223_0_gt.jpg	('xa',)
Saved image: ./gt_images_test/images_gt/2224_0_gt.jpg
Saved label: 2224_0_gt.jpg	('Co',)
Saved image: ./gt_images_test/images_gt/2225_0_gt.jpg
Saved label: 2225_0_gt.jpg	('Vi',)
Saved image: ./gt_images_test/images_gt/2226_0_gt.jpg
Saved label: 2226_0_gt.jpg	('Mc',)
Saved image: ./gt_images_test/images_gt/2227_0_gt.jpg
Saved label: 2227_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/2228_0_gt.jpg
Saved label: 2228_0_gt.jpg	('hiu',)
Saved image: ./gt_images_test/images_gt/2229_0_gt.jpg
Saved label: 2229_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/2230_0_gt.jpg
Saved label: 2230_0_gt.jpg	('Ma',)
Saved image: ./gt_images_test/images_gt/2231_0_gt.jpg
Saved label: 2231_0_gt.jpg	('tu',)
Saved image: ./gt_images_test/images_gt/2232_0_gt.jpg
Saved label: 2232_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/2233_0_gt.jpg
Saved label: 2233_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/2234_0_gt.jpg
Saved label: 2234_0_gt.jpg	('bit',)
Saved image: ./gt_images_test/images_gt/2235_0_gt.jpg
Saved label: 2235_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/2236_0_gt.jpg
Saved label: 2236_0_gt.jpg	('Hnh',)
Saved image: ./gt_images_test/images_gt/2237_0_gt.jpg
Saved label: 2237_0_gt.jpg	('hoa',)
Saved image: ./gt_images_test/images_gt/2238_0_gt.jpg
Saved label: 2238_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/2239_0_gt.jpg
Saved label: 2239_0_gt.jpg	('nc',)
Saved image: ./gt_images_test/images_gt/2240_0_gt.jpg
Saved label: 2240_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/2241_0_gt.jpg
Saved label: 2241_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/2242_0_gt.jpg
Saved label: 2242_0_gt.jpg	('chng',)
Saved image: ./gt_images_test/images_gt/2243_0_gt.jpg
Saved label: 2243_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/2244_0_gt.jpg
Saved label: 2244_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/2245_0_gt.jpg
Saved label: 2245_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/2246_0_gt.jpg
Saved label: 2246_0_gt.jpg	('Kh',)
Saved image: ./gt_images_test/images_gt/2247_0_gt.jpg
Saved label: 2247_0_gt.jpg	('gc',)
Saved image: ./gt_images_test/images_gt/2248_0_gt.jpg
Saved label: 2248_0_gt.jpg	('git',)
Saved image: ./gt_images_test/images_gt/2249_0_gt.jpg
Saved label: 2249_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/2250_0_gt.jpg
Saved label: 2250_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/2251_0_gt.jpg
Saved label: 2251_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/2252_0_gt.jpg
Saved label: 2252_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/2253_0_gt.jpg
Saved label: 2253_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/2254_0_gt.jpg
Saved label: 2254_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/2255_0_gt.jpg
Saved label: 2255_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/2256_0_gt.jpg
Saved label: 2256_0_gt.jpg	('rt',)
Saved image: ./gt_images_test/images_gt/2257_0_gt.jpg
Saved label: 2257_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/2258_0_gt.jpg
Saved label: 2258_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2259_0_gt.jpg
Saved label: 2259_0_gt.jpg	('g',)
Saved image: ./gt_images_test/images_gt/2260_0_gt.jpg
Saved label: 2260_0_gt.jpg	('gnh',)
Saved image: ./gt_images_test/images_gt/2261_0_gt.jpg
Saved label: 2261_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/2262_0_gt.jpg
Saved label: 2262_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/2263_0_gt.jpg
Saved label: 2263_0_gt.jpg	('Cao',)
Saved image: ./gt_images_test/images_gt/2264_0_gt.jpg
Saved label: 2264_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/2265_0_gt.jpg
Saved label: 2265_0_gt.jpg	('Da',)
Saved image: ./gt_images_test/images_gt/2266_0_gt.jpg
Saved label: 2266_0_gt.jpg	('ba',)
Saved image: ./gt_images_test/images_gt/2267_0_gt.jpg
Saved label: 2267_0_gt.jpg	('Mt',)
Saved image: ./gt_images_test/images_gt/2268_0_gt.jpg
Saved label: 2268_0_gt.jpg	('con',)
Saved image: ./gt_images_test/images_gt/2269_0_gt.jpg
Saved label: 2269_0_gt.jpg	('tng',)
Saved image: ./gt_images_test/images_gt/2270_0_gt.jpg
Saved label: 2270_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/2271_0_gt.jpg
Saved label: 2271_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/2272_0_gt.jpg
Saved label: 2272_0_gt.jpg	('ph',)
Saved image: ./gt_images_test/images_gt/2273_0_gt.jpg
Saved label: 2273_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/2274_0_gt.jpg
Saved label: 2274_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/2275_0_gt.jpg
Saved label: 2275_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/2276_0_gt.jpg
Saved label: 2276_0_gt.jpg	('Thn',)
Saved image: ./gt_images_test/images_gt/2277_0_gt.jpg
Saved label: 2277_0_gt.jpg	('Sang',)
Saved image: ./gt_images_test/images_gt/2278_0_gt.jpg
Saved label: 2278_0_gt.jpg	('hin',)
Saved image: ./gt_images_test/images_gt/2279_0_gt.jpg
Saved label: 2279_0_gt.jpg	('con',)
Saved image: ./gt_images_test/images_gt/2280_0_gt.jpg
Saved label: 2280_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/2281_0_gt.jpg
Saved label: 2281_0_gt.jpg	('gia',)
Saved image: ./gt_images_test/images_gt/2282_0_gt.jpg
Saved label: 2282_0_gt.jpg	('phng',)
Saved image: ./gt_images_test/images_gt/2283_0_gt.jpg
Saved label: 2283_0_gt.jpg	('Thi',)
Saved image: ./gt_images_test/images_gt/2284_0_gt.jpg
Saved label: 2284_0_gt.jpg	('Trung',)
Saved image: ./gt_images_test/images_gt/2285_0_gt.jpg
Saved label: 2285_0_gt.jpg	('Si',)
Saved image: ./gt_images_test/images_gt/2286_0_gt.jpg
Saved label: 2286_0_gt.jpg	('thang',)
Saved image: ./gt_images_test/images_gt/2287_0_gt.jpg
Saved label: 2287_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/2288_0_gt.jpg
Saved label: 2288_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/2289_0_gt.jpg
Saved label: 2289_0_gt.jpg	('Mo',)
Saved image: ./gt_images_test/images_gt/2290_0_gt.jpg
Saved label: 2290_0_gt.jpg	('Hoa',)
Saved image: ./gt_images_test/images_gt/2291_0_gt.jpg
Saved label: 2291_0_gt.jpg	('nn',)
Saved image: ./gt_images_test/images_gt/2292_0_gt.jpg
Saved label: 2292_0_gt.jpg	('C',)
Saved image: ./gt_images_test/images_gt/2293_0_gt.jpg
Saved label: 2293_0_gt.jpg	('H',)
Saved image: ./gt_images_test/images_gt/2294_0_gt.jpg
Saved label: 2294_0_gt.jpg	('Thp',)
Saved image: ./gt_images_test/images_gt/2295_0_gt.jpg
Saved label: 2295_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/2296_0_gt.jpg
Saved label: 2296_0_gt.jpg	('Thong',)
Saved image: ./gt_images_test/images_gt/2297_0_gt.jpg
Saved label: 2297_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/2298_0_gt.jpg
Saved label: 2298_0_gt.jpg	('Tit',)
Saved image: ./gt_images_test/images_gt/2299_0_gt.jpg
Saved label: 2299_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/2300_0_gt.jpg
Saved label: 2300_0_gt.jpg	('xa',)
Saved image: ./gt_images_test/images_gt/2301_0_gt.jpg
Saved label: 2301_0_gt.jpg	('trang',)
Saved image: ./gt_images_test/images_gt/2302_0_gt.jpg
Saved label: 2302_0_gt.jpg	('vic',)
Saved image: ./gt_images_test/images_gt/2303_0_gt.jpg
Saved label: 2303_0_gt.jpg	('Lc',)
Saved image: ./gt_images_test/images_gt/2304_0_gt.jpg
Saved label: 2304_0_gt.jpg	('Chc',)
Saved image: ./gt_images_test/images_gt/2305_0_gt.jpg
Saved label: 2305_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/2306_0_gt.jpg
Saved label: 2306_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/2307_0_gt.jpg
Saved label: 2307_0_gt.jpg	('C',)
Saved image: ./gt_images_test/images_gt/2308_0_gt.jpg
Saved label: 2308_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2309_0_gt.jpg
Saved label: 2309_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2310_0_gt.jpg
Saved label: 2310_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/2311_0_gt.jpg
Saved label: 2311_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/2312_0_gt.jpg
Saved label: 2312_0_gt.jpg	('Xa',)
Saved image: ./gt_images_test/images_gt/2313_0_gt.jpg
Saved label: 2313_0_gt.jpg	('Lng',)
Saved image: ./gt_images_test/images_gt/2314_0_gt.jpg
Saved label: 2314_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/2315_0_gt.jpg
Saved label: 2315_0_gt.jpg	('hm',)
Saved image: ./gt_images_test/images_gt/2316_0_gt.jpg
Saved label: 2316_0_gt.jpg	('bt',)
Saved image: ./gt_images_test/images_gt/2317_0_gt.jpg
Saved label: 2317_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/2318_0_gt.jpg
Saved label: 2318_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/2319_0_gt.jpg
Saved label: 2319_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2320_0_gt.jpg
Saved label: 2320_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/2321_0_gt.jpg
Saved label: 2321_0_gt.jpg	('Hnh',)
Saved image: ./gt_images_test/images_gt/2322_0_gt.jpg
Saved label: 2322_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/2323_0_gt.jpg
Saved label: 2323_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/2324_0_gt.jpg
Saved label: 2324_0_gt.jpg	('Bnh',)
Saved image: ./gt_images_test/images_gt/2325_0_gt.jpg
Saved label: 2325_0_gt.jpg	('vinh',)
Saved image: ./gt_images_test/images_gt/2326_0_gt.jpg
Saved label: 2326_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/2327_0_gt.jpg
Saved label: 2327_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/2328_0_gt.jpg
Saved label: 2328_0_gt.jpg	('Hoi',)
Saved image: ./gt_images_test/images_gt/2329_0_gt.jpg
Saved label: 2329_0_gt.jpg	('nghe',)
Saved image: ./gt_images_test/images_gt/2330_0_gt.jpg
Saved label: 2330_0_gt.jpg	('gn',)
Saved image: ./gt_images_test/images_gt/2331_0_gt.jpg
Saved label: 2331_0_gt.jpg	('Hng',)
Saved image: ./gt_images_test/images_gt/2332_0_gt.jpg
Saved label: 2332_0_gt.jpg	('s',)
Saved image: ./gt_images_test/images_gt/2333_0_gt.jpg
Saved label: 2333_0_gt.jpg	('thm',)
Saved image: ./gt_images_test/images_gt/2334_0_gt.jpg
Saved label: 2334_0_gt.jpg	('to',)
Saved image: ./gt_images_test/images_gt/2335_0_gt.jpg
Saved label: 2335_0_gt.jpg	('nm',)
Saved image: ./gt_images_test/images_gt/2336_0_gt.jpg
Saved label: 2336_0_gt.jpg	('Vy',)
Saved image: ./gt_images_test/images_gt/2337_0_gt.jpg
Saved label: 2337_0_gt.jpg	('Yu',)
Saved image: ./gt_images_test/images_gt/2338_0_gt.jpg
Saved label: 2338_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2339_0_gt.jpg
Saved label: 2339_0_gt.jpg	('Bn',)
Saved image: ./gt_images_test/images_gt/2340_0_gt.jpg
Saved label: 2340_0_gt.jpg	('Vinh',)
Saved image: ./gt_images_test/images_gt/2341_0_gt.jpg
Saved label: 2341_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/2342_0_gt.jpg
Saved label: 2342_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/2343_0_gt.jpg
Saved label: 2343_0_gt.jpg	('nhnh',)
Saved image: ./gt_images_test/images_gt/2344_0_gt.jpg
Saved label: 2344_0_gt.jpg	('hiu',)
Saved image: ./gt_images_test/images_gt/2345_0_gt.jpg
Saved label: 2345_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/2346_0_gt.jpg
Saved label: 2346_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/2347_0_gt.jpg
Saved label: 2347_0_gt.jpg	('thong',)
Saved image: ./gt_images_test/images_gt/2348_0_gt.jpg
Saved label: 2348_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/2349_0_gt.jpg
Saved label: 2349_0_gt.jpg	('Font',)
Saved image: ./gt_images_test/images_gt/2350_0_gt.jpg
Saved label: 2350_0_gt.jpg	('sanh',)
Saved image: ./gt_images_test/images_gt/2351_0_gt.jpg
Saved label: 2351_0_gt.jpg	('gi',)
Saved image: ./gt_images_test/images_gt/2352_0_gt.jpg
Saved label: 2352_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/2353_0_gt.jpg
Saved label: 2353_0_gt.jpg	('cn',)
Saved image: ./gt_images_test/images_gt/2354_0_gt.jpg
Saved label: 2354_0_gt.jpg	('ca',)
Saved image: ./gt_images_test/images_gt/2355_0_gt.jpg
Saved label: 2355_0_gt.jpg	('quan',)
Saved image: ./gt_images_test/images_gt/2356_0_gt.jpg
Saved label: 2356_0_gt.jpg	('Pht',)
Saved image: ./gt_images_test/images_gt/2357_0_gt.jpg
Saved label: 2357_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/2358_0_gt.jpg
Saved label: 2358_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/2359_0_gt.jpg
Saved label: 2359_0_gt.jpg	('Vui',)
Saved image: ./gt_images_test/images_gt/2360_0_gt.jpg
Saved label: 2360_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/2361_0_gt.jpg
Saved label: 2361_0_gt.jpg	('tng',)
Saved image: ./gt_images_test/images_gt/2362_0_gt.jpg
Saved label: 2362_0_gt.jpg	('khng',)
Saved image: ./gt_images_test/images_gt/2363_0_gt.jpg
Saved label: 2363_0_gt.jpg	('Vc',)
Saved image: ./gt_images_test/images_gt/2364_0_gt.jpg
Saved label: 2364_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/2365_0_gt.jpg
Saved label: 2365_0_gt.jpg	('Kh',)
Saved image: ./gt_images_test/images_gt/2366_0_gt.jpg
Saved label: 2366_0_gt.jpg	('no',)
Saved image: ./gt_images_test/images_gt/2367_0_gt.jpg
Saved label: 2367_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/2368_0_gt.jpg
Saved label: 2368_0_gt.jpg	('Chy',)
Saved image: ./gt_images_test/images_gt/2369_0_gt.jpg
Saved label: 2369_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/2370_0_gt.jpg
Saved label: 2370_0_gt.jpg	('gi',)
Saved image: ./gt_images_test/images_gt/2371_0_gt.jpg
Saved label: 2371_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/2372_0_gt.jpg
Saved label: 2372_0_gt.jpg	('ht',)
Saved image: ./gt_images_test/images_gt/2373_0_gt.jpg
Saved label: 2373_0_gt.jpg	('ro',)
Saved image: ./gt_images_test/images_gt/2374_0_gt.jpg
Saved label: 2374_0_gt.jpg	('nhin',)
Saved image: ./gt_images_test/images_gt/2375_0_gt.jpg
Saved label: 2375_0_gt.jpg	('lm',)
Saved image: ./gt_images_test/images_gt/2376_0_gt.jpg
Saved label: 2376_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/2377_0_gt.jpg
Saved label: 2377_0_gt.jpg	('mng',)
Saved image: ./gt_images_test/images_gt/2378_0_gt.jpg
Saved label: 2378_0_gt.jpg	('Knh',)
Saved image: ./gt_images_test/images_gt/2379_0_gt.jpg
Saved label: 2379_0_gt.jpg	('ph',)
Saved image: ./gt_images_test/images_gt/2380_0_gt.jpg
Saved label: 2380_0_gt.jpg	('an',)
Saved image: ./gt_images_test/images_gt/2381_0_gt.jpg
Saved label: 2381_0_gt.jpg	('Cng',)
Saved image: ./gt_images_test/images_gt/2382_0_gt.jpg
Saved label: 2382_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/2383_0_gt.jpg
Saved label: 2383_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/2384_0_gt.jpg
Saved label: 2384_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/2385_0_gt.jpg
Saved label: 2385_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/2386_0_gt.jpg
Saved label: 2386_0_gt.jpg	('Knh',)
Saved image: ./gt_images_test/images_gt/2387_0_gt.jpg
Saved label: 2387_0_gt.jpg	('cho',)
Saved image: ./gt_images_test/images_gt/2388_0_gt.jpg
Saved label: 2388_0_gt.jpg	('nm',)
Saved image: ./gt_images_test/images_gt/2389_0_gt.jpg
Saved label: 2389_0_gt.jpg	('Ph',)
Saved image: ./gt_images_test/images_gt/2390_0_gt.jpg
Saved label: 2390_0_gt.jpg	('Thun',)
Saved image: ./gt_images_test/images_gt/2391_0_gt.jpg
Saved label: 2391_0_gt.jpg	('gia',)
Saved image: ./gt_images_test/images_gt/2392_0_gt.jpg
Saved label: 2392_0_gt.jpg	('H',)
Saved image: ./gt_images_test/images_gt/2393_0_gt.jpg
Saved label: 2393_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/2394_0_gt.jpg
Saved label: 2394_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2395_0_gt.jpg
Saved label: 2395_0_gt.jpg	('khnh',)
Saved image: ./gt_images_test/images_gt/2396_0_gt.jpg
Saved label: 2396_0_gt.jpg	('tch',)
Saved image: ./gt_images_test/images_gt/2397_0_gt.jpg
Saved label: 2397_0_gt.jpg	('Phi',)
Saved image: ./gt_images_test/images_gt/2398_0_gt.jpg
Saved label: 2398_0_gt.jpg	('tinh',)
Saved image: ./gt_images_test/images_gt/2399_0_gt.jpg
Saved label: 2399_0_gt.jpg	('hoa',)
Saved image: ./gt_images_test/images_gt/2400_0_gt.jpg
Saved label: 2400_0_gt.jpg	('knh',)
Saved image: ./gt_images_test/images_gt/2401_0_gt.jpg
Saved label: 2401_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/2402_0_gt.jpg
Saved label: 2402_0_gt.jpg	('vng',)
Saved image: ./gt_images_test/images_gt/2403_0_gt.jpg
Saved label: 2403_0_gt.jpg	('bt',)
Saved image: ./gt_images_test/images_gt/2404_0_gt.jpg
Saved label: 2404_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/2405_0_gt.jpg
Saved label: 2405_0_gt.jpg	('My',)
Saved image: ./gt_images_test/images_gt/2406_0_gt.jpg
Saved label: 2406_0_gt.jpg	('php',)
Saved image: ./gt_images_test/images_gt/2407_0_gt.jpg
Saved label: 2407_0_gt.jpg	('Huyn',)
Saved image: ./gt_images_test/images_gt/2408_0_gt.jpg
Saved label: 2408_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/2409_0_gt.jpg
Saved label: 2409_0_gt.jpg	('hin',)
Saved image: ./gt_images_test/images_gt/2410_0_gt.jpg
Saved label: 2410_0_gt.jpg	('giy',)
Saved image: ./gt_images_test/images_gt/2411_0_gt.jpg
Saved label: 2411_0_gt.jpg	('nan',)
Saved image: ./gt_images_test/images_gt/2412_0_gt.jpg
Saved label: 2412_0_gt.jpg	('Ti',)
Saved image: ./gt_images_test/images_gt/2413_0_gt.jpg
Saved label: 2413_0_gt.jpg	('Phong',)
Saved image: ./gt_images_test/images_gt/2414_0_gt.jpg
Saved label: 2414_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/2415_0_gt.jpg
Saved label: 2415_0_gt.jpg	('Bnh',)
Saved image: ./gt_images_test/images_gt/2416_0_gt.jpg
Saved label: 2416_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2417_0_gt.jpg
Saved label: 2417_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/2418_0_gt.jpg
Saved label: 2418_0_gt.jpg	('ting',)
Saved image: ./gt_images_test/images_gt/2419_0_gt.jpg
Saved label: 2419_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/2420_0_gt.jpg
Saved label: 2420_0_gt.jpg	('X',)
Saved image: ./gt_images_test/images_gt/2421_0_gt.jpg
Saved label: 2421_0_gt.jpg	('thm',)
Saved image: ./gt_images_test/images_gt/2422_0_gt.jpg
Saved label: 2422_0_gt.jpg	('bc',)
Saved image: ./gt_images_test/images_gt/2423_0_gt.jpg
Saved label: 2423_0_gt.jpg	('Xun',)
Saved image: ./gt_images_test/images_gt/2424_0_gt.jpg
Saved label: 2424_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/2425_0_gt.jpg
Saved label: 2425_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/2426_0_gt.jpg
Saved label: 2426_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/2427_0_gt.jpg
Saved label: 2427_0_gt.jpg	('vo',)
Saved image: ./gt_images_test/images_gt/2428_0_gt.jpg
Saved label: 2428_0_gt.jpg	('ngy',)
Saved image: ./gt_images_test/images_gt/2429_0_gt.jpg
Saved label: 2429_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/2430_0_gt.jpg
Saved label: 2430_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/2431_0_gt.jpg
Saved label: 2431_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/2432_0_gt.jpg
Saved label: 2432_0_gt.jpg	('Bun',)
Saved image: ./gt_images_test/images_gt/2433_0_gt.jpg
Saved label: 2433_0_gt.jpg	('gin',)
Saved image: ./gt_images_test/images_gt/2434_0_gt.jpg
Saved label: 2434_0_gt.jpg	('thnh',)
Saved image: ./gt_images_test/images_gt/2435_0_gt.jpg
Saved label: 2435_0_gt.jpg	('ngha',)
Saved image: ./gt_images_test/images_gt/2436_0_gt.jpg
Saved label: 2436_0_gt.jpg	('thi',)
Saved image: ./gt_images_test/images_gt/2437_0_gt.jpg
Saved label: 2437_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2438_0_gt.jpg
Saved label: 2438_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/2439_0_gt.jpg
Saved label: 2439_0_gt.jpg	('mnh',)
Saved image: ./gt_images_test/images_gt/2440_0_gt.jpg
Saved label: 2440_0_gt.jpg	('bn',)
Saved image: ./gt_images_test/images_gt/2441_0_gt.jpg
Saved label: 2441_0_gt.jpg	('Hoan',)
Saved image: ./gt_images_test/images_gt/2442_0_gt.jpg
Saved label: 2442_0_gt.jpg	('Tr',)
Saved image: ./gt_images_test/images_gt/2443_0_gt.jpg
Saved label: 2443_0_gt.jpg	('dng',)
Saved image: ./gt_images_test/images_gt/2444_0_gt.jpg
Saved label: 2444_0_gt.jpg	('cht',)
Saved image: ./gt_images_test/images_gt/2445_0_gt.jpg
Saved label: 2445_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/2446_0_gt.jpg
Saved label: 2446_0_gt.jpg	('S',)
Saved image: ./gt_images_test/images_gt/2447_0_gt.jpg
Saved label: 2447_0_gt.jpg	('Thng',)
Saved image: ./gt_images_test/images_gt/2448_0_gt.jpg
Saved label: 2448_0_gt.jpg	('Bit',)
Saved image: ./gt_images_test/images_gt/2449_0_gt.jpg
Saved label: 2449_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/2450_0_gt.jpg
Saved label: 2450_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/2451_0_gt.jpg
Saved label: 2451_0_gt.jpg	('An',)
Saved image: ./gt_images_test/images_gt/2452_0_gt.jpg
Saved label: 2452_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/2453_0_gt.jpg
Saved label: 2453_0_gt.jpg	('gai',)
Saved image: ./gt_images_test/images_gt/2454_0_gt.jpg
Saved label: 2454_0_gt.jpg	('Hnh',)
Saved image: ./gt_images_test/images_gt/2455_0_gt.jpg
Saved label: 2455_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/2456_0_gt.jpg
Saved label: 2456_0_gt.jpg	('li',)
Saved image: ./gt_images_test/images_gt/2457_0_gt.jpg
Saved label: 2457_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2458_0_gt.jpg
Saved label: 2458_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/2459_0_gt.jpg
Saved label: 2459_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/2460_0_gt.jpg
Saved label: 2460_0_gt.jpg	('hn',)
Saved image: ./gt_images_test/images_gt/2461_0_gt.jpg
Saved label: 2461_0_gt.jpg	('vng',)
Saved image: ./gt_images_test/images_gt/2462_0_gt.jpg
Saved label: 2462_0_gt.jpg	('qun',)
Saved image: ./gt_images_test/images_gt/2463_0_gt.jpg
Saved label: 2463_0_gt.jpg	('tui',)
Saved image: ./gt_images_test/images_gt/2464_0_gt.jpg
Saved label: 2464_0_gt.jpg	('ngy',)
Saved image: ./gt_images_test/images_gt/2465_0_gt.jpg
Saved label: 2465_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/2466_0_gt.jpg
Saved label: 2466_0_gt.jpg	('Thanh',)
Saved image: ./gt_images_test/images_gt/2467_0_gt.jpg
Saved label: 2467_0_gt.jpg	('cht',)
Saved image: ./gt_images_test/images_gt/2468_0_gt.jpg
Saved label: 2468_0_gt.jpg	('N',)
Saved image: ./gt_images_test/images_gt/2469_0_gt.jpg
Saved label: 2469_0_gt.jpg	('Hu',)
Saved image: ./gt_images_test/images_gt/2470_0_gt.jpg
Saved label: 2470_0_gt.jpg	('ci',)
Saved image: ./gt_images_test/images_gt/2471_0_gt.jpg
Saved label: 2471_0_gt.jpg	('ro',)
Saved image: ./gt_images_test/images_gt/2472_0_gt.jpg
Saved label: 2472_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/2473_0_gt.jpg
Saved label: 2473_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/2474_0_gt.jpg
Saved label: 2474_0_gt.jpg	('khua',)
Saved image: ./gt_images_test/images_gt/2475_0_gt.jpg
Saved label: 2475_0_gt.jpg	('b',)
Saved image: ./gt_images_test/images_gt/2476_0_gt.jpg
Saved label: 2476_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/2477_0_gt.jpg
Saved label: 2477_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/2478_0_gt.jpg
Saved label: 2478_0_gt.jpg	('Vng',)
Saved image: ./gt_images_test/images_gt/2479_0_gt.jpg
Saved label: 2479_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/2480_0_gt.jpg
Saved label: 2480_0_gt.jpg	('chm',)
Saved image: ./gt_images_test/images_gt/2481_0_gt.jpg
Saved label: 2481_0_gt.jpg	('Nu',)
Saved image: ./gt_images_test/images_gt/2482_0_gt.jpg
Saved label: 2482_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/2483_0_gt.jpg
Saved label: 2483_0_gt.jpg	('yn',)
Saved image: ./gt_images_test/images_gt/2484_0_gt.jpg
Saved label: 2484_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/2485_0_gt.jpg
Saved label: 2485_0_gt.jpg	('x',)
Saved image: ./gt_images_test/images_gt/2486_0_gt.jpg
Saved label: 2486_0_gt.jpg	('u',)
Saved image: ./gt_images_test/images_gt/2487_0_gt.jpg
Saved label: 2487_0_gt.jpg	('minh',)
Saved image: ./gt_images_test/images_gt/2488_0_gt.jpg
Saved label: 2488_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2489_0_gt.jpg
Saved label: 2489_0_gt.jpg	('nc',)
Saved image: ./gt_images_test/images_gt/2490_0_gt.jpg
Saved label: 2490_0_gt.jpg	('mnh',)
Saved image: ./gt_images_test/images_gt/2491_0_gt.jpg
Saved label: 2491_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/2492_0_gt.jpg
Saved label: 2492_0_gt.jpg	('Khoe',)
Saved image: ./gt_images_test/images_gt/2493_0_gt.jpg
Saved label: 2493_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2494_0_gt.jpg
Saved label: 2494_0_gt.jpg	('nhau',)
Saved image: ./gt_images_test/images_gt/2495_0_gt.jpg
Saved label: 2495_0_gt.jpg	('ai',)
Saved image: ./gt_images_test/images_gt/2496_0_gt.jpg
Saved label: 2496_0_gt.jpg	('xun',)
Saved image: ./gt_images_test/images_gt/2497_0_gt.jpg
Saved label: 2497_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/2498_0_gt.jpg
Saved label: 2498_0_gt.jpg	('tuyt',)
Saved image: ./gt_images_test/images_gt/2499_0_gt.jpg
Saved label: 2499_0_gt.jpg	('li',)
Saved image: ./gt_images_test/images_gt/2500_0_gt.jpg
Saved label: 2500_0_gt.jpg	('nhng',)
Saved image: ./gt_images_test/images_gt/2501_0_gt.jpg
Saved label: 2501_0_gt.jpg	('cuc',)
Saved image: ./gt_images_test/images_gt/2502_0_gt.jpg
Saved label: 2502_0_gt.jpg	('ln',)
Saved image: ./gt_images_test/images_gt/2503_0_gt.jpg
Saved label: 2503_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/2504_0_gt.jpg
Saved label: 2504_0_gt.jpg	('May',)
Saved image: ./gt_images_test/images_gt/2505_0_gt.jpg
Saved label: 2505_0_gt.jpg	('Chc',)
Saved image: ./gt_images_test/images_gt/2506_0_gt.jpg
Saved label: 2506_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/2507_0_gt.jpg
Saved label: 2507_0_gt.jpg	('rng',)
Saved image: ./gt_images_test/images_gt/2508_0_gt.jpg
Saved label: 2508_0_gt.jpg	('sm',)
Saved image: ./gt_images_test/images_gt/2509_0_gt.jpg
Saved label: 2509_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2510_0_gt.jpg
Saved label: 2510_0_gt.jpg	('ny',)
Saved image: ./gt_images_test/images_gt/2511_0_gt.jpg
Saved label: 2511_0_gt.jpg	('kh',)
Saved image: ./gt_images_test/images_gt/2512_0_gt.jpg
Saved label: 2512_0_gt.jpg	('gi',)
Saved image: ./gt_images_test/images_gt/2513_0_gt.jpg
Saved label: 2513_0_gt.jpg	('nin',)
Saved image: ./gt_images_test/images_gt/2514_0_gt.jpg
Saved label: 2514_0_gt.jpg	('Bt',)
Saved image: ./gt_images_test/images_gt/2515_0_gt.jpg
Saved label: 2515_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/2516_0_gt.jpg
Saved label: 2516_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/2517_0_gt.jpg
Saved label: 2517_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/2518_0_gt.jpg
Saved label: 2518_0_gt.jpg	('Vng',)
Saved image: ./gt_images_test/images_gt/2519_0_gt.jpg
Saved label: 2519_0_gt.jpg	('tiu',)
Saved image: ./gt_images_test/images_gt/2520_0_gt.jpg
Saved label: 2520_0_gt.jpg	('Ni',)
Saved image: ./gt_images_test/images_gt/2521_0_gt.jpg
Saved label: 2521_0_gt.jpg	('bin',)
Saved image: ./gt_images_test/images_gt/2522_0_gt.jpg
Saved label: 2522_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/2523_0_gt.jpg
Saved label: 2523_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/2524_0_gt.jpg
Saved label: 2524_0_gt.jpg	('Khng',)
Saved image: ./gt_images_test/images_gt/2525_0_gt.jpg
Saved label: 2525_0_gt.jpg	('An',)
Saved image: ./gt_images_test/images_gt/2526_0_gt.jpg
Saved label: 2526_0_gt.jpg	('dy',)
Saved image: ./gt_images_test/images_gt/2527_0_gt.jpg
Saved label: 2527_0_gt.jpg	('nghip',)
Saved image: ./gt_images_test/images_gt/2528_0_gt.jpg
Saved label: 2528_0_gt.jpg	('Bay',)
Saved image: ./gt_images_test/images_gt/2529_0_gt.jpg
Saved label: 2529_0_gt.jpg	('nht',)
Saved image: ./gt_images_test/images_gt/2530_0_gt.jpg
Saved label: 2530_0_gt.jpg	('Nhin',)
Saved image: ./gt_images_test/images_gt/2531_0_gt.jpg
Saved label: 2531_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/2532_0_gt.jpg
Saved label: 2532_0_gt.jpg	('Trn',)
Saved image: ./gt_images_test/images_gt/2533_0_gt.jpg
Saved label: 2533_0_gt.jpg	('ngc',)
Saved image: ./gt_images_test/images_gt/2534_0_gt.jpg
Saved label: 2534_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/2535_0_gt.jpg
Saved label: 2535_0_gt.jpg	('Kiu',)
Saved image: ./gt_images_test/images_gt/2536_0_gt.jpg
Saved label: 2536_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/2537_0_gt.jpg
Saved label: 2537_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/2538_0_gt.jpg
Saved label: 2538_0_gt.jpg	('tiu',)
Saved image: ./gt_images_test/images_gt/2539_0_gt.jpg
Saved label: 2539_0_gt.jpg	('thc',)
Saved image: ./gt_images_test/images_gt/2540_0_gt.jpg
Saved label: 2540_0_gt.jpg	('gi',)
Saved image: ./gt_images_test/images_gt/2541_0_gt.jpg
Saved label: 2541_0_gt.jpg	('Hm',)
Saved image: ./gt_images_test/images_gt/2542_0_gt.jpg
Saved label: 2542_0_gt.jpg	('tui',)
Saved image: ./gt_images_test/images_gt/2543_0_gt.jpg
Saved label: 2543_0_gt.jpg	('tiu',)
Saved image: ./gt_images_test/images_gt/2544_0_gt.jpg
Saved label: 2544_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/2545_0_gt.jpg
Saved label: 2545_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/2546_0_gt.jpg
Saved label: 2546_0_gt.jpg	('cm',)
Saved image: ./gt_images_test/images_gt/2547_0_gt.jpg
Saved label: 2547_0_gt.jpg	('khng',)
Saved image: ./gt_images_test/images_gt/2548_0_gt.jpg
Saved label: 2548_0_gt.jpg	('ngun',)
Saved image: ./gt_images_test/images_gt/2549_0_gt.jpg
Saved label: 2549_0_gt.jpg	('thin',)
Saved image: ./gt_images_test/images_gt/2550_0_gt.jpg
Saved label: 2550_0_gt.jpg	('nu',)
Saved image: ./gt_images_test/images_gt/2551_0_gt.jpg
Saved label: 2551_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/2552_0_gt.jpg
Saved label: 2552_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/2553_0_gt.jpg
Saved label: 2553_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/2554_0_gt.jpg
Saved label: 2554_0_gt.jpg	('khn',)
Saved image: ./gt_images_test/images_gt/2555_0_gt.jpg
Saved label: 2555_0_gt.jpg	('Lu',)
Saved image: ./gt_images_test/images_gt/2556_0_gt.jpg
Saved label: 2556_0_gt.jpg	('nim',)
Saved image: ./gt_images_test/images_gt/2557_0_gt.jpg
Saved label: 2557_0_gt.jpg	('Ni',)
Saved image: ./gt_images_test/images_gt/2558_0_gt.jpg
Saved label: 2558_0_gt.jpg	('Mang',)
Saved image: ./gt_images_test/images_gt/2559_0_gt.jpg
Saved label: 2559_0_gt.jpg	('o',)
Saved image: ./gt_images_test/images_gt/2560_0_gt.jpg
Saved label: 2560_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/2561_0_gt.jpg
Saved label: 2561_0_gt.jpg	('thnh',)
Saved image: ./gt_images_test/images_gt/2562_0_gt.jpg
Saved label: 2562_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/2563_0_gt.jpg
Saved label: 2563_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/2564_0_gt.jpg
Saved label: 2564_0_gt.jpg	('Qun',)
Saved image: ./gt_images_test/images_gt/2565_0_gt.jpg
Saved label: 2565_0_gt.jpg	('gian',)
Saved image: ./gt_images_test/images_gt/2566_0_gt.jpg
Saved label: 2566_0_gt.jpg	('cnh',)
Saved image: ./gt_images_test/images_gt/2567_0_gt.jpg
Saved label: 2567_0_gt.jpg	('mng',)
Saved image: ./gt_images_test/images_gt/2568_0_gt.jpg
Saved label: 2568_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/2569_0_gt.jpg
Saved label: 2569_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/2570_0_gt.jpg
Saved label: 2570_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/2571_0_gt.jpg
Saved label: 2571_0_gt.jpg	('Thy',)
Saved image: ./gt_images_test/images_gt/2572_0_gt.jpg
Saved label: 2572_0_gt.jpg	('u',)
Saved image: ./gt_images_test/images_gt/2573_0_gt.jpg
Saved label: 2573_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/2574_0_gt.jpg
Saved label: 2574_0_gt.jpg	('ngy',)
Saved image: ./gt_images_test/images_gt/2575_0_gt.jpg
Saved label: 2575_0_gt.jpg	('g',)
Saved image: ./gt_images_test/images_gt/2576_0_gt.jpg
Saved label: 2576_0_gt.jpg	('nhy',)
Saved image: ./gt_images_test/images_gt/2577_0_gt.jpg
Saved label: 2577_0_gt.jpg	('vi',)
Saved image: ./gt_images_test/images_gt/2578_0_gt.jpg
Saved label: 2578_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/2579_0_gt.jpg
Saved label: 2579_0_gt.jpg	('vui',)
Saved image: ./gt_images_test/images_gt/2580_0_gt.jpg
Saved label: 2580_0_gt.jpg	('sinh',)
Saved image: ./gt_images_test/images_gt/2581_0_gt.jpg
Saved label: 2581_0_gt.jpg	('bun',)
Saved image: ./gt_images_test/images_gt/2582_0_gt.jpg
Saved label: 2582_0_gt.jpg	('bao',)
Saved image: ./gt_images_test/images_gt/2583_0_gt.jpg
Saved label: 2583_0_gt.jpg	('ngm',)
Saved image: ./gt_images_test/images_gt/2584_0_gt.jpg
Saved label: 2584_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/2585_0_gt.jpg
Saved label: 2585_0_gt.jpg	('Thnh',)
Saved image: ./gt_images_test/images_gt/2586_0_gt.jpg
Saved label: 2586_0_gt.jpg	('theo',)
Saved image: ./gt_images_test/images_gt/2587_0_gt.jpg
Saved label: 2587_0_gt.jpg	('qua',)
Saved image: ./gt_images_test/images_gt/2588_0_gt.jpg
Saved label: 2588_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/2589_0_gt.jpg
Saved label: 2589_0_gt.jpg	('Khn',)
Saved image: ./gt_images_test/images_gt/2590_0_gt.jpg
Saved label: 2590_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/2591_0_gt.jpg
Saved label: 2591_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/2592_0_gt.jpg
Saved label: 2592_0_gt.jpg	('thu',)
Saved image: ./gt_images_test/images_gt/2593_0_gt.jpg
Saved label: 2593_0_gt.jpg	('thang',)
Saved image: ./gt_images_test/images_gt/2594_0_gt.jpg
Saved label: 2594_0_gt.jpg	('khang',)
Saved image: ./gt_images_test/images_gt/2595_0_gt.jpg
Saved label: 2595_0_gt.jpg	('nghn',)
Saved image: ./gt_images_test/images_gt/2596_0_gt.jpg
Saved label: 2596_0_gt.jpg	('Tin',)
Saved image: ./gt_images_test/images_gt/2597_0_gt.jpg
Saved label: 2597_0_gt.jpg	('hi',)
Saved image: ./gt_images_test/images_gt/2598_0_gt.jpg
Saved label: 2598_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/2599_0_gt.jpg
Saved label: 2599_0_gt.jpg	('ta',)
Saved image: ./gt_images_test/images_gt/2600_0_gt.jpg
Saved label: 2600_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/2601_0_gt.jpg
Saved label: 2601_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/2602_0_gt.jpg
Saved label: 2602_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2603_0_gt.jpg
Saved label: 2603_0_gt.jpg	('Bn',)
Saved image: ./gt_images_test/images_gt/2604_0_gt.jpg
Saved label: 2604_0_gt.jpg	('so',)
Saved image: ./gt_images_test/images_gt/2605_0_gt.jpg
Saved label: 2605_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/2606_0_gt.jpg
Saved label: 2606_0_gt.jpg	('dung',)
Saved image: ./gt_images_test/images_gt/2607_0_gt.jpg
Saved label: 2607_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/2608_0_gt.jpg
Saved label: 2608_0_gt.jpg	('N',)
Saved image: ./gt_images_test/images_gt/2609_0_gt.jpg
Saved label: 2609_0_gt.jpg	('thu',)
Saved image: ./gt_images_test/images_gt/2610_0_gt.jpg
Saved label: 2610_0_gt.jpg	('No',)
Saved image: ./gt_images_test/images_gt/2611_0_gt.jpg
Saved label: 2611_0_gt.jpg	('b',)
Saved image: ./gt_images_test/images_gt/2612_0_gt.jpg
Saved label: 2612_0_gt.jpg	('Kinh',)
Saved image: ./gt_images_test/images_gt/2613_0_gt.jpg
Saved label: 2613_0_gt.jpg	('ny',)
Saved image: ./gt_images_test/images_gt/2614_0_gt.jpg
Saved label: 2614_0_gt.jpg	('Cnh',)
Saved image: ./gt_images_test/images_gt/2615_0_gt.jpg
Saved label: 2615_0_gt.jpg	('Vui',)
Saved image: ./gt_images_test/images_gt/2616_0_gt.jpg
Saved label: 2616_0_gt.jpg	('a',)
Saved image: ./gt_images_test/images_gt/2617_0_gt.jpg
Saved label: 2617_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/2618_0_gt.jpg
Saved label: 2618_0_gt.jpg	('Ngi',)
Saved image: ./gt_images_test/images_gt/2619_0_gt.jpg
Saved label: 2619_0_gt.jpg	('Sen',)
Saved image: ./gt_images_test/images_gt/2620_0_gt.jpg
Saved label: 2620_0_gt.jpg	('Thin',)
Saved image: ./gt_images_test/images_gt/2621_0_gt.jpg
Saved label: 2621_0_gt.jpg	('vng',)
Saved image: ./gt_images_test/images_gt/2622_0_gt.jpg
Saved label: 2622_0_gt.jpg	('Hoi',)
Saved image: ./gt_images_test/images_gt/2623_0_gt.jpg
Saved label: 2623_0_gt.jpg	('Sung',)
Saved image: ./gt_images_test/images_gt/2624_0_gt.jpg
Saved label: 2624_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/2625_0_gt.jpg
Saved label: 2625_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/2626_0_gt.jpg
Saved label: 2626_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/2627_0_gt.jpg
Saved label: 2627_0_gt.jpg	('ym',)
Saved image: ./gt_images_test/images_gt/2628_0_gt.jpg
Saved label: 2628_0_gt.jpg	('Php',)
Saved image: ./gt_images_test/images_gt/2629_0_gt.jpg
Saved label: 2629_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/2630_0_gt.jpg
Saved label: 2630_0_gt.jpg	('anh',)
Saved image: ./gt_images_test/images_gt/2631_0_gt.jpg
Saved label: 2631_0_gt.jpg	('thnh',)
Saved image: ./gt_images_test/images_gt/2632_0_gt.jpg
Saved label: 2632_0_gt.jpg	('Bch',)
Saved image: ./gt_images_test/images_gt/2633_0_gt.jpg
Saved label: 2633_0_gt.jpg	('Chuyn',)
Saved image: ./gt_images_test/images_gt/2634_0_gt.jpg
Saved label: 2634_0_gt.jpg	('Khoy',)
Saved image: ./gt_images_test/images_gt/2635_0_gt.jpg
Saved label: 2635_0_gt.jpg	('chy',)
Saved image: ./gt_images_test/images_gt/2636_0_gt.jpg
Saved label: 2636_0_gt.jpg	('Thnh',)
Saved image: ./gt_images_test/images_gt/2637_0_gt.jpg
Saved label: 2637_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/2638_0_gt.jpg
Saved label: 2638_0_gt.jpg	('Khp',)
Saved image: ./gt_images_test/images_gt/2639_0_gt.jpg
Saved label: 2639_0_gt.jpg	('bt',)
Saved image: ./gt_images_test/images_gt/2640_0_gt.jpg
Saved label: 2640_0_gt.jpg	('xun',)
Saved image: ./gt_images_test/images_gt/2641_0_gt.jpg
Saved label: 2641_0_gt.jpg	('luyn',)
Saved image: ./gt_images_test/images_gt/2642_0_gt.jpg
Saved label: 2642_0_gt.jpg	('Khang',)
Saved image: ./gt_images_test/images_gt/2643_0_gt.jpg
Saved label: 2643_0_gt.jpg	('gng',)
Saved image: ./gt_images_test/images_gt/2644_0_gt.jpg
Saved label: 2644_0_gt.jpg	('Lun',)
Saved image: ./gt_images_test/images_gt/2645_0_gt.jpg
Saved label: 2645_0_gt.jpg	('Thng',)
Saved image: ./gt_images_test/images_gt/2646_0_gt.jpg
Saved label: 2646_0_gt.jpg	('bin',)
Saved image: ./gt_images_test/images_gt/2647_0_gt.jpg
Saved label: 2647_0_gt.jpg	('Lc',)
Saved image: ./gt_images_test/images_gt/2648_0_gt.jpg
Saved label: 2648_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/2649_0_gt.jpg
Saved label: 2649_0_gt.jpg	('chng',)
Saved image: ./gt_images_test/images_gt/2650_0_gt.jpg
Saved label: 2650_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/2651_0_gt.jpg
Saved label: 2651_0_gt.jpg	('Cu',)
Saved image: ./gt_images_test/images_gt/2652_0_gt.jpg
Saved label: 2652_0_gt.jpg	('chng',)
Saved image: ./gt_images_test/images_gt/2653_0_gt.jpg
Saved label: 2653_0_gt.jpg	('chng',)
Saved image: ./gt_images_test/images_gt/2654_0_gt.jpg
Saved label: 2654_0_gt.jpg	('xanh',)
Saved image: ./gt_images_test/images_gt/2655_0_gt.jpg
Saved label: 2655_0_gt.jpg	('Vn',)
Saved image: ./gt_images_test/images_gt/2656_0_gt.jpg
Saved label: 2656_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/2657_0_gt.jpg
Saved label: 2657_0_gt.jpg	('Ngc',)
Saved image: ./gt_images_test/images_gt/2658_0_gt.jpg
Saved label: 2658_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/2659_0_gt.jpg
Saved label: 2659_0_gt.jpg	('Vn',)
Saved image: ./gt_images_test/images_gt/2660_0_gt.jpg
Saved label: 2660_0_gt.jpg	('Vin',)
Saved image: ./gt_images_test/images_gt/2661_0_gt.jpg
Saved label: 2661_0_gt.jpg	('chng',)
Saved image: ./gt_images_test/images_gt/2662_0_gt.jpg
Saved label: 2662_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2663_0_gt.jpg
Saved label: 2663_0_gt.jpg	('cht',)
Saved image: ./gt_images_test/images_gt/2664_0_gt.jpg
Saved label: 2664_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/2665_0_gt.jpg
Saved label: 2665_0_gt.jpg	('Yu',)
Saved image: ./gt_images_test/images_gt/2666_0_gt.jpg
Saved label: 2666_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/2667_0_gt.jpg
Saved label: 2667_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/2668_0_gt.jpg
Saved label: 2668_0_gt.jpg	('Dc',)
Saved image: ./gt_images_test/images_gt/2669_0_gt.jpg
Saved label: 2669_0_gt.jpg	('Trng',)
Saved image: ./gt_images_test/images_gt/2670_0_gt.jpg
Saved label: 2670_0_gt.jpg	('Tnh',)
Saved image: ./gt_images_test/images_gt/2671_0_gt.jpg
Saved label: 2671_0_gt.jpg	('an',)
Saved image: ./gt_images_test/images_gt/2672_0_gt.jpg
Saved label: 2672_0_gt.jpg	('An',)
Saved image: ./gt_images_test/images_gt/2673_0_gt.jpg
Saved label: 2673_0_gt.jpg	('mnh',)
Saved image: ./gt_images_test/images_gt/2674_0_gt.jpg
Saved label: 2674_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2675_0_gt.jpg
Saved label: 2675_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/2676_0_gt.jpg
Saved label: 2676_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/2677_0_gt.jpg
Saved label: 2677_0_gt.jpg	('gp',)
Saved image: ./gt_images_test/images_gt/2678_0_gt.jpg
Saved label: 2678_0_gt.jpg	('chi',)
Saved image: ./gt_images_test/images_gt/2679_0_gt.jpg
Saved label: 2679_0_gt.jpg	('tham',)
Saved image: ./gt_images_test/images_gt/2680_0_gt.jpg
Saved label: 2680_0_gt.jpg	('nay',)
Saved image: ./gt_images_test/images_gt/2681_0_gt.jpg
Saved label: 2681_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/2682_0_gt.jpg
Saved label: 2682_0_gt.jpg	('niu',)
Saved image: ./gt_images_test/images_gt/2683_0_gt.jpg
Saved label: 2683_0_gt.jpg	('ta',)
Saved image: ./gt_images_test/images_gt/2684_0_gt.jpg
Saved label: 2684_0_gt.jpg	('ra',)
Saved image: ./gt_images_test/images_gt/2685_0_gt.jpg
Saved label: 2685_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/2686_0_gt.jpg
Saved label: 2686_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/2687_0_gt.jpg
Saved label: 2687_0_gt.jpg	('gii',)
Saved image: ./gt_images_test/images_gt/2688_0_gt.jpg
Saved label: 2688_0_gt.jpg	('tht',)
Saved image: ./gt_images_test/images_gt/2689_0_gt.jpg
Saved label: 2689_0_gt.jpg	('dong',)
Saved image: ./gt_images_test/images_gt/2690_0_gt.jpg
Saved label: 2690_0_gt.jpg	('Cung',)
Saved image: ./gt_images_test/images_gt/2691_0_gt.jpg
Saved label: 2691_0_gt.jpg	('Qunh',)
Saved image: ./gt_images_test/images_gt/2692_0_gt.jpg
Saved label: 2692_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/2693_0_gt.jpg
Saved label: 2693_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/2694_0_gt.jpg
Saved label: 2694_0_gt.jpg	('Khai',)
Saved image: ./gt_images_test/images_gt/2695_0_gt.jpg
Saved label: 2695_0_gt.jpg	('sao',)
Saved image: ./gt_images_test/images_gt/2696_0_gt.jpg
Saved label: 2696_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/2697_0_gt.jpg
Saved label: 2697_0_gt.jpg	('Thng',)
Saved image: ./gt_images_test/images_gt/2698_0_gt.jpg
Saved label: 2698_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/2699_0_gt.jpg
Saved label: 2699_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2700_0_gt.jpg
Saved label: 2700_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/2701_0_gt.jpg
Saved label: 2701_0_gt.jpg	('Thi',)
Saved image: ./gt_images_test/images_gt/2702_0_gt.jpg
Saved label: 2702_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/2703_0_gt.jpg
Saved label: 2703_0_gt.jpg	('chng',)
Saved image: ./gt_images_test/images_gt/2704_0_gt.jpg
Saved label: 2704_0_gt.jpg	('vi',)
Saved image: ./gt_images_test/images_gt/2705_0_gt.jpg
Saved label: 2705_0_gt.jpg	('Nm',)
Saved image: ./gt_images_test/images_gt/2706_0_gt.jpg
Saved label: 2706_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/2707_0_gt.jpg
Saved label: 2707_0_gt.jpg	('Trm',)
Saved image: ./gt_images_test/images_gt/2708_0_gt.jpg
Saved label: 2708_0_gt.jpg	('Tho',)
Saved image: ./gt_images_test/images_gt/2709_0_gt.jpg
Saved label: 2709_0_gt.jpg	('quyn',)
Saved image: ./gt_images_test/images_gt/2710_0_gt.jpg
Saved label: 2710_0_gt.jpg	('tch',)
Saved image: ./gt_images_test/images_gt/2711_0_gt.jpg
Saved label: 2711_0_gt.jpg	('nng',)
Saved image: ./gt_images_test/images_gt/2712_0_gt.jpg
Saved label: 2712_0_gt.jpg	('Lng',)
Saved image: ./gt_images_test/images_gt/2713_0_gt.jpg
Saved label: 2713_0_gt.jpg	('kp',)
Saved image: ./gt_images_test/images_gt/2714_0_gt.jpg
Saved label: 2714_0_gt.jpg	('Thin',)
Saved image: ./gt_images_test/images_gt/2715_0_gt.jpg
Saved label: 2715_0_gt.jpg	('Trng',)
Saved image: ./gt_images_test/images_gt/2716_0_gt.jpg
Saved label: 2716_0_gt.jpg	('d',)
Saved image: ./gt_images_test/images_gt/2717_0_gt.jpg
Saved label: 2717_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/2718_0_gt.jpg
Saved label: 2718_0_gt.jpg	('khch',)
Saved image: ./gt_images_test/images_gt/2719_0_gt.jpg
Saved label: 2719_0_gt.jpg	('ph',)
Saved image: ./gt_images_test/images_gt/2720_0_gt.jpg
Saved label: 2720_0_gt.jpg	('chc',)
Saved image: ./gt_images_test/images_gt/2721_0_gt.jpg
Saved label: 2721_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/2722_0_gt.jpg
Saved label: 2722_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/2723_0_gt.jpg
Saved label: 2723_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/2724_0_gt.jpg
Saved label: 2724_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/2725_0_gt.jpg
Saved label: 2725_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/2726_0_gt.jpg
Saved label: 2726_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2727_0_gt.jpg
Saved label: 2727_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/2728_0_gt.jpg
Saved label: 2728_0_gt.jpg	('lo',)
Saved image: ./gt_images_test/images_gt/2729_0_gt.jpg
Saved label: 2729_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/2730_0_gt.jpg
Saved label: 2730_0_gt.jpg	('y',)
Saved image: ./gt_images_test/images_gt/2731_0_gt.jpg
Saved label: 2731_0_gt.jpg	('s',)
Saved image: ./gt_images_test/images_gt/2732_0_gt.jpg
Saved label: 2732_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/2733_0_gt.jpg
Saved label: 2733_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2734_0_gt.jpg
Saved label: 2734_0_gt.jpg	('gia',)
Saved image: ./gt_images_test/images_gt/2735_0_gt.jpg
Saved label: 2735_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/2736_0_gt.jpg
Saved label: 2736_0_gt.jpg	('Hng',)
Saved image: ./gt_images_test/images_gt/2737_0_gt.jpg
Saved label: 2737_0_gt.jpg	('Phch',)
Saved image: ./gt_images_test/images_gt/2738_0_gt.jpg
Saved label: 2738_0_gt.jpg	('ht',)
Saved image: ./gt_images_test/images_gt/2739_0_gt.jpg
Saved label: 2739_0_gt.jpg	('nng',)
Saved image: ./gt_images_test/images_gt/2740_0_gt.jpg
Saved label: 2740_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/2741_0_gt.jpg
Saved label: 2741_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/2742_0_gt.jpg
Saved label: 2742_0_gt.jpg	('an',)
Saved image: ./gt_images_test/images_gt/2743_0_gt.jpg
Saved label: 2743_0_gt.jpg	('Ci',)
Saved image: ./gt_images_test/images_gt/2744_0_gt.jpg
Saved label: 2744_0_gt.jpg	('Ta',)
Saved image: ./gt_images_test/images_gt/2745_0_gt.jpg
Saved label: 2745_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/2746_0_gt.jpg
Saved label: 2746_0_gt.jpg	('mng',)
Saved image: ./gt_images_test/images_gt/2747_0_gt.jpg
Saved label: 2747_0_gt.jpg	('cht',)
Saved image: ./gt_images_test/images_gt/2748_0_gt.jpg
Saved label: 2748_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/2749_0_gt.jpg
Saved label: 2749_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/2750_0_gt.jpg
Saved label: 2750_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2751_0_gt.jpg
Saved label: 2751_0_gt.jpg	('tn',)
Saved image: ./gt_images_test/images_gt/2752_0_gt.jpg
Saved label: 2752_0_gt.jpg	('tu',)
Saved image: ./gt_images_test/images_gt/2753_0_gt.jpg
Saved label: 2753_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/2754_0_gt.jpg
Saved label: 2754_0_gt.jpg	('tr',)
Saved image: ./gt_images_test/images_gt/2755_0_gt.jpg
Saved label: 2755_0_gt.jpg	('cnh',)
Saved image: ./gt_images_test/images_gt/2756_0_gt.jpg
Saved label: 2756_0_gt.jpg	('Phong',)
Saved image: ./gt_images_test/images_gt/2757_0_gt.jpg
Saved label: 2757_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/2758_0_gt.jpg
Saved label: 2758_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/2759_0_gt.jpg
Saved label: 2759_0_gt.jpg	('Ra',)
Saved image: ./gt_images_test/images_gt/2760_0_gt.jpg
Saved label: 2760_0_gt.jpg	('Hy',)
Saved image: ./gt_images_test/images_gt/2761_0_gt.jpg
Saved label: 2761_0_gt.jpg	('CHC',)
Saved image: ./gt_images_test/images_gt/2762_0_gt.jpg
Saved label: 2762_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/2763_0_gt.jpg
Saved label: 2763_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/2764_0_gt.jpg
Saved label: 2764_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/2765_0_gt.jpg
Saved label: 2765_0_gt.jpg	('tc',)
Saved image: ./gt_images_test/images_gt/2766_0_gt.jpg
Saved label: 2766_0_gt.jpg	('lai',)
Saved image: ./gt_images_test/images_gt/2767_0_gt.jpg
Saved label: 2767_0_gt.jpg	('hnh',)
Saved image: ./gt_images_test/images_gt/2768_0_gt.jpg
Saved label: 2768_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/2769_0_gt.jpg
Saved label: 2769_0_gt.jpg	('ci',)
Saved image: ./gt_images_test/images_gt/2770_0_gt.jpg
Saved label: 2770_0_gt.jpg	('li',)
Saved image: ./gt_images_test/images_gt/2771_0_gt.jpg
Saved label: 2771_0_gt.jpg	('To',)
Saved image: ./gt_images_test/images_gt/2772_0_gt.jpg
Saved label: 2772_0_gt.jpg	('Tng',)
Saved image: ./gt_images_test/images_gt/2773_0_gt.jpg
Saved label: 2773_0_gt.jpg	('Hi',)
Saved image: ./gt_images_test/images_gt/2774_0_gt.jpg
Saved label: 2774_0_gt.jpg	('H',)
Saved image: ./gt_images_test/images_gt/2775_0_gt.jpg
Saved label: 2775_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/2776_0_gt.jpg
Saved label: 2776_0_gt.jpg	('k',)
Saved image: ./gt_images_test/images_gt/2777_0_gt.jpg
Saved label: 2777_0_gt.jpg	('chu',)
Saved image: ./gt_images_test/images_gt/2778_0_gt.jpg
Saved label: 2778_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/2779_0_gt.jpg
Saved label: 2779_0_gt.jpg	('sn',)
Saved image: ./gt_images_test/images_gt/2780_0_gt.jpg
Saved label: 2780_0_gt.jpg	('Lc',)
Saved image: ./gt_images_test/images_gt/2781_0_gt.jpg
Saved label: 2781_0_gt.jpg	('nht',)
Saved image: ./gt_images_test/images_gt/2782_0_gt.jpg
Saved label: 2782_0_gt.jpg	('Tr',)
Saved image: ./gt_images_test/images_gt/2783_0_gt.jpg
Saved label: 2783_0_gt.jpg	('Thun',)
Saved image: ./gt_images_test/images_gt/2784_0_gt.jpg
Saved label: 2784_0_gt.jpg	('Em',)
Saved image: ./gt_images_test/images_gt/2785_0_gt.jpg
Saved label: 2785_0_gt.jpg	('khang',)
Saved image: ./gt_images_test/images_gt/2786_0_gt.jpg
Saved label: 2786_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/2787_0_gt.jpg
Saved label: 2787_0_gt.jpg	('Vn',)
Saved image: ./gt_images_test/images_gt/2788_0_gt.jpg
Saved label: 2788_0_gt.jpg	('tro',)
Saved image: ./gt_images_test/images_gt/2789_0_gt.jpg
Saved label: 2789_0_gt.jpg	('Sc',)
Saved image: ./gt_images_test/images_gt/2790_0_gt.jpg
Saved label: 2790_0_gt.jpg	('Lu',)
Saved image: ./gt_images_test/images_gt/2791_0_gt.jpg
Saved label: 2791_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/2792_0_gt.jpg
Saved label: 2792_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2793_0_gt.jpg
Saved label: 2793_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/2794_0_gt.jpg
Saved label: 2794_0_gt.jpg	('Phn',)
Saved image: ./gt_images_test/images_gt/2795_0_gt.jpg
Saved label: 2795_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/2796_0_gt.jpg
Saved label: 2796_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/2797_0_gt.jpg
Saved label: 2797_0_gt.jpg	('Con',)
Saved image: ./gt_images_test/images_gt/2798_0_gt.jpg
Saved label: 2798_0_gt.jpg	('chi',)
Saved image: ./gt_images_test/images_gt/2799_0_gt.jpg
Saved label: 2799_0_gt.jpg	('h',)
Saved image: ./gt_images_test/images_gt/2800_0_gt.jpg
Saved label: 2800_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/2801_0_gt.jpg
Saved label: 2801_0_gt.jpg	('ngc',)
Saved image: ./gt_images_test/images_gt/2802_0_gt.jpg
Saved label: 2802_0_gt.jpg	('thm',)
Saved image: ./gt_images_test/images_gt/2803_0_gt.jpg
Saved label: 2803_0_gt.jpg	('nguyn',)
Saved image: ./gt_images_test/images_gt/2804_0_gt.jpg
Saved label: 2804_0_gt.jpg	('ngi',)
Saved image: ./gt_images_test/images_gt/2805_0_gt.jpg
Saved label: 2805_0_gt.jpg	('Hng',)
Saved image: ./gt_images_test/images_gt/2806_0_gt.jpg
Saved label: 2806_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/2807_0_gt.jpg
Saved label: 2807_0_gt.jpg	('thn',)
Saved image: ./gt_images_test/images_gt/2808_0_gt.jpg
Saved label: 2808_0_gt.jpg	('Thnh',)
Saved image: ./gt_images_test/images_gt/2809_0_gt.jpg
Saved label: 2809_0_gt.jpg	('bit',)
Saved image: ./gt_images_test/images_gt/2810_0_gt.jpg
Saved label: 2810_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/2811_0_gt.jpg
Saved label: 2811_0_gt.jpg	('minh',)
Saved image: ./gt_images_test/images_gt/2812_0_gt.jpg
Saved label: 2812_0_gt.jpg	('mn',)
Saved image: ./gt_images_test/images_gt/2813_0_gt.jpg
Saved label: 2813_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/2814_0_gt.jpg
Saved label: 2814_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/2815_0_gt.jpg
Saved label: 2815_0_gt.jpg	('vng',)
Saved image: ./gt_images_test/images_gt/2816_0_gt.jpg
Saved label: 2816_0_gt.jpg	('Rt',)
Saved image: ./gt_images_test/images_gt/2817_0_gt.jpg
Saved label: 2817_0_gt.jpg	('Khang',)
Saved image: ./gt_images_test/images_gt/2818_0_gt.jpg
Saved label: 2818_0_gt.jpg	('Xun',)
Saved image: ./gt_images_test/images_gt/2819_0_gt.jpg
Saved label: 2819_0_gt.jpg	('Cha',)
Saved image: ./gt_images_test/images_gt/2820_0_gt.jpg
Saved label: 2820_0_gt.jpg	('Sang',)
Saved image: ./gt_images_test/images_gt/2821_0_gt.jpg
Saved label: 2821_0_gt.jpg	('bao',)
Saved image: ./gt_images_test/images_gt/2822_0_gt.jpg
Saved label: 2822_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/2823_0_gt.jpg
Saved label: 2823_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/2824_0_gt.jpg
Saved label: 2824_0_gt.jpg	('tri',)
Saved image: ./gt_images_test/images_gt/2825_0_gt.jpg
Saved label: 2825_0_gt.jpg	('nghinh',)
Saved image: ./gt_images_test/images_gt/2826_0_gt.jpg
Saved label: 2826_0_gt.jpg	('bc',)
Saved image: ./gt_images_test/images_gt/2827_0_gt.jpg
Saved label: 2827_0_gt.jpg	('bung',)
Saved image: ./gt_images_test/images_gt/2828_0_gt.jpg
Saved label: 2828_0_gt.jpg	('Sng',)
Saved image: ./gt_images_test/images_gt/2829_0_gt.jpg
Saved label: 2829_0_gt.jpg	('trc',)
Saved image: ./gt_images_test/images_gt/2830_0_gt.jpg
Saved label: 2830_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/2831_0_gt.jpg
Saved label: 2831_0_gt.jpg	('ngy',)
Saved image: ./gt_images_test/images_gt/2832_0_gt.jpg
Saved label: 2832_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/2833_0_gt.jpg
Saved label: 2833_0_gt.jpg	('an',)
Saved image: ./gt_images_test/images_gt/2834_0_gt.jpg
Saved label: 2834_0_gt.jpg	('nguyn',)
Saved image: ./gt_images_test/images_gt/2835_0_gt.jpg
Saved label: 2835_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/2836_0_gt.jpg
Saved label: 2836_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/2837_0_gt.jpg
Saved label: 2837_0_gt.jpg	('Cu',)
Saved image: ./gt_images_test/images_gt/2838_0_gt.jpg
Saved label: 2838_0_gt.jpg	('nng',)
Saved image: ./gt_images_test/images_gt/2839_0_gt.jpg
Saved label: 2839_0_gt.jpg	('dng',)
Saved image: ./gt_images_test/images_gt/2840_0_gt.jpg
Saved label: 2840_0_gt.jpg	('Ti',)
Saved image: ./gt_images_test/images_gt/2841_0_gt.jpg
Saved label: 2841_0_gt.jpg	('Tr',)
Saved image: ./gt_images_test/images_gt/2842_0_gt.jpg
Saved label: 2842_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/2843_0_gt.jpg
Saved label: 2843_0_gt.jpg	('lnh',)
Saved image: ./gt_images_test/images_gt/2844_0_gt.jpg
Saved label: 2844_0_gt.jpg	('thi',)
Saved image: ./gt_images_test/images_gt/2845_0_gt.jpg
Saved label: 2845_0_gt.jpg	('nm',)
Saved image: ./gt_images_test/images_gt/2846_0_gt.jpg
Saved label: 2846_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/2847_0_gt.jpg
Saved label: 2847_0_gt.jpg	('lnh',)
Saved image: ./gt_images_test/images_gt/2848_0_gt.jpg
Saved label: 2848_0_gt.jpg	('Ng',)
Saved image: ./gt_images_test/images_gt/2849_0_gt.jpg
Saved label: 2849_0_gt.jpg	('rng',)
Saved image: ./gt_images_test/images_gt/2850_0_gt.jpg
Saved label: 2850_0_gt.jpg	('bay',)
Saved image: ./gt_images_test/images_gt/2851_0_gt.jpg
Saved label: 2851_0_gt.jpg	('y',)
Saved image: ./gt_images_test/images_gt/2852_0_gt.jpg
Saved label: 2852_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/2853_0_gt.jpg
Saved label: 2853_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2854_0_gt.jpg
Saved label: 2854_0_gt.jpg	('ln',)
Saved image: ./gt_images_test/images_gt/2855_0_gt.jpg
Saved label: 2855_0_gt.jpg	('Mc',)
Saved image: ./gt_images_test/images_gt/2856_0_gt.jpg
Saved label: 2856_0_gt.jpg	('Chc',)
Saved image: ./gt_images_test/images_gt/2857_0_gt.jpg
Saved label: 2857_0_gt.jpg	('hn',)
Saved image: ./gt_images_test/images_gt/2858_0_gt.jpg
Saved label: 2858_0_gt.jpg	('quay',)
Saved image: ./gt_images_test/images_gt/2859_0_gt.jpg
Saved label: 2859_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/2860_0_gt.jpg
Saved label: 2860_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/2861_0_gt.jpg
Saved label: 2861_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/2862_0_gt.jpg
Saved label: 2862_0_gt.jpg	('em',)
Saved image: ./gt_images_test/images_gt/2863_0_gt.jpg
Saved label: 2863_0_gt.jpg	('b',)
Saved image: ./gt_images_test/images_gt/2864_0_gt.jpg
Saved label: 2864_0_gt.jpg	('Mi',)
Saved image: ./gt_images_test/images_gt/2865_0_gt.jpg
Saved label: 2865_0_gt.jpg	('trn',)
Saved image: ./gt_images_test/images_gt/2866_0_gt.jpg
Saved label: 2866_0_gt.jpg	('bi',)
Saved image: ./gt_images_test/images_gt/2867_0_gt.jpg
Saved label: 2867_0_gt.jpg	('do',)
Saved image: ./gt_images_test/images_gt/2868_0_gt.jpg
Saved label: 2868_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/2869_0_gt.jpg
Saved label: 2869_0_gt.jpg	('thnh',)
Saved image: ./gt_images_test/images_gt/2870_0_gt.jpg
Saved label: 2870_0_gt.jpg	('chng',)
Saved image: ./gt_images_test/images_gt/2871_0_gt.jpg
Saved label: 2871_0_gt.jpg	('tin',)
Saved image: ./gt_images_test/images_gt/2872_0_gt.jpg
Saved label: 2872_0_gt.jpg	('Gn',)
Saved image: ./gt_images_test/images_gt/2873_0_gt.jpg
Saved label: 2873_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/2874_0_gt.jpg
Saved label: 2874_0_gt.jpg	('mun',)
Saved image: ./gt_images_test/images_gt/2875_0_gt.jpg
Saved label: 2875_0_gt.jpg	('Ngt',)
Saved image: ./gt_images_test/images_gt/2876_0_gt.jpg
Saved label: 2876_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/2877_0_gt.jpg
Saved label: 2877_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/2878_0_gt.jpg
Saved label: 2878_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/2879_0_gt.jpg
Saved label: 2879_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/2880_0_gt.jpg
Saved label: 2880_0_gt.jpg	('vi',)
Saved image: ./gt_images_test/images_gt/2881_0_gt.jpg
Saved label: 2881_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/2882_0_gt.jpg
Saved label: 2882_0_gt.jpg	('bng',)
Saved image: ./gt_images_test/images_gt/2883_0_gt.jpg
Saved label: 2883_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/2884_0_gt.jpg
Saved label: 2884_0_gt.jpg	('ni',)
Saved image: ./gt_images_test/images_gt/2885_0_gt.jpg
Saved label: 2885_0_gt.jpg	('bch',)
Saved image: ./gt_images_test/images_gt/2886_0_gt.jpg
Saved label: 2886_0_gt.jpg	('vn',)
Saved image: ./gt_images_test/images_gt/2887_0_gt.jpg
Saved label: 2887_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/2888_0_gt.jpg
Saved label: 2888_0_gt.jpg	('danh',)
Saved image: ./gt_images_test/images_gt/2889_0_gt.jpg
Saved label: 2889_0_gt.jpg	('nin',)
Saved image: ./gt_images_test/images_gt/2890_0_gt.jpg
Saved label: 2890_0_gt.jpg	('bun',)
Saved image: ./gt_images_test/images_gt/2891_0_gt.jpg
Saved label: 2891_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/2892_0_gt.jpg
Saved label: 2892_0_gt.jpg	('Kn',)
Saved image: ./gt_images_test/images_gt/2893_0_gt.jpg
Saved label: 2893_0_gt.jpg	('Khi',)
Saved image: ./gt_images_test/images_gt/2894_0_gt.jpg
Saved label: 2894_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/2895_0_gt.jpg
Saved label: 2895_0_gt.jpg	('Bc',)
Saved image: ./gt_images_test/images_gt/2896_0_gt.jpg
Saved label: 2896_0_gt.jpg	('gi',)
Saved image: ./gt_images_test/images_gt/2897_0_gt.jpg
Saved label: 2897_0_gt.jpg	('Lp',)
Saved image: ./gt_images_test/images_gt/2898_0_gt.jpg
Saved label: 2898_0_gt.jpg	('Vn',)
Saved image: ./gt_images_test/images_gt/2899_0_gt.jpg
Saved label: 2899_0_gt.jpg	('thuyn',)
Saved image: ./gt_images_test/images_gt/2900_0_gt.jpg
Saved label: 2900_0_gt.jpg	('gi',)
Saved image: ./gt_images_test/images_gt/2901_0_gt.jpg
Saved label: 2901_0_gt.jpg	('Cn',)
Saved image: ./gt_images_test/images_gt/2902_0_gt.jpg
Saved label: 2902_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/2903_0_gt.jpg
Saved label: 2903_0_gt.jpg	('yu',)
Saved image: ./gt_images_test/images_gt/2904_0_gt.jpg
Saved label: 2904_0_gt.jpg	('Trc',)
Saved image: ./gt_images_test/images_gt/2905_0_gt.jpg
Saved label: 2905_0_gt.jpg	('Khng',)
Saved image: ./gt_images_test/images_gt/2906_0_gt.jpg
Saved label: 2906_0_gt.jpg	('thm',)
Saved image: ./gt_images_test/images_gt/2907_0_gt.jpg
Saved label: 2907_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/2908_0_gt.jpg
Saved label: 2908_0_gt.jpg	('Thi',)
Saved image: ./gt_images_test/images_gt/2909_0_gt.jpg
Saved label: 2909_0_gt.jpg	('mng',)
Saved image: ./gt_images_test/images_gt/2910_0_gt.jpg
Saved label: 2910_0_gt.jpg	('ri',)
Saved image: ./gt_images_test/images_gt/2911_0_gt.jpg
Saved label: 2911_0_gt.jpg	('h',)
Saved image: ./gt_images_test/images_gt/2912_0_gt.jpg
Saved label: 2912_0_gt.jpg	('Ht',)
Saved image: ./gt_images_test/images_gt/2913_0_gt.jpg
Saved label: 2913_0_gt.jpg	('y',)
Saved image: ./gt_images_test/images_gt/2914_0_gt.jpg
Saved label: 2914_0_gt.jpg	('kh',)
Saved image: ./gt_images_test/images_gt/2915_0_gt.jpg
Saved label: 2915_0_gt.jpg	('dy',)
Saved image: ./gt_images_test/images_gt/2916_0_gt.jpg
Saved label: 2916_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/2917_0_gt.jpg
Saved label: 2917_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/2918_0_gt.jpg
Saved label: 2918_0_gt.jpg	('Theo',)
Saved image: ./gt_images_test/images_gt/2919_0_gt.jpg
Saved label: 2919_0_gt.jpg	('Ly',)
Saved image: ./gt_images_test/images_gt/2920_0_gt.jpg
Saved label: 2920_0_gt.jpg	('ngc',)
Saved image: ./gt_images_test/images_gt/2921_0_gt.jpg
Saved label: 2921_0_gt.jpg	('Bung',)
Saved image: ./gt_images_test/images_gt/2922_0_gt.jpg
Saved label: 2922_0_gt.jpg	('Ti',)
Saved image: ./gt_images_test/images_gt/2923_0_gt.jpg
Saved label: 2923_0_gt.jpg	('Nhn',)
Saved image: ./gt_images_test/images_gt/2924_0_gt.jpg
Saved label: 2924_0_gt.jpg	('Tn',)
Saved image: ./gt_images_test/images_gt/2925_0_gt.jpg
Saved label: 2925_0_gt.jpg	('Ct',)
Saved image: ./gt_images_test/images_gt/2926_0_gt.jpg
Saved label: 2926_0_gt.jpg	('ly',)
Saved image: ./gt_images_test/images_gt/2927_0_gt.jpg
Saved label: 2927_0_gt.jpg	('N',)
Saved image: ./gt_images_test/images_gt/2928_0_gt.jpg
Saved label: 2928_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/2929_0_gt.jpg
Saved label: 2929_0_gt.jpg	('rng',)
Saved image: ./gt_images_test/images_gt/2930_0_gt.jpg
Saved label: 2930_0_gt.jpg	('Bng',)
Saved image: ./gt_images_test/images_gt/2931_0_gt.jpg
Saved label: 2931_0_gt.jpg	('trng',)
Saved image: ./gt_images_test/images_gt/2932_0_gt.jpg
Saved label: 2932_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/2933_0_gt.jpg
Saved label: 2933_0_gt.jpg	('ph',)
Saved image: ./gt_images_test/images_gt/2934_0_gt.jpg
Saved label: 2934_0_gt.jpg	('lng',)
Saved image: ./gt_images_test/images_gt/2935_0_gt.jpg
Saved label: 2935_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/2936_0_gt.jpg
Saved label: 2936_0_gt.jpg	('hoa',)
Saved image: ./gt_images_test/images_gt/2937_0_gt.jpg
Saved label: 2937_0_gt.jpg	('bao',)
Saved image: ./gt_images_test/images_gt/2938_0_gt.jpg
Saved label: 2938_0_gt.jpg	('cu',)
Saved image: ./gt_images_test/images_gt/2939_0_gt.jpg
Saved label: 2939_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/2940_0_gt.jpg
Saved label: 2940_0_gt.jpg	('cnh',)
Saved image: ./gt_images_test/images_gt/2941_0_gt.jpg
Saved label: 2941_0_gt.jpg	('di',)
Saved image: ./gt_images_test/images_gt/2942_0_gt.jpg
Saved label: 2942_0_gt.jpg	('Nguyt',)
Saved image: ./gt_images_test/images_gt/2943_0_gt.jpg
Saved label: 2943_0_gt.jpg	('Hng',)
Saved image: ./gt_images_test/images_gt/2944_0_gt.jpg
Saved label: 2944_0_gt.jpg	('Thy',)
Saved image: ./gt_images_test/images_gt/2945_0_gt.jpg
Saved label: 2945_0_gt.jpg	('xa',)
Saved image: ./gt_images_test/images_gt/2946_0_gt.jpg
Saved label: 2946_0_gt.jpg	('Khi',)
Saved image: ./gt_images_test/images_gt/2947_0_gt.jpg
Saved label: 2947_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2948_0_gt.jpg
Saved label: 2948_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2949_0_gt.jpg
Saved label: 2949_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/2950_0_gt.jpg
Saved label: 2950_0_gt.jpg	('thy',)
Saved image: ./gt_images_test/images_gt/2951_0_gt.jpg
Saved label: 2951_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/2952_0_gt.jpg
Saved label: 2952_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/2953_0_gt.jpg
Saved label: 2953_0_gt.jpg	('Long',)
Saved image: ./gt_images_test/images_gt/2954_0_gt.jpg
Saved label: 2954_0_gt.jpg	('s',)
Saved image: ./gt_images_test/images_gt/2955_0_gt.jpg
Saved label: 2955_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/2956_0_gt.jpg
Saved label: 2956_0_gt.jpg	('M',)
Saved image: ./gt_images_test/images_gt/2957_0_gt.jpg
Saved label: 2957_0_gt.jpg	('tng',)
Saved image: ./gt_images_test/images_gt/2958_0_gt.jpg
Saved label: 2958_0_gt.jpg	('Mn',)
Saved image: ./gt_images_test/images_gt/2959_0_gt.jpg
Saved label: 2959_0_gt.jpg	('Tm',)
Saved image: ./gt_images_test/images_gt/2960_0_gt.jpg
Saved label: 2960_0_gt.jpg	('chin',)
Saved image: ./gt_images_test/images_gt/2961_0_gt.jpg
Saved label: 2961_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/2962_0_gt.jpg
Saved label: 2962_0_gt.jpg	('bt',)
Saved image: ./gt_images_test/images_gt/2963_0_gt.jpg
Saved label: 2963_0_gt.jpg	('hp',)
Saved image: ./gt_images_test/images_gt/2964_0_gt.jpg
Saved label: 2964_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/2965_0_gt.jpg
Saved label: 2965_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/2966_0_gt.jpg
Saved label: 2966_0_gt.jpg	('th',)
Saved image: ./gt_images_test/images_gt/2967_0_gt.jpg
Saved label: 2967_0_gt.jpg	('di',)
Saved image: ./gt_images_test/images_gt/2968_0_gt.jpg
Saved label: 2968_0_gt.jpg	('mt',)
Saved image: ./gt_images_test/images_gt/2969_0_gt.jpg
Saved label: 2969_0_gt.jpg	('Lng',)
Saved image: ./gt_images_test/images_gt/2970_0_gt.jpg
Saved label: 2970_0_gt.jpg	('nin',)
Saved image: ./gt_images_test/images_gt/2971_0_gt.jpg
Saved label: 2971_0_gt.jpg	('bin',)
Saved image: ./gt_images_test/images_gt/2972_0_gt.jpg
Saved label: 2972_0_gt.jpg	('Con',)
Saved image: ./gt_images_test/images_gt/2973_0_gt.jpg
Saved label: 2973_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/2974_0_gt.jpg
Saved label: 2974_0_gt.jpg	('tinh',)
Saved image: ./gt_images_test/images_gt/2975_0_gt.jpg
Saved label: 2975_0_gt.jpg	('Mun',)
Saved image: ./gt_images_test/images_gt/2976_0_gt.jpg
Saved label: 2976_0_gt.jpg	('Mt',)
Saved image: ./gt_images_test/images_gt/2977_0_gt.jpg
Saved label: 2977_0_gt.jpg	('Cha',)
Saved image: ./gt_images_test/images_gt/2978_0_gt.jpg
Saved label: 2978_0_gt.jpg	('Hnh',)
Saved image: ./gt_images_test/images_gt/2979_0_gt.jpg
Saved label: 2979_0_gt.jpg	('Dng',)
Saved image: ./gt_images_test/images_gt/2980_0_gt.jpg
Saved label: 2980_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/2981_0_gt.jpg
Saved label: 2981_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2982_0_gt.jpg
Saved label: 2982_0_gt.jpg	('qu',)
Saved image: ./gt_images_test/images_gt/2983_0_gt.jpg
Saved label: 2983_0_gt.jpg	('vinh',)
Saved image: ./gt_images_test/images_gt/2984_0_gt.jpg
Saved label: 2984_0_gt.jpg	('thnh',)
Saved image: ./gt_images_test/images_gt/2985_0_gt.jpg
Saved label: 2985_0_gt.jpg	('s',)
Saved image: ./gt_images_test/images_gt/2986_0_gt.jpg
Saved label: 2986_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/2987_0_gt.jpg
Saved label: 2987_0_gt.jpg	('an',)
Saved image: ./gt_images_test/images_gt/2988_0_gt.jpg
Saved label: 2988_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/2989_0_gt.jpg
Saved label: 2989_0_gt.jpg	('Trn',)
Saved image: ./gt_images_test/images_gt/2990_0_gt.jpg
Saved label: 2990_0_gt.jpg	('mi',)
Saved image: ./gt_images_test/images_gt/2991_0_gt.jpg
Saved label: 2991_0_gt.jpg	('by',)
Saved image: ./gt_images_test/images_gt/2992_0_gt.jpg
Saved label: 2992_0_gt.jpg	('thin',)
Saved image: ./gt_images_test/images_gt/2993_0_gt.jpg
Saved label: 2993_0_gt.jpg	('vng',)
Saved image: ./gt_images_test/images_gt/2994_0_gt.jpg
Saved label: 2994_0_gt.jpg	('tin',)
Saved image: ./gt_images_test/images_gt/2995_0_gt.jpg
Saved label: 2995_0_gt.jpg	('chng',)
Saved image: ./gt_images_test/images_gt/2996_0_gt.jpg
Saved label: 2996_0_gt.jpg	('n',)
Saved image: ./gt_images_test/images_gt/2997_0_gt.jpg
Saved label: 2997_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/2998_0_gt.jpg
Saved label: 2998_0_gt.jpg	('V',)
Saved image: ./gt_images_test/images_gt/2999_0_gt.jpg
Saved label: 2999_0_gt.jpg	('Minh',)
Saved image: ./gt_images_test/images_gt/3000_0_gt.jpg
Saved label: 3000_0_gt.jpg	('Sc',)
Saved image: ./gt_images_test/images_gt/3001_0_gt.jpg
Saved label: 3001_0_gt.jpg	('Vn',)
Saved image: ./gt_images_test/images_gt/3002_0_gt.jpg
Saved label: 3002_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/3003_0_gt.jpg
Saved label: 3003_0_gt.jpg	('p',)
Saved image: ./gt_images_test/images_gt/3004_0_gt.jpg
Saved label: 3004_0_gt.jpg	('cha',)
Saved image: ./gt_images_test/images_gt/3005_0_gt.jpg
Saved label: 3005_0_gt.jpg	('l',)
Saved image: ./gt_images_test/images_gt/3006_0_gt.jpg
Saved label: 3006_0_gt.jpg	('u',)
Saved image: ./gt_images_test/images_gt/3007_0_gt.jpg
Saved label: 3007_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/3008_0_gt.jpg
Saved label: 3008_0_gt.jpg	('sn',)
Saved image: ./gt_images_test/images_gt/3009_0_gt.jpg
Saved label: 3009_0_gt.jpg	('ting',)
Saved image: ./gt_images_test/images_gt/3010_0_gt.jpg
Saved label: 3010_0_gt.jpg	('danh',)
Saved image: ./gt_images_test/images_gt/3011_0_gt.jpg
Saved label: 3011_0_gt.jpg	('tm',)
Saved image: ./gt_images_test/images_gt/3012_0_gt.jpg
Saved label: 3012_0_gt.jpg	('Ci',)
Saved image: ./gt_images_test/images_gt/3013_0_gt.jpg
Saved label: 3013_0_gt.jpg	('Ht',)
Saved image: ./gt_images_test/images_gt/3014_0_gt.jpg
Saved label: 3014_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/3015_0_gt.jpg
Saved label: 3015_0_gt.jpg	('Lm',)
Saved image: ./gt_images_test/images_gt/3016_0_gt.jpg
Saved label: 3016_0_gt.jpg	('u',)
Saved image: ./gt_images_test/images_gt/3017_0_gt.jpg
Saved label: 3017_0_gt.jpg	('Chu',)
Saved image: ./gt_images_test/images_gt/3018_0_gt.jpg
Saved label: 3018_0_gt.jpg	('V',)
Saved image: ./gt_images_test/images_gt/3019_0_gt.jpg
Saved label: 3019_0_gt.jpg	('phc',)
Saved image: ./gt_images_test/images_gt/3020_0_gt.jpg
Saved label: 3020_0_gt.jpg	('Mn',)
Saved image: ./gt_images_test/images_gt/3021_0_gt.jpg
Saved label: 3021_0_gt.jpg	('i',)
Saved image: ./gt_images_test/images_gt/3022_0_gt.jpg
Saved label: 3022_0_gt.jpg	('di',)
Saved image: ./gt_images_test/images_gt/3023_0_gt.jpg
Saved label: 3023_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/3024_0_gt.jpg
Saved label: 3024_0_gt.jpg	('nu',)
Saved image: ./gt_images_test/images_gt/3025_0_gt.jpg
Saved label: 3025_0_gt.jpg	('bc',)
Saved image: ./gt_images_test/images_gt/3026_0_gt.jpg
Saved label: 3026_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/3027_0_gt.jpg
Saved label: 3027_0_gt.jpg	('To',)
Saved image: ./gt_images_test/images_gt/3028_0_gt.jpg
Saved label: 3028_0_gt.jpg	('Hiu',)
Saved image: ./gt_images_test/images_gt/3029_0_gt.jpg
Saved label: 3029_0_gt.jpg	('Knh',)
Saved image: ./gt_images_test/images_gt/3030_0_gt.jpg
Saved label: 3030_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/3031_0_gt.jpg
Saved label: 3031_0_gt.jpg	('tin',)
Saved image: ./gt_images_test/images_gt/3032_0_gt.jpg
Saved label: 3032_0_gt.jpg	('Bi',)
Saved image: ./gt_images_test/images_gt/3033_0_gt.jpg
Saved label: 3033_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/3034_0_gt.jpg
Saved label: 3034_0_gt.jpg	('Hng',)
Saved image: ./gt_images_test/images_gt/3035_0_gt.jpg
Saved label: 3035_0_gt.jpg	('An',)
Saved image: ./gt_images_test/images_gt/3036_0_gt.jpg
Saved label: 3036_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/3037_0_gt.jpg
Saved label: 3037_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/3038_0_gt.jpg
Saved label: 3038_0_gt.jpg	('Ni',)
Saved image: ./gt_images_test/images_gt/3039_0_gt.jpg
Saved label: 3039_0_gt.jpg	('thch',)
Saved image: ./gt_images_test/images_gt/3040_0_gt.jpg
Saved label: 3040_0_gt.jpg	('nhau',)
Saved image: ./gt_images_test/images_gt/3041_0_gt.jpg
Saved label: 3041_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/3042_0_gt.jpg
Saved label: 3042_0_gt.jpg	('li',)
Saved image: ./gt_images_test/images_gt/3043_0_gt.jpg
Saved label: 3043_0_gt.jpg	('Cha',)
Saved image: ./gt_images_test/images_gt/3044_0_gt.jpg
Saved label: 3044_0_gt.jpg	('su',)
Saved image: ./gt_images_test/images_gt/3045_0_gt.jpg
Saved label: 3045_0_gt.jpg	('cng',)
Saved image: ./gt_images_test/images_gt/3046_0_gt.jpg
Saved label: 3046_0_gt.jpg	('m',)
Saved image: ./gt_images_test/images_gt/3047_0_gt.jpg
Saved label: 3047_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/3048_0_gt.jpg
Saved label: 3048_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/3049_0_gt.jpg
Saved label: 3049_0_gt.jpg	('ng',)
Saved image: ./gt_images_test/images_gt/3050_0_gt.jpg
Saved label: 3050_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/3051_0_gt.jpg
Saved label: 3051_0_gt.jpg	('dng',)
Saved image: ./gt_images_test/images_gt/3052_0_gt.jpg
Saved label: 3052_0_gt.jpg	('mn',)
Saved image: ./gt_images_test/images_gt/3053_0_gt.jpg
Saved label: 3053_0_gt.jpg	('u',)
Saved image: ./gt_images_test/images_gt/3054_0_gt.jpg
Saved label: 3054_0_gt.jpg	('nin',)
Saved image: ./gt_images_test/images_gt/3055_0_gt.jpg
Saved label: 3055_0_gt.jpg	('Ngc',)
Saved image: ./gt_images_test/images_gt/3056_0_gt.jpg
Saved label: 3056_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/3057_0_gt.jpg
Saved label: 3057_0_gt.jpg	('chn',)
Saved image: ./gt_images_test/images_gt/3058_0_gt.jpg
Saved label: 3058_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/3059_0_gt.jpg
Saved label: 3059_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/3060_0_gt.jpg
Saved label: 3060_0_gt.jpg	('',)
Saved image: ./gt_images_test/images_gt/3061_0_gt.jpg
Saved label: 3061_0_gt.jpg	('sng',)
Saved image: ./gt_images_test/images_gt/3062_0_gt.jpg
Saved label: 3062_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/3063_0_gt.jpg
Saved label: 3063_0_gt.jpg	('Qu',)
Saved image: ./gt_images_test/images_gt/3064_0_gt.jpg
Saved label: 3064_0_gt.jpg	('nh',)
Saved image: ./gt_images_test/images_gt/3065_0_gt.jpg
Saved label: 3065_0_gt.jpg	('Th',)
Saved image: ./gt_images_test/images_gt/3066_0_gt.jpg
Saved label: 3066_0_gt.jpg	('t',)
Saved image: ./gt_images_test/images_gt/3067_0_gt.jpg
Saved label: 3067_0_gt.jpg	('cc',)
Saved image: ./gt_images_test/images_gt/3068_0_gt.jpg
Saved label: 3068_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/3069_0_gt.jpg
Saved label: 3069_0_gt.jpg	('nhn',)
Saved image: ./gt_images_test/images_gt/3070_0_gt.jpg
Saved label: 3070_0_gt.jpg	('nhng',)
Saved image: ./gt_images_test/images_gt/3071_0_gt.jpg
Saved label: 3071_0_gt.jpg	('v',)
Saved image: ./gt_images_test/images_gt/3072_0_gt.jpg
Saved label: 3072_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/3073_0_gt.jpg
Saved label: 3073_0_gt.jpg	('ln',)
Saved image: ./gt_images_test/images_gt/3074_0_gt.jpg
Saved label: 3074_0_gt.jpg	('Qun',)
Saved image: ./gt_images_test/images_gt/3075_0_gt.jpg
Saved label: 3075_0_gt.jpg	('Vu',)
Saved image: ./gt_images_test/images_gt/3076_0_gt.jpg
Saved label: 3076_0_gt.jpg	('ti',)
Saved image: ./gt_images_test/images_gt/3077_0_gt.jpg
Saved label: 3077_0_gt.jpg	('No',)
Saved image: ./gt_images_test/images_gt/3078_0_gt.jpg
Saved label: 3078_0_gt.jpg	('c',)
Saved image: ./gt_images_test/images_gt/3079_0_gt.jpg
Saved label: 3079_0_gt.jpg	('Doanh',)
Saved image: ./gt_images_test/images_gt/3080_0_gt.jpg
Saved label: 3080_0_gt.jpg	('ta',)
Saved image: ./gt_images_test/images_gt/3081_0_gt.jpg
Saved label: 3081_0_gt.jpg	('Lc',)
Saved image: ./gt_images_test/images_gt/3082_0_gt.jpg
Saved label: 3082_0_gt.jpg	('vinh',)
Saved image: ./gt_images_test/images_gt/3083_0_gt.jpg
Saved label: 3083_0_gt.jpg	('chen',)
Saved image: ./gt_images_test/images_gt/3084_0_gt.jpg
Saved label: 3084_0_gt.jpg	('lc',)
Saved image: ./gt_images_test/images_gt/3085_0_gt.jpg
Saved label: 3085_0_gt.jpg	('Phc',)
Saved image: ./gt_images_test/images_gt/3086_0_gt.jpg
Saved label: 3086_0_gt.jpg	('tnh',)
Saved image: ./gt_images_test/images_gt/3087_0_gt.jpg
Saved label: 3087_0_gt.jpg	('lm',)
Saved image: ./gt_images_test/images_gt/3088_0_gt.jpg
Saved label: 3088_0_gt.jpg	('Thnh',)
Saved image: ./gt_images_test/images_gt/3089_0_gt.jpg
Saved label: 3089_0_gt.jpg	('h',)
Saved image: ./gt_images_test/images_gt/3090_0_gt.jpg
Saved label: 3090_0_gt.jpg	('chic',)
Saved image: ./gt_images_test/images_gt/3091_0_gt.jpg
Saved label: 3091_0_gt.jpg	('nguyn',)
Saved image: ./gt_images_test/images_gt/3092_0_gt.jpg
Saved label: 3092_0_gt.jpg	('thng',)
Saved image: ./gt_images_test/images_gt/3093_0_gt.jpg
Saved label: 3093_0_gt.jpg	('ph',)
Saved image: ./gt_images_test/images_gt/3094_0_gt.jpg
Saved label: 3094_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/3095_0_gt.jpg
Saved label: 3095_0_gt.jpg	('ch',)
Saved image: ./gt_images_test/images_gt/3096_0_gt.jpg
Saved label: 3096_0_gt.jpg	('thc',)
Saved image: ./gt_images_test/images_gt/3097_0_gt.jpg
Saved label: 3097_0_gt.jpg	('Kn',)
Saved image: ./gt_images_test/images_gt/3098_0_gt.jpg
Saved label: 3098_0_gt.jpg	('Gic',)
Saved image: ./gt_images_test/images_gt/3099_0_gt.jpg
Saved label: 3099_0_gt.jpg	('Nhn',)
Saved image: ./gt_images_test/images_gt/3100_0_gt.jpg
Saved label: 3100_0_gt.jpg	('giao',)
Saved image: ./gt_images_test/images_gt/3101_0_gt.jpg
Saved label: 3101_0_gt.jpg	('Ph',)
Saved image: ./gt_images_test/images_gt/3102_0_gt.jpg
Saved label: 3102_0_gt.jpg	('Nam',)
Saved image: ./gt_images_test/images_gt/3103_0_gt.jpg
Saved label: 3103_0_gt.jpg	('Tng',)
Saved image: ./gt_images_test/images_gt/3104_0_gt.jpg
Saved label: 3104_0_gt.jpg	('ting',)
Saved image: ./gt_images_test/images_gt/3105_0_gt.jpg
Saved label: 3105_0_gt.jpg	('tt',)
Saved image: ./gt_images_test/images_gt/3106_0_gt.jpg
Saved label: 3106_0_gt.jpg	('u',)
Saved image: ./gt_images_test/images_gt/3107_0_gt.jpg
Saved label: 3107_0_gt.jpg	('vt',)
All ground truth images and labels have been saved.
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = ./saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from ./saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = ./saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from ./saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = ./saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/last.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from ./saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/last.pth.
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = ./saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from ./saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training-->22431'

'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

each epoch iteration: 535
Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

Start training from scratch.
iteration:0--> lr: 0.0--> train loss:0.054546188563108444
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training-->22431'

'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

each epoch iteration: 535
Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

Start training from scratch.
iteration:0--> lr: 0.0--> train loss:0.010645919479429722
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/50000.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training-->22431'

'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

each epoch iteration: 535
Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/50000.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

Resume training from iteration 50000.
iteration:50000--> lr: 6.357202997890846e-06--> train loss:0.03122146986424923
eval model at last
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = False
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = False
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

Read vision model from /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth.
DataParallel(
  (module): DINO_Finetune(
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 512, kernel_size=(4, 4), stride=(4, 4))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (head): Identity()
      (norm_seg): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      )
    )
    (encoder): Mlp(
      (fc1): Linear(in_features=512, out_features=512, bias=True)
      (act): GELU()
      (fc2): Linear(in_features=512, out_features=512, bias=True)
      (drop): Dropout(p=0.1, inplace=False)
    )
    (decoder): NRTRDecoder(
      (trg_word_emb): Embedding(226, 512, padding_idx=225)
      (position_enc): PositionalEncoding(
        (dropout): Dropout(p=0, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (layer_stack): ModuleList(
        (0): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): TFDecoderLayer(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (enc_attn): MultiHeadAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=False)
            (linear_k): Linear(in_features=512, out_features=512, bias=False)
            (linear_v): Linear(in_features=512, out_features=512, bias=False)
            (attention): ScaledDotProductAttention(
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (fc): Linear(in_features=512, out_features=512, bias=False)
            (proj_drop): Dropout(p=0.1, inplace=False)
          )
          (mlp): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=256, bias=True)
            (w_2): Linear(in_features=256, out_features=512, bias=True)
            (act): GELU()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (classifier): Linear(in_features=512, out_features=225, bias=True)
    )
    (loss): TFLoss(
      (loss_ce): CrossEntropyLoss()
    )
  )
)

Trainable params num: 52925665

eval model
ModelConfig(
	(0): arch = vit_base
	(1): clip_grad = None
	(2): dataset_augmentation_severity = 0
	(3): dataset_case_sensitive = True
	(4): dataset_charset_path = data/charset_36.txt
	(5): dataset_charset_type = DICTVI
	(6): dataset_data_aug = True
	(7): dataset_eval_case_sensitive = True
	(8): dataset_filter_single_punctuation = False
	(9): dataset_image_height = 32
	(10): dataset_image_width = 128
	(11): dataset_mask = False
	(12): dataset_max_length = 25
	(13): dataset_multiscales = False
	(14): dataset_num_workers = 8
	(15): dataset_pin_memory = True
	(16): dataset_portion = 1.0
	(17): dataset_scheme = supervised
	(18): dataset_smooth_factor = 0.1
	(19): dataset_smooth_label = False
	(20): dataset_test_batch_size = 42
	(21): dataset_test_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(22): dataset_train_batch_size = 42
	(23): dataset_train_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Data/ViCalligraphy-Augment/ViCalligrphy_3000_VNI_7000_Unicode/training']
	(24): dataset_train_weights = None
	(25): dataset_type = ST
	(26): dataset_use_sm = False
	(27): dataset_valid_batch_size = 42
	(28): dataset_valid_roots = ['/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation']
	(29): decoder_d_embedding = 512
	(30): decoder_d_inner = 256
	(31): decoder_d_k = 64
	(32): decoder_d_model = 512
	(33): decoder_d_v = 64
	(34): decoder_max_seq_len = 25
	(35): decoder_n_head = 8
	(36): decoder_n_layers = 6
	(37): decoder_num_classes = 226
	(38): decoder_padding_idx = 225
	(39): decoder_start_idx = 224
	(40): decoder_type = NRTRDecoder
	(41): drop_path_rate = 0.1
	(42): global_debug = False
	(43): global_name = CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(44): global_phase = train
	(45): global_seed = None
	(46): global_stage = train-supervised
	(47): global_workdir = workdir/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive
	(48): lr = 0.0005
	(49): min_lr = 1e-06
	(50): model_checkpoint = /mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/saved_models/CCD_finetune_100epochs_ViCalligraphy_3000-VNI_7000-Unicode_base_case_sensitive/best_accuracy.pth
	(51): model_name = semimtr.modules.model_abinet.ABINetModel
	(52): model_strict = True
	(53): mp_num = 4
	(54): num_workers = 10
	(55): optimizer = adamw
	(56): optimizer_args_betas = (0.9, 0.999)
	(57): optimizer_bn_wd = False
	(58): optimizer_clip_grad = 20
	(59): optimizer_lr = 0.0001
	(60): optimizer_scheduler_gamma = 0.1
	(61): optimizer_scheduler_periods = [3, 1, 1]
	(62): optimizer_true_wd = False
	(63): optimizer_type = Adam
	(64): optimizer_wd = 0.0
	(65): out_dim = 65536
	(66): output_dir = ./saved_models/
	(67): patch_size = 4
	(68): seed = 0
	(69): training_epochs = 100
	(70): training_eval_iters = 5000
	(71): training_hist_iters = 10000000
	(72): training_save_iters = 10000
	(73): training_show_iters = 5000
	(74): training_start_iters = 0
	(75): training_stats_iters = 1000
	(76): warmup_epochs = 1
	(77): weight_decay = 0.05
)
Construct dataset.
'current_dataset_path:/mlcv2/WorkingSpace/Personal/hamh/Ha/Methods/CCD/CCD_Ha/Dino/training_eval_ViCalligraphy/evaluation-->3108'

